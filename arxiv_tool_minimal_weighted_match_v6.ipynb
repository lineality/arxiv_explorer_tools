{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ba77c8fe-bdc2-4e48-91f2-a942055118eb",
        "SOMfhOwlr-zu",
        "YepU-A4Fr_J3",
        "ItIQ_onG-IXX",
        "WPnLaV3fpCkv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba77c8fe-bdc2-4e48-91f2-a942055118eb"
      },
      "source": [
        "# Arxiv Explorer Tools - minimal weighted match\n",
        "- Fast: ~5-10 sec to run vs. 5-10 min for embedding or TFIDF versions.\n",
        "- multi-topic: use as many pre-set seaches as you want\n",
        "- extracts articles on topics of interest from the too-many-to-look-through daily pages of articles that come out each day.\n",
        "- saves results to json (for automation later) and html (for easy reading and linking)\n",
        "- minimal weighted match uses a list of phrases and an integer weight for each\n",
        "- arxiv reading uses 'beautiful soup'\n",
        "\n",
        "### Setup & Install:\n",
        "- have python installed and use an python env\n",
        "- use a jupyter notebook or script, etc.\n",
        "- for specialty topics you can create extensive weighted search profiles.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11e7a29-5a13-4c90-b3a3-f4409a9013b2"
      },
      "source": [
        "\n",
        "- https://pypi.org/project/beautifulsoup4/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdea8fa-7a5d-4d32-a88b-1b1f8619e1b3"
      },
      "source": [
        "requirements.txt ->\n",
        "```\n",
        "scikit-learn\n",
        "scipy\n",
        "numpy\n",
        "beautifulsoup4\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "e4c5c9be-949c-4c72-b2cf-b26df5316aa2"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "\n",
        "\n",
        "def duration_min_sec(start_time, end_time):\n",
        "\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    duration_seconds = duration.total_seconds()\n",
        "\n",
        "    minutes = int(duration_seconds // 60)\n",
        "    seconds = duration_seconds % 60\n",
        "    time_message = f\"{minutes}_min__{seconds:.1f}_sec\"\n",
        "\n",
        "    return time_message\n",
        "\n",
        "# # start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "# duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "# print(f\"Duration to run -> {duration_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minimal weighted matching code"
      ],
      "metadata": {
        "id": "SOMfhOwlr-zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "# from collections import Counter\n",
        "\n",
        "\n",
        "# And an even more simplistic basic key word search (with optional weights)\n",
        "\n",
        "import re\n",
        "\n",
        "def rank_documents_on_weighted_matches(documents, keyword_weights):\n",
        "    \"\"\"\n",
        "    Ranks documents based on the presence of weighted keywords-phrases.\n",
        "    comparison looks at text without:\n",
        "    - captialization\n",
        "    - spaces\n",
        "    - newlines\n",
        "    - special symbols\n",
        "\n",
        "    Parameters:\n",
        "    documents (list of str): The list of documents to be ranked.\n",
        "    keyword_weights (list of tuple): A list of tuples, where the first element is the keyword and the\n",
        "    second element is the corresponding weight.\n",
        "\n",
        "    Returns:\n",
        "    list of (str, float): A list of tuples, where the first element is the document and the\n",
        "    second element is the ranking score.\n",
        "    \"\"\"\n",
        "\n",
        "    ranked_documents = []\n",
        "\n",
        "    for document in documents:\n",
        "        score = 0\n",
        "        # Make the document lowercase and strip all symbols, spaces, and newline characters\n",
        "        match_document = re.sub(r'[^\\w\\s]', '', document.lower()).replace('\\n', '').replace(' ','')\n",
        "        # print(match_document)\n",
        "        for keyword, weight in keyword_weights:\n",
        "\n",
        "            # Make the keyword lowercase and strip all symbols, spaces, and newline characters\n",
        "            match_keyword = re.sub(r'[^\\w\\s]', '', keyword.lower()).replace('\\n', '').replace(' ','')\n",
        "            # print(match_keyword)\n",
        "            # Check if the keyword-phrase is in the document\n",
        "            if match_keyword in match_document:\n",
        "                # If the keyword-phrase is in the document, add its weight to the score\n",
        "                score += weight\n",
        "\n",
        "        ranked_documents.append((document, score))\n",
        "\n",
        "    # Sort the documents by their ranking scores in descending order\n",
        "    ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_documents\n",
        "\n",
        "\n",
        "# ################\n",
        "# # Example usage\n",
        "# ################\n",
        "# corpus = [\n",
        "#     \"This is the first document about machine learning.\",\n",
        "#     \"The second document discusses data analysis and visualization.\",\n",
        "#     \"The third document focuses on natural language processing.\",\n",
        "#     \"The fourth document talks about deep learning and neural networks.\",\n",
        "#     \"\"\"to test line breaks\n",
        "#     Emotion mining\n",
        "#      data\n",
        "#     analysis\n",
        "#     Keywords: emotion mining, sentiment analysis, natural disasters, psychology, technological disasters\"\"\",\n",
        "# ]\n",
        "\n",
        "# keyword_weights = [(\"machine learning\", 3), (\"data analysis\", 2), (\"natural language processing\", 4), (\"deep learning\", 5), (\"neural networks\", 6)]\n",
        "\n",
        "# ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "# for document, score in ranked_documents:\n",
        "#     print(f\"Document: {document}\\nScore: {score}\\n\")\n"
      ],
      "metadata": {
        "id": "bqy_ZPvpr-6o"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv Explorerer\n"
      ],
      "metadata": {
        "id": "YepU-A4Fr_J3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "19bd0781-5480-4ec0-9709-07330763fd06"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# Arxiv Explorerer\n",
        "###################\n",
        "\n",
        "# step 1: embed the search-phrase\n",
        "# step 2: embed each text\n",
        "# step 3: get scores\n",
        "# step 4: evaluates if score is succss or fail\n",
        "# step 5: if success: do stuff with text\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "start_time_whole_single_task = datetime.now()\n",
        "\n",
        "\n",
        "# ##########################################\n",
        "# # Make comparison phrase and vectorize it\n",
        "# ##########################################\n",
        "# comparison_phrase = \"computer vision resolution enhancement\"\n",
        "# # comparison_phrase = \"cyber security\"\n",
        "# # comparison_phrase = \"natural language processing\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hght1gb699Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Article Corpus"
      ],
      "metadata": {
        "id": "ItIQ_onG-IXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_segment_time = datetime.now()\n",
        "\n",
        "#####################\n",
        "# Get Article Corpus\n",
        "#####################\n",
        "\n",
        "# List to hold all article data\n",
        "article_data = []\n",
        "\n",
        "# # Make a request to the website\n",
        "r = requests.get('https://arxiv.org/list/cs/new')\n",
        "\n",
        "url = \"https://arxiv.org/list/cs/new\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# # Find all the articles\n",
        "articles = soup.find_all('dt')\n",
        "\n",
        "# # Find all the titles\n",
        "articles_title = soup.find_all('div', {'class': 'list-title mathjax'})\n",
        "\n",
        "# Find all the subject on the page\n",
        "articles_subject = soup.find_all('dd')\n",
        "\n",
        "\n",
        "###############\n",
        "# make corpus\n",
        "###############\n",
        "\n",
        "corpus = []\n",
        "report_list = []\n",
        "article_dicts = []\n",
        "\n",
        "for this_index, article in enumerate(articles):\n",
        "\n",
        "    ################################################\n",
        "    # Extract each field of data about each article\n",
        "    ################################################\n",
        "\n",
        "    # Extract the title\n",
        "    title = articles_title[this_index].text.split('Title:')[1].strip()\n",
        "\n",
        "    # Extract the subjects\n",
        "    subjects = articles_subject[this_index].find('span', {'class': 'primary-subject'}).text\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "\n",
        "    abstract_p = article.find_next_sibling('dd').find('p', {'class': 'mathjax'})\n",
        "\n",
        "    # Extract the abstract\n",
        "    if abstract_p:\n",
        "        abstract = abstract_p.text.strip()\n",
        "    else:\n",
        "        abstract = \"\"\n",
        "\n",
        "    pdf_link_segment = article.find('a', {'title': 'Download PDF'})['href']\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "    pdf_link = f\"https://arxiv.org{pdf_link_segment}\"\n",
        "    paper_link = f\"https://arxiv.org/abs/{arxiv_id[6:]}\"\n",
        "\n",
        "    # extracted_article_string = title + \" \" + abstract + \" \" + str(subjects)\n",
        "\n",
        "    # assemble corpus\n",
        "    article_characters = f\"{this_index}|||| \"\n",
        "\n",
        "    article_characters += f\"\\n'arxiv_id': {arxiv_id}, \"\n",
        "    article_characters += f\"\\n'paper_link': {paper_link}, \"\n",
        "    article_characters += f\"\\n'pdf_link': {pdf_link}, \"\n",
        "\n",
        "    article_characters += \"\\nTitle: \" + title + \" \"\n",
        "    article_characters += \"\\nSubjects: \" + subjects + \" \"\n",
        "    article_characters += \"\\nAbstract: \" + abstract\n",
        "\n",
        "    ##################################\n",
        "    # Make Bundles (sharing an index)\n",
        "    ##################################\n",
        "\n",
        "    # # add to corpus: just the meaningful text\n",
        "    # corpus.append(extracted_article_string)\n",
        "\n",
        "    # add to simple report_list: includes link and article ID info\n",
        "    report_list.append(article_characters)\n",
        "\n",
        "    # Append the data to the list\n",
        "    article_dicts.append({\n",
        "        'title': title,\n",
        "        'abstract': abstract,\n",
        "        'paper_link': paper_link,\n",
        "        'pdf_link': pdf_link,\n",
        "        'subjects': subjects,\n",
        "        'arxiv_id': arxiv_id,\n",
        "        'article_sequence_index': this_index,\n",
        "    })\n",
        "\n",
        "    # using this because only basic search works\n",
        "    corpus = report_list\n",
        "\n",
        "\n",
        "# # Segment Timer\n",
        "# start_segment_time = datetime.now()\n",
        "end_segment_time = datetime.now()\n",
        "duration_time = duration_min_sec(start_segment_time, end_segment_time)\n",
        "print(f\"Duration to run segment -> {duration_time}\")"
      ],
      "metadata": {
        "id": "e8FPqO0u-IXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f60230-107d-41e0-8ab7-8c93cdb4d58a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run segment -> 0_min__1.7_sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (size of corpus)\n",
        "len(corpus)"
      ],
      "metadata": {
        "id": "bve1wNfDBC-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948173bd-d789-4570-d956-523b928aaa9e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "611"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# print and save: code"
      ],
      "metadata": {
        "id": "WPnLaV3fpCkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def print_and_save(ranked_documents, top_n):\n",
        "    # Posix UTC Seconds\n",
        "    # make readable time\n",
        "    # from datetime import datetime\n",
        "    date_time = datetime.now()\n",
        "    clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    results_json_list = []\n",
        "\n",
        "    for document, score in ranked_documents:\n",
        "\n",
        "        if score != 0:\n",
        "\n",
        "            blurb = f\"Document: {document}\\nScore: {score}\\n\"\n",
        "\n",
        "            print(blurb)\n",
        "\n",
        "        this_index = int(document.split('||||')[0])\n",
        "\n",
        "        data_dict = article_dicts[this_index]\n",
        "\n",
        "        results_json_list.append(data_dict)\n",
        "\n",
        "        counter += 1\n",
        "        if counter >= top_n:\n",
        "            break\n",
        "\n",
        "\n",
        "    #############\n",
        "    # Write Data\n",
        "    #############\n",
        "\n",
        "    # Save the data to a JSON file\n",
        "    with open(f'articles_{clean_timestamp}.json', 'w') as f:\n",
        "        json.dump(results_json_list, f)\n",
        "\n",
        "    # Create an HTML file\n",
        "    html = '<html><body>'\n",
        "    for article in results_json_list:\n",
        "        html += f'<h2><a href=\"{article[\"paper_link\"]}\">{article[\"title\"]}</a></h2>'\n",
        "        html += f'<p>{article[\"abstract\"]}</p>'\n",
        "        html += f'<p>Subjects: {str(article[\"subjects\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"paper_link\"]}\">{article[\"paper_link\"]}</a>'\n",
        "        html += f'<p>paper link: {str(article[\"paper_link\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"pdf_link\"]}\">{article[\"pdf_link\"]}</a>'\n",
        "        html += f'<p>pdf link: {str(article[\"pdf_link\"])}</p>'\n",
        "\n",
        "        html += f'<p>arxiv id: {str(article[\"arxiv_id\"])}</p>'\n",
        "        html += f'<p>article_sequence_index id: {str(article[\"article_sequence_index\"])}</p>'\n",
        "\n",
        "    html += '</body></html>'\n",
        "\n",
        "\n",
        "    # Save the HTML to a file\n",
        "    with open(f'articles{clean_timestamp}.html', 'w') as f:\n",
        "        f.write(html)"
      ],
      "metadata": {
        "id": "fsrVgItspCx2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find top-n articles: use keyword/weights"
      ],
      "metadata": {
        "id": "bt_SeRE_l345"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Results Returned\n",
        "top_n = 3\n",
        "\n",
        "list_of_lists_of_weights = [\n",
        "    # keyword_weights =\n",
        "    [\n",
        "        (\"computer vision\", 3),\n",
        "        (\"resolution\", 2),\n",
        "        # (\"natural language processing\", 4),\n",
        "        # (\"deep learning\", 5),\n",
        "        (\"neural networks\", 6),\n",
        "    ],\n",
        "\n",
        "    # keyword_weights =\n",
        "    [\n",
        "        (\"education\", 3),\n",
        "        (\"learning\", 2),\n",
        "        (\"development\", 4),\n",
        "        (\"psychology\", 6),\n",
        "    ],\n",
        "]\n",
        "\n",
        "for keyword_weights in list_of_lists_of_weights:\n",
        "    print(\"\\n\\nSet:\")\n",
        "    ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "    print_and_save(ranked_documents, top_n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CixuXw-Fl3-f",
        "outputId": "672d4cc1-c06f-4d78-a1b1-3744fae213e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set:\n",
            "Document: 9|||| \n",
            "'arxiv_id': arXiv:2407.17480, \n",
            "'paper_link': https://arxiv.org/abs/2407.17480, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.17480, \n",
            "Title: Universal Approximation Theory: The basic theory for deep learning-based computer vision models \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Computer vision (CV) is one of the most crucial fields in artificial intelligence. In recent years, a variety of deep learning models based on convolutional neural networks (CNNs) and Transformers have been designed to tackle diverse problems in CV. These algorithms have found practical applications in areas such as robotics and facial recognition. Despite the increasing power of current CV models, several fundamental questions remain unresolved: Why do CNNs require deep layers? What ensures the generalization ability of CNNs? Why do residual-based networks outperform fully convolutional networks like VGG? What is the fundamental difference between residual-based CNNs and Transformer-based networks? Why can CNNs utilize LoRA and pruning techniques? The root cause of these questions lies in the lack of a robust theoretical foundation for deep learning models in CV. To address these critical issues and techniques, we employ the Universal Approximation Theorem (UAT) to provide a theoretical basis for convolution- and Transformer-based models in CV. By doing so, we aim to elucidate these questions from a theoretical perspective.\n",
            "Score: 9\n",
            "\n",
            "Document: 78|||| \n",
            "'arxiv_id': arXiv:2407.17630, \n",
            "'paper_link': https://arxiv.org/abs/2407.17630, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.17630, \n",
            "Title: Revising the Problem of Partial Labels from the Perspective of CNNs' Robustness \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Convolutional neural networks (CNNs) have gained increasing popularity and versatility in recent decades, finding applications in diverse domains. These remarkable achievements are greatly attributed to the support of extensive datasets with precise labels. However, annotating image datasets is intricate and complex, particularly in the case of multi-label datasets. Hence, the concept of partial-label setting has been proposed to reduce annotation costs, and numerous corresponding solutions have been introduced. The evaluation methods for these existing solutions have been primarily based on accuracy. That is, their performance is assessed by their predictive accuracy on the test set. However, we insist that such an evaluation is insufficient and one-sided. On one hand, since the quality of the test set has not been evaluated, the assessment results are unreliable. On the other hand, the partial-label problem may also be raised by undergoing adversarial attacks. Therefore, incorporating robustness into the evaluation system is crucial. For this purpose, we first propose two attack models to generate multiple partial-label datasets with varying degrees of label missing rates. Subsequently, we introduce a lightweight partial-label solution using pseudo-labeling techniques and a designed loss function. Then, we employ D-Score to analyze both the proposed and existing methods to determine whether they can enhance robustness while improving accuracy. Extensive experimental results demonstrate that while certain methods may improve accuracy, the enhancement in robustness is not significant, and in some cases, it even diminishes.\n",
            "Score: 9\n",
            "\n",
            "Document: 132|||| \n",
            "'arxiv_id': arXiv:2407.17755, \n",
            "'paper_link': https://arxiv.org/abs/2407.17755, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.17755, \n",
            "Title: Enhancing Eye Disease Diagnosis with Deep Learning and Synthetic Data Augmentation \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: In recent years, the focus is on improving the diagnosis of diabetic retinopathy (DR) using machine learning and deep learning technologies. Researchers have explored various approaches, including the use of high-definition medical imaging, AI-driven algorithms such as convolutional neural networks (CNNs) and generative adversarial networks (GANs). Among all the available tools, CNNs have emerged as a preferred tool due to their superior classification accuracy and efficiency. Although the accuracy of CNNs is comparatively better but it can be improved by introducing some hybrid models by combining various machine learning and deep learning models. Therefore, in this paper, an ensemble learning technique is proposed for early detection and management of DR with higher accuracy. The proposed model is tested on the APTOS dataset and it is showing supremacy on the validation accuracy ($99\\%)$ in comparison to the previous models. Hence, the model can be helpful for early detection and treatment of the DR, thereby enhancing the overall quality of care for affected individuals.\n",
            "Score: 9\n",
            "\n",
            "\n",
            "\n",
            "Set:\n",
            "Document: 255|||| \n",
            "'arxiv_id': arXiv:2407.18022, \n",
            "'paper_link': https://arxiv.org/abs/2407.18022, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.18022, \n",
            "Title: Learning mental states estimation through self-observation: a developmental synergy between intentions and beliefs representations in a deep-learning model of Theory of Mind \n",
            "Subjects: Neural and Evolutionary Computing (cs.NE) \n",
            "Abstract: Theory of Mind (ToM), the ability to attribute beliefs, intentions, or mental states to others, is a crucial feature of human social interaction. In complex environments, where the human sensory system reaches its limits, behaviour is strongly driven by our beliefs about the state of the world around us. Accessing others' mental states, e.g., beliefs and intentions, allows for more effective social interactions in natural contexts. Yet, these variables are not directly observable, making understanding ToM a challenging quest of interest for different fields, including psychology, machine learning and robotics. In this paper, we contribute to this topic by showing a developmental synergy between learning to predict low-level mental states (e.g., intentions, goals) and attributing high-level ones (i.e., beliefs). Specifically, we assume that learning beliefs attribution can occur by observing one's own decision processes involving beliefs, e.g., in a partially observable environment. Using a simple feed-forward deep learning model, we show that, when learning to predict others' intentions and actions, more accurate predictions can be acquired earlier if beliefs attribution is learnt simultaneously. Furthermore, we show that the learning performance improves even when observed actors have a different embodiment than the observer and the gain is higher when observing beliefs-driven chunks of behaviour. We propose that our computational approach can inform the understanding of human social cognitive development and be relevant for the design of future adaptive social robots able to autonomously understand, assist, and learn from human interaction partners in novel natural environments and tasks.\n",
            "Score: 12\n",
            "\n",
            "Document: 3|||| \n",
            "'arxiv_id': arXiv:2407.17474, \n",
            "'paper_link': https://arxiv.org/abs/2407.17474, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.17474, \n",
            "Title: \"My Kind of Woman\": Analysing Gender Stereotypes in AI through The Averageness Theory and EU Law \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: This study delves into gender classification systems, shedding light on the interaction between social stereotypes and algorithmic determinations. Drawing on the \"averageness theory,\" which suggests a relationship between a face's attractiveness and the human ability to ascertain its gender, we explore the potential propagation of human bias into artificial intelligence (AI) systems. Utilising the AI model Stable Diffusion 2.1, we have created a dataset containing various connotations of attractiveness to test whether the correlation between attractiveness and accuracy in gender classification observed in human cognition persists within AI. Our findings indicate that akin to human dynamics, AI systems exhibit variations in gender classification accuracy based on attractiveness, mirroring social prejudices and stereotypes in their algorithmic decisions. This discovery underscores the critical need to consider the impacts of human perceptions on data collection and highlights the necessity for a multidisciplinary and intersectional approach to AI development and AI data training. By incorporating cognitive psychology and feminist legal theory, we examine how data used for AI training can foster gender diversity and fairness under the scope of the AI Act and GDPR, reaffirming how psychological and feminist legal theories can offer valuable insights for ensuring the protection of gender equality and non-discrimination in AI systems.\n",
            "Score: 10\n",
            "\n",
            "Document: 145|||| \n",
            "'arxiv_id': arXiv:2407.17773, \n",
            "'paper_link': https://arxiv.org/abs/2407.17773, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.17773, \n",
            "Title: KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: This paper investigates visual analogical reasoning in large multimodal models (LMMs) compared to human adults and children. A \"visual analogy\" is an abstract rule inferred from one image and applied to another. While benchmarks exist for testing visual reasoning in LMMs, they require advanced skills and omit basic visual analogies that even young children can make. Inspired by developmental psychology, we propose a new benchmark of 1,400 visual transformations of everyday objects to test LMMs on visual analogical reasoning and compare them to children and adults. We structure the evaluation into three stages: identifying what changed (e.g., color, number, etc.), how it changed (e.g., added one object), and applying the rule to new scenarios. Our findings show that while models like GPT-4V, LLaVA-1.5, and MANTIS identify the \"what\" effectively, they struggle with quantifying the \"how\" and extrapolating this rule to new objects. In contrast, children and adults exhibit much stronger analogical reasoning at all three stages. Additionally, the strongest tested model, GPT-4V, performs better in tasks involving simple visual attributes like color and size, correlating with quicker human adult response times. Conversely, more complex tasks such as number, rotation, and reflection, which necessitate extensive cognitive processing and understanding of the 3D physical world, present more significant challenges. Altogether, these findings highlight the limitations of training models on data that primarily consists of 2D images and text.\n",
            "Score: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See files\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7IHdEI-v2jr",
        "outputId": "0371f193-5035-496f-c3dd-27ebd02ef3fb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "articles2024-07-28__190237972425.html\tarticles_2024-07-28__191209596630.json\tsample_data\n",
            "articles_2024-07-28__190237972425.json\tarticles2024-07-28__191245260107.html\n",
            "articles2024-07-28__191209596630.html\tarticles_2024-07-28__191245260107.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Timer"
      ],
      "metadata": {
        "id": "BBVy06WenHk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_time_whole_single_task = datetime.now()\n",
        "duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "print(f\"Duration to run -> {duration_time}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCnRP_8anHbW",
        "outputId": "63ad53c1-319d-4c08-be16-e7df7946cf07"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run -> 0_min__1.9_sec\n"
          ]
        }
      ]
    }
  ]
}