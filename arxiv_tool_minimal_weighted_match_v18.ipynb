{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XjAYgxPXDnnq",
        "SOMfhOwlr-zu",
        "YepU-A4Fr_J3",
        "ItIQ_onG-IXX",
        "WPnLaV3fpCkv",
        "MaYRyhUgm1ol"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba77c8fe-bdc2-4e48-91f2-a942055118eb"
      },
      "source": [
        "# Arxiv Explorer Tools - minimal weighted match\n",
        "- Fast: ~5-10 sec to run vs. 5-10 min for embedding or TFIDF versions.\n",
        "- multi-topic: use as many pre-set seaches as you want\n",
        "- extracts articles on topics of interest from the too-many-to-look-through daily pages of articles that come out each day.\n",
        "- saves search results to json (for automation later) and html (for easy reading and linking)\n",
        "- saves all articles for archiving\n",
        "- minimal weighted match uses a list of phrases and an weight for each\n",
        "- set score_floor and top_n to filter which results you see\n",
        "- arxiv site reading uses 'beautiful soup'\n",
        "\n",
        "### Setup & Install:\n",
        "- have python installed and use an python env\n",
        "- use a jupyter notebook or script, etc.\n",
        "- for specialty topics you can create extensive weighted search profiles.\n",
        "\n",
        "Note: should be able to run as a script or in a server, but notebooks are useful\n",
        "\n",
        "### See:\n",
        "- https://medium.com/@GeoffreyGordonAshbrook/search-with-non-generative-ai-d0a3cc77164b\n",
        "- https://github.com/lineality/arxiv_explorer_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdea8fa-7a5d-4d32-a88b-1b1f8619e1b3"
      },
      "source": [
        "requirements.txt ->\n",
        "```\n",
        "requests\n",
        "scikit-learn\n",
        "scipy\n",
        "numpy\n",
        "beautifulsoup4\n",
        "```\n",
        "- https://pypi.org/project/beautifulsoup4/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "XjAYgxPXDnnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # standard library\n",
        "import time  # standard library\n",
        "from datetime import datetime  # standard library\n",
        "from bs4 import BeautifulSoup  # pip install beautifulsoup4\n",
        "import requests  # standard library\n",
        "import json  # standard library"
      ],
      "metadata": {
        "id": "djUT2fDyDLJe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e4c5c9be-949c-4c72-b2cf-b26df5316aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4080ca9f-10a5-462d-bca8-e05963283a84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTally time at end.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "Code-Bundle For Time:\n",
        " Commented-out code is for use in different places in the code.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def duration_min_sec(start_time, end_time):\n",
        "\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    duration_seconds = duration.total_seconds()\n",
        "\n",
        "    minutes = int(duration_seconds // 60)\n",
        "    seconds = duration_seconds % 60\n",
        "    time_message = f\"{minutes}_min__{seconds:.1f}_sec\"\n",
        "\n",
        "    return time_message\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Start (and Stop) your Time Tracking:\n",
        "\"\"\"\n",
        "start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "\n",
        "\"\"\"\n",
        "Tally time at end.\n",
        "\"\"\"\n",
        "# # start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "# duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "# print(f\"Duration to run -> {duration_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minimal weighted matching code"
      ],
      "metadata": {
        "id": "SOMfhOwlr-zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An even more simplistic basic key word search (with optional weights)\n",
        "\n",
        "def rank_documents_on_weighted_matches(documents, keyword_weights):\n",
        "    \"\"\"\n",
        "    Ranks documents based on the presence of weighted keywords-phrases.\n",
        "    comparison looks at text without:\n",
        "    - captialization\n",
        "    - spaces\n",
        "    - newlines\n",
        "    - special symbols\n",
        "\n",
        "    Parameters:\n",
        "    documents (list of str): The list of documents to be ranked.\n",
        "    keyword_weights (list of tuple): A list of tuples, where the first element is the keyword and the\n",
        "    second element is the corresponding weight.\n",
        "\n",
        "    Returns:\n",
        "    list of (str, float): A list of tuples, where the first element is the document and the\n",
        "    second element is the ranking score.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    string cleaning steps:\n",
        "    - lower\n",
        "    - strip extra spaces\n",
        "    - remove symbols\n",
        "    - remove newlines\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ranked_documents = []\n",
        "\n",
        "    for document in documents:\n",
        "        score = 0\n",
        "        # Make the document lowercase and strip all symbols, spaces, and newline characters\n",
        "        match_this_cleaned_document = re.sub(r'[^\\w\\s]', '', document.lower()).replace('\\n', '').replace(' ','')\n",
        "        # print(match_this_cleaned_document)\n",
        "        for keyword, weight in keyword_weights:\n",
        "\n",
        "            # Make the keyword lowercase and strip all symbols, spaces, and newline characters\n",
        "            match_this_cleaned_keyword = re.sub(r'[^\\w\\s]', '', keyword.lower()).replace('\\n', '').replace(' ','')\n",
        "            # print(match_this_cleaned_keyword)\n",
        "            # Check if the keyword-phrase is in the document\n",
        "            if match_this_cleaned_keyword in match_this_cleaned_document:\n",
        "                # If the keyword-phrase is in the document, add its weight to the score\n",
        "                score += weight\n",
        "\n",
        "        ranked_documents.append((document, score))\n",
        "\n",
        "    # Sort the documents by their ranking scores in descending order\n",
        "    ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_documents\n",
        "\n",
        "\n",
        "# ################\n",
        "# # Example usage\n",
        "# ################\n",
        "# corpus = [\n",
        "#     \"This is the first document about machine learning.\",\n",
        "#     \"The second document discusses data analysis and visualization.\",\n",
        "#     \"The third document focuses on natural language processing.\",\n",
        "#     \"The fourth document talks about deep learning and neural networks.\",\n",
        "#     \"\"\"to test line breaks\n",
        "#     Emotion mining\n",
        "#      data\n",
        "#     analysis\n",
        "#     Keywords: emotion mining, sentiment analysis, natural disasters, psychology, technological disasters\"\"\",\n",
        "# ]\n",
        "\n",
        "# keyword_weights = [(\"machine learning\", 3), (\"data analysis\", 2), (\"natural language processing\", 4), (\"deep learning\", 5), (\"neural networks\", 6)]\n",
        "\n",
        "# ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "# for document, score in ranked_documents:\n",
        "#     print(f\"Document: {document}\\nScore: {score}\\n\")\n"
      ],
      "metadata": {
        "id": "bqy_ZPvpr-6o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv Explorerer\n"
      ],
      "metadata": {
        "id": "YepU-A4Fr_J3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "19bd0781-5480-4ec0-9709-07330763fd06"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# Arxiv Explorerer\n",
        "###################\n",
        "# step 1: embed the search-phrase\n",
        "# step 2: embed each text\n",
        "# step 3: get scores\n",
        "# step 4: evaluates if score is succss or fail\n",
        "# step 5: if success: do stuff with text\n",
        "\n",
        "\n",
        "# # Imports\n",
        "# from bs4 import BeautifulSoup  # pip install beautifulsoup4\n",
        "# import requests  # standard library\n",
        "# import json  # standard library\n",
        "# from datetime import datetime  # standard library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Article Corpus"
      ],
      "metadata": {
        "id": "ItIQ_onG-IXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_segment_time = datetime.now()\n",
        "\n",
        "#####################\n",
        "# Get Article Corpus\n",
        "#####################\n",
        "\n",
        "# List to hold all article data\n",
        "article_data = []\n",
        "\n",
        "# # Make a request to the website\n",
        "r = requests.get('https://arxiv.org/list/cs/new')\n",
        "\n",
        "url = \"https://arxiv.org/list/cs/new\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# # Find all the articles\n",
        "articles = soup.find_all('dt')\n",
        "\n",
        "# # Find all the titles\n",
        "articles_title = soup.find_all('div', {'class': 'list-title mathjax'})\n",
        "\n",
        "# Find all the subject on the page\n",
        "articles_subject = soup.find_all('dd')\n",
        "\n",
        "\n",
        "###############\n",
        "# make corpus\n",
        "###############\n",
        "\n",
        "corpus = []\n",
        "report_list = []\n",
        "article_dicts = []\n",
        "\n",
        "for this_index, article in enumerate(articles):\n",
        "\n",
        "    ################################################\n",
        "    # Extract each field of data about each article\n",
        "    ################################################\n",
        "\n",
        "    # Extract the title\n",
        "    title = articles_title[this_index].text.split('Title:')[1].strip()\n",
        "\n",
        "    # Extract the subjects\n",
        "    subjects = articles_subject[this_index].find('span', {'class': 'primary-subject'}).text\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "\n",
        "    abstract_p = article.find_next_sibling('dd').find('p', {'class': 'mathjax'})\n",
        "\n",
        "    # Extract the abstract\n",
        "    if abstract_p:\n",
        "        abstract = abstract_p.text.strip()\n",
        "    else:\n",
        "        abstract = \"\"\n",
        "\n",
        "    pdf_link_segment = article.find('a', {'title': 'Download PDF'})['href']\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "    pdf_link = f\"https://arxiv.org{pdf_link_segment}\"\n",
        "    paper_link = f\"https://arxiv.org/abs/{arxiv_id[6:]}\"\n",
        "\n",
        "    # extracted_article_string = title + \" \" + abstract + \" \" + str(subjects)\n",
        "\n",
        "    # assemble corpus\n",
        "    article_characters = f\"{this_index}|||| \"\n",
        "\n",
        "    article_characters += f\"\\n'arxiv_id': {arxiv_id}, \"\n",
        "    article_characters += f\"\\n'paper_link': {paper_link}, \"\n",
        "    article_characters += f\"\\n'pdf_link': {pdf_link}, \"\n",
        "\n",
        "    article_characters += \"\\nTitle: \" + title + \" \"\n",
        "    article_characters += \"\\nSubjects: \" + subjects + \" \"\n",
        "    article_characters += \"\\nAbstract: \" + abstract\n",
        "\n",
        "    ##################################\n",
        "    # Make Bundles (sharing an index)\n",
        "    ##################################\n",
        "\n",
        "    # # add to corpus: just the meaningful text\n",
        "    # corpus.append(extracted_article_string)\n",
        "\n",
        "    # add to simple report_list: includes link and article ID info\n",
        "    report_list.append(article_characters)\n",
        "\n",
        "    # Append the data to the list\n",
        "    article_dicts.append({\n",
        "        'title': title,\n",
        "        'abstract': abstract,\n",
        "        'paper_link': paper_link,\n",
        "        'pdf_link': pdf_link,\n",
        "        'subjects': subjects,\n",
        "        'arxiv_id': arxiv_id,\n",
        "        'article_sequence_index': this_index,\n",
        "    })\n",
        "\n",
        "    # using this because only basic search works\n",
        "    corpus = report_list\n",
        "\n",
        "\n",
        "# # Segment Timer\n",
        "# start_segment_time = datetime.now()\n",
        "end_segment_time = datetime.now()\n",
        "duration_time = duration_min_sec(start_segment_time, end_segment_time)\n",
        "print(f\"Duration to run segment -> {duration_time}\")\n",
        "\n",
        "# ALL Save the data to a JSON file\n",
        "date_time = datetime.now()\n",
        "all_article_dicts_clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "with open(f'all_arxiv_article_dicts_{all_article_dicts_clean_timestamp}.json', 'a') as f:\n",
        "    json.dump(article_dicts, f)"
      ],
      "metadata": {
        "id": "e8FPqO0u-IXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "defdd76d-7af8-4d32-d959-1565c4a2a27f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run segment -> 0_min__5.6_sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (size of corpus)\n",
        "len(corpus)"
      ],
      "metadata": {
        "id": "bve1wNfDBC-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfcacbb-e0be-4533-9111-5c9a0ca25eb1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "612"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# print and save: code"
      ],
      "metadata": {
        "id": "WPnLaV3fpCkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datetime import datetime  # standard library\n",
        "\n",
        "########################################\n",
        "# Filter, Save, & Print the Raw Results\n",
        "########################################\n",
        "# ALL Save the data to a JSON file\n",
        "date_time = datetime.now()\n",
        "all_arxiv_results_clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "all_articles_list = []\n",
        "all_results_json_list = []\n",
        "\n",
        "\n",
        "def result_counter(ranked_documents):\n",
        "    \"\"\"\n",
        "    count non-zero scored results\n",
        "    \"\"\"\n",
        "\n",
        "    result_count = 0\n",
        "\n",
        "    for this_doc in ranked_documents:\n",
        "        score = this_doc[1]\n",
        "\n",
        "        if score != 0:\n",
        "            result_count += 1\n",
        "\n",
        "    return result_count\n",
        "\n",
        "\n",
        "def score_filtered_result_counter(ranked_documents, score_floor=0):\n",
        "    \"\"\"\n",
        "    count non-zero scored results that are greater than or equal to score_floor\n",
        "    \"\"\"\n",
        "\n",
        "    result_count = 0\n",
        "\n",
        "    for this_doc in ranked_documents:\n",
        "        score = this_doc[1]\n",
        "\n",
        "        if score != 0 and score >= score_floor:\n",
        "            result_count += 1\n",
        "\n",
        "    return result_count\n",
        "\n",
        "\n",
        "def print_and_save(ranked_documents, top_n, name_of_set, score_floor=5):\n",
        "    # Posix UTC Seconds\n",
        "    # make readable time\n",
        "    # from datetime import datetime\n",
        "    date_time = datetime.now()\n",
        "    clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    results_json_list = []\n",
        "\n",
        "    for document, score in ranked_documents:\n",
        "\n",
        "        if score >= score_floor:\n",
        "\n",
        "            blurb = f\"Document: {document}\\nScore: {score}\\n\"\n",
        "\n",
        "            print(blurb)\n",
        "\n",
        "        this_index = int(document.split('||||')[0])\n",
        "\n",
        "        data_dict = article_dicts[this_index]\n",
        "\n",
        "        results_json_list.append(data_dict)\n",
        "        all_results_json_list.append(data_dict)\n",
        "\n",
        "        counter += 1\n",
        "        if counter >= top_n:\n",
        "            break\n",
        "\n",
        "    #############\n",
        "    # Write Data\n",
        "    #############\n",
        "\n",
        "    # Save the data to a JSON file\n",
        "    with open(f'{name_of_set}_articles_{clean_timestamp}.json', 'w') as f:\n",
        "        json.dump(results_json_list, f)\n",
        "\n",
        "    # Create an HTML file\n",
        "    html = '<html><body>'\n",
        "    for article in results_json_list:\n",
        "        html += f'<h2><a href=\"{article[\"paper_link\"]}\">{article[\"title\"]}</a></h2>'\n",
        "        html += f'<p>{article[\"abstract\"]}</p>'\n",
        "        html += f'<p>Subjects: {str(article[\"subjects\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"paper_link\"]}\">{article[\"paper_link\"]}</a>'\n",
        "        html += f'<p>paper link: {str(article[\"paper_link\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"pdf_link\"]}\">{article[\"pdf_link\"]}</a>'\n",
        "        html += f'<p>pdf link: {str(article[\"pdf_link\"])}</p>'\n",
        "\n",
        "        html += f'<p>arxiv id: {str(article[\"arxiv_id\"])}</p>'\n",
        "        html += f'<p>article_sequence_index id: {str(article[\"article_sequence_index\"])}</p>'\n",
        "\n",
        "    html += '</body></html>'\n",
        "\n",
        "\n",
        "    # Save the HTML to a file\n",
        "    with open(f'{name_of_set}_articles{clean_timestamp}.html', 'w') as f:\n",
        "        f.write(html)\n",
        "\n",
        "\n",
        "def match_print_save(list_of_lists_of_weights, top_n, score_floor):\n",
        "    date_time = datetime.now()\n",
        "    clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "    counter = 0\n",
        "    for keyword_weights in list_of_lists_of_weights:\n",
        "\n",
        "        ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "        # user first list item as name of set\n",
        "        name_of_set = list_of_lists_of_weights[counter][0][0]\n",
        "\n",
        "        result_quantity = result_counter(ranked_documents)\n",
        "\n",
        "        score_floor_filtered_quantity = score_filtered_result_counter(ranked_documents, score_floor)\n",
        "\n",
        "        this_max_number = top_n\n",
        "\n",
        "        if top_n > result_quantity:\n",
        "            this_max_number = result_quantity\n",
        "\n",
        "        print(f\"\\n\\nSet Name: {name_of_set}\")\n",
        "        print(f\"Total Matches in Set: {result_quantity}\")\n",
        "        print(f\"Matches Above Score-Floor in Set: {score_floor_filtered_quantity}\")\n",
        "        print(clean_timestamp)\n",
        "\n",
        "        print(f\"\\nShowing {score_floor_filtered_quantity} in top-{this_max_number} out of {result_quantity} total results.     -> {score_floor_filtered_quantity} of {this_max_number}/{result_quantity}\")\n",
        "        print(f\"(Ceiling set at {top_n} (top_n) filtered results.)    -> {top_n}\")\n",
        "        print(f\"(Minimum-included-score, 'Score-Floor' set at {score_floor}) -> {score_floor}\\n\\n\")\n",
        "\n",
        "        print_and_save(ranked_documents, top_n, name_of_set, score_floor)\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "        # ALL Save the data to a JSON file\n",
        "        with open(f'all_arxiv_results_{all_arxiv_results_clean_timestamp}.json', 'a') as f:\n",
        "            json.dump(all_results_json_list, f)"
      ],
      "metadata": {
        "id": "peVzbe-Di2xH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multi-set search(es)\n",
        "(optional)"
      ],
      "metadata": {
        "id": "MaYRyhUgm1ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ########\n",
        "# # Batch\n",
        "# ########\n",
        "\n",
        "# # example multi-list\n",
        "\n",
        "# list_of_lists_of_weights = [\n",
        "#     # keyword_weights =\n",
        "#     [\n",
        "#         (\"computer vision\", 3),\n",
        "#         (\"resolution\", 2),\n",
        "#         # (\"natural language processing\", 4),\n",
        "#         # (\"deep learning\", 5),\n",
        "#         (\"neural networks\", 6),\n",
        "#     ],\n",
        "\n",
        "\n",
        "#     # keyword_weights =\n",
        "#     [\n",
        "#         (\"distance measure\", 10),\n",
        "#         (\"similarity measure\", 10),\n",
        "#         (\"vector distance\", 10),\n",
        "#         (\"distance metric\", 10),\n",
        "#         (\"similarity metric\", 10),\n",
        "#         (\"dimension reduction\", 10),\n",
        "\n",
        "\n",
        "#         (\"similarity\", 1),\n",
        "#         (\"distance\", 1),\n",
        "#         (\"metric\", 1),\n",
        "\n",
        "#     ],\n",
        "\n",
        "\n",
        "#     # # keyword_weights =\n",
        "#     # (\"cognitive science\", 2),  # much too broad...\n",
        "#     [\n",
        "#         (\"mental health\", 5),\n",
        "#         (\"psychological health\", 5),\n",
        "#         (\"psycholog\", 2),  # stem vs. lemma\n",
        "\n",
        "\n",
        "#         (\"mental health care\", 3),\n",
        "#         (\"neuroscience\", 2),\n",
        "#         (\"psychological assessment\", 2),\n",
        "#         (\"personality assessment\", 2),\n",
        "#         (\"personality inference\", 2),\n",
        "#         (\"personality traits\", 2),\n",
        "#         (\"personality dimensions\", 2),\n",
        "#         (\"emotion\", 15),\n",
        "#         (\"sports psychology\", 15),\n",
        "#         # (\"\", 2),\n",
        "#         # (\"\", 2),\n",
        "\n",
        "\n",
        "\n",
        "#         # disease terms\n",
        "#         (\"depression\", 5),\n",
        "#         (\"anxiety\", 5),\n",
        "#         (\"mental disorders\", 2),\n",
        "#         (\"social anxiety disorder\", 4),\n",
        "#         (\"mental illness\", 2),\n",
        "#         (\"Major Depressive Disorder\", 2),\n",
        "#         (\"MDD\", 2),\n",
        "#         (\"psychological stressors\", 2),\n",
        "#         (\"cognitive impairment\", 2),\n",
        "#         (\"mci\", 2),\n",
        "#         # (\"\", 2),\n",
        "#         # (\"\", 2),\n",
        "#         # (\"\", 2),\n",
        "\n",
        "#         ],\n",
        "\n",
        "\n",
        "#     # # keyword_weights =\n",
        "#     [\n",
        "#         (\"benchmark\", 5),\n",
        "#         (\"model evaluation\", 5),\n",
        "#         (\"test\", 2),\n",
        "#         (\"measure\", 2),\n",
        "#     ],\n",
        "\n",
        "\n",
        "#     # # keyword_weights =\n",
        "#     [\n",
        "#         (\"training set\", 5),\n",
        "#         (\"synthetic\", 2),\n",
        "#         (\"generate\", 2),\n",
        "#         (\"measure\", 2),\n",
        "#     ],\n",
        "\n",
        "#     # keyword_weights =\n",
        "#     [\n",
        "#         (\"graph\", 5),\n",
        "#         (\"graph generation\", 8),\n",
        "#         (\"subgraph\", 2),\n",
        "#         (\"hierarchical graph\", 2),\n",
        "#         (\"embedding\", 2),\n",
        "#         (\"knowledge graph\", 2),\n",
        "\n",
        "#         (\"graph neural networks\", 2),\n",
        "#         (\"graph representation\", 2),\n",
        "#         (\"node\", 2),\n",
        "#          ## collisions: cryptograph, geograph,\n",
        "#     ],\n",
        "\n",
        "# ]\n",
        "\n",
        "# top_n = 45\n",
        "# score_floor = 3\n",
        "# match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "id": "Sn35USTbt3MM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find top-n articles: use keyword/weights"
      ],
      "metadata": {
        "id": "bt_SeRE_l345"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Manifold Approximation\", 10),\n",
        "        (\"UMAP\", 10),\n",
        "        (\"Uniform Manifold Approximation and Projection\", 10),\n",
        "        (\"Manifold hypothesis\", 10),\n",
        "        (\"dimensionality reduction\", 10),\n",
        "        (\"dimension reduction\", 10),\n",
        "        (\"dimension reduction technique\", 10),\n",
        "\n",
        "        (\"stress\", 1),\n",
        "        (\"Manifold\", 1),\n",
        "        (\"lower-dimensional\", 1),\n",
        "        (\"visualiz\", 1),\n",
        "        (\"projection\", 1),\n",
        "        (\"project\", 1),\n",
        "        (\"dimensionality\", 1),\n",
        "        (\"reduction\", 1),\n",
        "    ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLOy8bu3elSO",
        "outputId": "26553371-f431-420d-a636-857189346697"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Manifold Approximation\n",
            "Total Matches in Set: 77\n",
            "Matches Above Score-Floor in Set: 15\n",
            "2024-09-12__120154747042\n",
            "\n",
            "Showing 15 in top-45 out of 77 total results.     -> 15 of 45/77\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 235|||| \n",
            "'arxiv_id': arXiv:2409.07257, \n",
            "'paper_link': https://arxiv.org/abs/2409.07257, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07257, \n",
            "Title: TopoMap++: A faster and more space efficient technique to compute projections with topological guarantees \n",
            "Subjects: Graphics (cs.GR) \n",
            "Abstract: High-dimensional data, characterized by many features, can be difficult to visualize effectively. Dimensionality reduction techniques, such as PCA, UMAP, and t-SNE, address this challenge by projecting the data into a lower-dimensional space while preserving important relationships. TopoMap is another technique that excels at preserving the underlying structure of the data, leading to interpretable visualizations. In particular, TopoMap maps the high-dimensional data into a visual space, guaranteeing that the 0-dimensional persistence diagram of the Rips filtration of the visual space matches the one from the high-dimensional data. However, the original TopoMap algorithm can be slow and its layout can be too sparse for large and complex datasets. In this paper, we propose three improvements to TopoMap: 1) a more space-efficient layout, 2) a significantly faster implementation, and 3) a novel TreeMap-based representation that makes use of the topological hierarchy to aid the exploration of the projections. These advancements make TopoMap, now referred to as TopoMap++, a more powerful tool for visualizing high-dimensional data which we demonstrate through different use case scenarios.\n",
            "Score: 26\n",
            "\n",
            "Document: 254|||| \n",
            "'arxiv_id': arXiv:2409.07306, \n",
            "'paper_link': https://arxiv.org/abs/2409.07306, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07306, \n",
            "Title: Visual Compositional Data Analytics for Spatial Transcriptomics \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: For the Bio+Med-Vis Challenge 2024, we propose a visual analytics system as a redesign for the scatter pie chart visualization of cell type proportions of spatial transcriptomics data. Our design uses three linked views: a view of the histological image of the tissue, a stacked bar chart showing cell type proportions of the spots, and a scatter plot showing a dimensionality reduction of the multivariate proportions. Furthermore, we apply a compositional data analysis framework, the Aitchison geometry, to the proportions for dimensionality reduction and $k$-means clustering. Leveraging brushing and linking, the system allows one to explore and uncover patterns in the cell type mixtures and relate them to their spatial locations on the cellular tissue. This redesign shifts the pattern recognition workload from the human visual system to computational methods commonly used in visual analytics. We provide the code and setup instructions of our visual analytics system on GitHub (this https URL).\n",
            "Score: 13\n",
            "\n",
            "Document: 58|||| \n",
            "'arxiv_id': arXiv:2409.06846, \n",
            "'paper_link': https://arxiv.org/abs/2409.06846, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06846, \n",
            "Title: Stratospheric aerosol source inversion: Noise, variability, and uncertainty quantification \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Stratospheric aerosols play an important role in the earth system and can affect the climate on timescales of months to years. However, estimating the characteristics of partially observed aerosol injections, such as those from volcanic eruptions, is fraught with uncertainties. This article presents a framework for stratospheric aerosol source inversion which accounts for background aerosol noise and earth system internal variability via a Bayesian approximation error approach. We leverage specially designed earth system model simulations using the Energy Exascale Earth System Model (E3SM). A comprehensive framework for data generation, data processing, dimension reduction, operator learning, and Bayesian inversion is presented where each component of the framework is designed to address particular challenges in stratospheric modeling on the global scale. We present numerical results using synthesized observational data to rigorously assess the ability of our approach to estimate aerosol sources and associate uncertainty with those estimates.\n",
            "Score: 11\n",
            "\n",
            "Document: 227|||| \n",
            "'arxiv_id': arXiv:2409.07242, \n",
            "'paper_link': https://arxiv.org/abs/2409.07242, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07242, \n",
            "Title: Orthogonal Mode Decomposition for Finite Discrete Signals \n",
            "Subjects: Systems and Control (eess.SY) \n",
            "Abstract: In this paper, an orthogonal mode decomposition method is proposed to decompose ffnite length real signals on both the real and imaginary axes of the complex plane. The interpolation function space of ffnite length discrete signal is constructed, and the relationship between the dimensionality of the interpolation function space and its subspaces and the band width of the interpolation function is analyzed. It is proved that the intrinsic mode is actually the narrow band signal whose intrinsic instantaneous frequency is always positive (or always negative). Thus, the eigenmode decomposition problem is transformed into the orthogonal projection problem of interpolation function space to its low frequency subspace or narrow band subspace. Different from the existing mode decomposition methods, the orthogonal modal decomposition is a local time-frequency domain algorithm. Each operation extracts a speciffc mode. The global decomposition results obtained under the precise deffnition of eigenmodes have uniqueness and orthogonality. The computational complexity of the orthogonal mode decomposition method is also much smaller than that of the existing mode decomposition methods.\n",
            "Score: 3\n",
            "\n",
            "Document: 322|||| \n",
            "'arxiv_id': arXiv:2409.06714, \n",
            "'paper_link': https://arxiv.org/abs/2409.06714, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06714, \n",
            "Title: FCDM: Sparse-view Sinogram Inpainting with Frequency Domain Convolution Enhanced Diffusion Models \n",
            "Subjects: Image and Video Processing (eess.IV) \n",
            "Abstract: Reducing the radiation dose in computed tomography (CT) is crucial, but it often results in sparse-view CT, where the number of available projections is significantly reduced. This reduction in projection data makes it challenging to accurately reconstruct high-quality CT images. In this condition, a sinogram, which is a collection of these projections, becomes incomplete. Sinogram inpainting then becomes essential because it enables accurate image reconstruction with limited projections. Existing models performing well on conventional RGB images for inpainting mostly fail in the case of sinograms. Further, these models usually do not make full use of unique properties, e.g., frequency features and absorption characteristics in the sinogram, and cannot handle large-area masks and complex real-world projections well.\n",
            "To address these limitations, we propose a novel model called the Frequency Convolution Diffusion Model (FCDM). It employs frequency domain convolutions to extract frequency information from various angles and capture the intricate relationships between these angles, which is essential for high-quality CT reconstruction. We also design a specific loss function based on the unique properties of a sinogram to maintain the consistency in physical properties, which allows the model to learn more effectively even in larger mask areas. We compare FCDM using both simulations and real data with nine inpainting models examples, among which two are designed for sinogram and seven for RGB. The results indicate that our model significantly improves the quality of the inpainted sinograms in terms of both visually and quantitatively, with an SSIM of more than 0.95 and PSNR of more than 30, achieving up to a 33% improvement in SSIM and a 29% improvement in PSNR compared to the baseline.\n",
            "Score: 3\n",
            "\n",
            "Document: 20|||| \n",
            "'arxiv_id': arXiv:2409.06734, \n",
            "'paper_link': https://arxiv.org/abs/2409.06734, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06734, \n",
            "Title: ARIM-mdx Data System: Towards a Nationwide Data Platform for Materials Science \n",
            "Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) \n",
            "Abstract: In modern materials science, effective and high-volume data management across leading-edge experimental facilities and world-class supercomputers is indispensable for cutting-edge research. Such facilities and supercomputers are typically utilized by a wide range of researchers across different fields and organizations in academia and industry. However, existing integrated systems that handle data from these resources have primarily focused just on smaller-scale cross-institutional or single-domain operations. As a result, they often lack the scalability, efficiency, agility, and interdisciplinarity, needed for handling substantial volumes of data from various researchers.\n",
            "In this paper, we introduce ARIM-mdx data system, a nationwide data platform for materials science in Japan. The platform involves 8 universities and institutes all over Japan through the governmental materials science project. Currently in its trial phase, the ARIM-mdx data system is utilized by over 800 researchers from around 140 organizations in academia and industry, being intended to gradually expand its reach. The system employs a hybrid architecture, combining a peta-scale dedicated storage system for security and stability with a high-performance academic cloud for efficiency and scalability. Through direct network connections between them, the system achieves 4.7x latency reduction compared to a conventional approach, resulting in near real-time interactive data analysis. It also utilizes specialized IoT devices for secure data transfer from equipment computers and connects to multiple supercomputers via an academic ultra-fast network, achieving 4x faster data transfer compared to the public Internet. The ARIM-mdx data system, as a pioneering nationwide data platform, has the potential to contribute to the creation of new research communities and accelerates innovations.\n",
            "Score: 2\n",
            "\n",
            "Document: 140|||| \n",
            "'arxiv_id': arXiv:2409.07037, \n",
            "'paper_link': https://arxiv.org/abs/2409.07037, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07037, \n",
            "Title: A Reynolds-semi-robust and pressure robust Hybrid High-Order method for the time dependent incompressible Navier--Stokes equations on general meshes \n",
            "Subjects: Numerical Analysis (math.NA) \n",
            "Abstract: In this work we develop and analyze a Reynolds-semi-robust and pressure-robust Hybrid High-Order (HHO) discretization of the incompressible Navier--Stokes equations. Reynolds-semi-robustness refers to the fact that, under suitable regularity assumptions, the right-hand side of the velocity error estimate does not depend on the inverse of the viscosity. This property is obtained here through a penalty term which involves a subtle projection of the convective term on a subgrid space constructed element by element. The estimated convergence order for the $L^\\infty(L^2)$- and $L^2(\\text{energy})$-norm of the velocity is $h^{k+\\frac12}$, which matches the best results for continuous and discontinuous Galerkin methods and corresponds to the one expected for HHO methods in convection-dominated regimes. Two-dimensional numerical results on a variety of polygonal meshes complete the exposition.\n",
            "Score: 2\n",
            "\n",
            "Document: 204|||| \n",
            "'arxiv_id': arXiv:2409.07189, \n",
            "'paper_link': https://arxiv.org/abs/2409.07189, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07189, \n",
            "Title: A Perspective on AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Molecular dynamics simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently been developed as a 'human-in-the-loop' strategy, which leverages high-performance computing to accelerate the researcher's ability to solve the hyperdimensional sampling problem. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular motion, iMD-VR enables researchers and students to efficiently and intuitively explore and navigate these complex, high-dimensional systems. iMD-VR platforms offer a unique opportunity to quickly generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the possibility of employing user-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL is an important technique in robotics that enables agents to mimic complex behaviors from expert demonstrations, thus circumventing the need for explicit programming or intricate reward design. We review the utilization of IL for manipulation tasks in robotics and discuss how iMD-VR recordings could be used to train IL models for solving specific molecular 'tasks'. We then investigate how such approaches could be applied to the data captured from iMD-VR recordings. Finally, we outline the future research directions and potential challenges of using AI agents to augment human expertise to efficiently navigate conformational spaces, highlighting how this approach could provide valuable insight across domains such as materials science, protein engineering, and computer-aided drug design.\n",
            "Score: 2\n",
            "\n",
            "Document: 213|||| \n",
            "'arxiv_id': arXiv:2409.07208, \n",
            "'paper_link': https://arxiv.org/abs/2409.07208, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07208, \n",
            "Title: Almost-catalytic Computation \n",
            "Subjects: Computational Complexity (cs.CC) \n",
            "Abstract: Designing algorithms for space bounded models with restoration requirements on the space used by the algorithm is an important challenge posed about the catalytic computation model introduced by Buhrman et al. (2014). Motivated by the scenarios where we do not need to restore unless is useful, we define $ACL(A)$ to be the class of languages that can be accepted by almost-catalytic Turing machines with respect to $A$ (which we call the catalytic set), that uses at most $c\\log n$ work space and $n^c$ catalytic space.\n",
            "We show that if there are almost-catalytic algorithms for a problem with catalytic set as $A \\subseteq \\Sigma^*$ and its complement respectively, then the problem can be solved by a ZPP algorithm. Using this, we derive that to design catalytic algorithms, it suffices to design almost-catalytic algorithms where the catalytic set is the set of strings of odd weight ($PARITY$). Towards this, we consider two complexity measures of the set $A$ which are maximized for $PARITY$ - random projection complexity (${\\cal R}(A)$) and the subcube partition complexity (${\\cal P}(A)$).\n",
            "By making use of error-correcting codes, we show that for all $k \\ge 1$, there is a language $A_k \\subseteq \\Sigma^*$ such that $DSPACE(n^k) \\subseteq ACL(A_k)$ where for every $m \\ge 1$, $\\mathcal{R}(A_k \\cap \\{0,1\\}^m) \\ge \\frac{m}{4}$ and $\\mathcal{P}(A_k \\cap \\{0,1\\}^m)=2^{m/4}$. This contrasts the catalytic machine model where it is unclear if it can accept all languages in $DSPACE(\\log^{1+\\epsilon} n)$ for any $\\epsilon > 0$.\n",
            "Improving the partition complexity of the catalytic set $A$ further, we show that for all $k \\ge 1$, there is a $A_k \\subseteq \\{0,1\\}^*$ such that $\\mathsf{DSPACE}(\\log^k n) \\subseteq ACL(A_k)$ where for every $m \\ge 1$, $\\mathcal{R}(A_k \\cap \\{0,1\\}^m) \\ge \\frac{m}{4}$ and $\\mathcal{P}(A_k \\cap \\{0,1\\}^m)=2^{m/4+\\Omega(\\log m)}$.\n",
            "Score: 2\n",
            "\n",
            "Document: 358|||| \n",
            "'arxiv_id': arXiv:2409.07171, \n",
            "'paper_link': https://arxiv.org/abs/2409.07171, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07171, \n",
            "Title: AC-IND: Sparse CT reconstruction based on attenuation coefficient estimation and implicit neural distribution \n",
            "Subjects: Image and Video Processing (eess.IV) \n",
            "Abstract: Computed tomography (CT) reconstruction plays a crucial role in industrial nondestructive testing and medical diagnosis. Sparse view CT reconstruction aims to reconstruct high-quality CT images while only using a small number of projections, which helps to improve the detection speed of industrial assembly lines and is also meaningful for reducing radiation in medical scenarios. Sparse CT reconstruction methods based on implicit neural representations (INRs) have recently shown promising performance, but still produce artifacts because of the difficulty of obtaining useful prior information. In this work, we incorporate a powerful prior: the total number of material categories of objects. To utilize the prior, we design AC-IND, a self-supervised method based on Attenuation Coefficient Estimation and Implicit Neural Distribution. Specifically, our method first transforms the traditional INR from scalar mapping to probability distribution mapping. Then we design a compact attenuation coefficient estimator initialized with values from a rough reconstruction and fast segmentation. Finally, our algorithm finishes the CT reconstruction by jointly optimizing the estimator and the generated distribution. Through experiments, we find that our method not only outperforms the comparative methods in sparse CT reconstruction but also can automatically generate semantic segmentation maps.\n",
            "Score: 2\n",
            "\n",
            "Document: 374|||| \n",
            "'arxiv_id': arXiv:2009.04552, \n",
            "'paper_link': https://arxiv.org/abs/2009.04552, \n",
            "'pdf_link': https://arxiv.org/pdf/2009.04552, \n",
            "Title: KNN-DBSCAN: a DBSCAN in high dimensions \n",
            "Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) \n",
            "Abstract: Clustering is a fundamental task in machine learning. One of the most successful and broadly used algorithms is DBSCAN, a density-based clustering algorithm. DBSCAN requires $\\epsilon$-nearest neighbor graphs of the input dataset, which are computed with range-search algorithms and spatial data structures like KD-trees. Despite many efforts to design scalable implementations for DBSCAN, existing work is limited to low-dimensional datasets, as constructing $\\epsilon$-nearest neighbor graphs is expensive in high-dimensions. In this paper, we modify DBSCAN to enable use of $\\kappa$-nearest neighbor graphs of the input dataset. The $\\kappa$-nearest neighbor graphs are constructed using approximate algorithms based on randomized projections. Although these algorithms can become inaccurate or expensive in high-dimensions, they possess a much lower memory overhead than constructing $\\epsilon$-nearest neighbor graphs. We delineate the conditions under which $k$NN-DBSCAN produces the same clustering as DBSCAN. We also present an efficient parallel implementation of the overall algorithm using OpenMP for shared memory and MPI for distributed memory parallelism. We present results on up to 16 billion points in 20 dimensions, and perform weak and strong scaling studies using synthetic data. Our code is efficient in both low and high dimensions. We can cluster one billion points in 3D in less than one second on 28K cores on the Frontera system at the Texas Advanced Computing Center (TACC). In our largest run, we cluster 65 billion points in 20 dimensions in less than 40 seconds using 114,688 x86 cores on TACC's Frontera system. Also, we compare with a state of the art parallel DBSCAN code; on 20d/4M point dataset, our code is up to 37$\\times$ faster.\n",
            "Score: 2\n",
            "\n",
            "Document: 375|||| \n",
            "'arxiv_id': arXiv:2108.06339, \n",
            "'paper_link': https://arxiv.org/abs/2108.06339, \n",
            "'pdf_link': https://arxiv.org/pdf/2108.06339, \n",
            "Title: Approximation and generalization properties of the random projection classification method \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: The generalization gap of a classifier is related to the complexity of the set of functions among which the classifier is chosen. We study a family of low-complexity classifiers consisting of thresholding a random one-dimensional feature. The feature is obtained by projecting the data on a random line after embedding it into a higher-dimensional space parametrized by monomials of order up to k. More specifically, the extended data is projected n-times and the best classifier among those n, based on its performance on training data, is chosen. We show that this type of classifier is extremely flexible as, given full knowledge of the class conditional densities, under mild conditions, the error of these classifiers would converge to the optimal (Bayes) error as k and n go to infinity. We also bound the generalization gap of the random classifiers. In general, these bounds are better than those for any classifier with VC dimension greater than O(ln n). In particular, the bounds imply that, unless the number of projections n is extremely large, the generalization gap of the random projection approach is significantly smaller than that of a linear classifier in the extended space. Thus, for certain classification problems (e.g., those with a large Rashomon ratio), there is a potntially large gain in generalization properties by selecting parameters at random, rather than selecting the best one amongst the class.\n",
            "Score: 2\n",
            "\n",
            "Document: 417|||| \n",
            "'arxiv_id': arXiv:2311.15143, \n",
            "'paper_link': https://arxiv.org/abs/2311.15143, \n",
            "'pdf_link': https://arxiv.org/pdf/2311.15143, \n",
            "Title: Reduced Augmentation Implicit Low-rank (RAIL) integrators for advection-diffusion and Fokker-Planck models \n",
            "Subjects: Numerical Analysis (math.NA) \n",
            "Abstract: This paper introduces a novel computational approach termed the Reduced Augmentation Implicit Low-rank (RAIL) method by investigating two predominant research directions in low-rank solutions to time-dependent partial differential equations (PDEs): dynamical low-rank (DLR), and step and truncation (SAT) tensor methods. The RAIL method, along with the development of the SAT approach, is designed to enhance the efficiency of traditional full-rank implicit solvers from method-of-lines discretizations of time-dependent PDEs, while maintaining accuracy and stability. We consider spectral methods for spatial discretization, and diagonally implicit Runge-Kutta (DIRK) and implicit-explicit (IMEX) RK methods for time discretization. The efficiency gain is achieved by investigating low-rank structures within solutions at each RK stage using a singular value decomposition (SVD). In particular, we develop a reduced augmentation procedure to predict the basis functions to construct projection subspaces. This procedure balances algorithm accuracy and efficiency by incorporating as many bases as possible from previous RK stages and predictions, and by optimizing the basis representation through SVD truncation. As such, one can form implicit schemes for updating basis functions in a dimension-by-dimension manner, similar in spirit to the K-L step in the DLR framework. We also apply a globally mass conservative post-processing step at the end of each RK stage. We validate the RAIL method through numerical simulations of advection-diffusion problems and a Fokker-Planck model, showcasing its ability to efficiently handle time-dependent PDEs while maintaining global mass conservation. Our approach generalizes and bridges the DLR and SAT approaches, offering a comprehensive framework for efficiently and accurately solving time-dependent PDEs with implicit treatment.\n",
            "Score: 2\n",
            "\n",
            "Document: 525|||| \n",
            "'arxiv_id': arXiv:2408.10824, \n",
            "'paper_link': https://arxiv.org/abs/2408.10824, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.10824, \n",
            "Title: Emerging clean technologies: policy-driven cost reductions, implications and perspectives \n",
            "Subjects: Systems and Control (eess.SY) \n",
            "Abstract: Hydrogen production from water electrolysis, direct air capture (DAC), and synthetic kerosene derived from hydrogen and CO2 (`e-kerosene') are expected to play an important role in global decarbonization efforts. So far, the economics of these nascent technologies hamper their market diffusion. However, a wave of recent policy support in the United States, Europe, China, and elsewhere is anticipated to drive their commercial liftoff and bring their costs down. To this end, we evaluate the potential cost reductions driven by policy-induced scale-up of these emerging technologies through 2030 using an experience curves approach accounting for both local and global learning effects. We then analyze the consequences of projected cost declines on the competitiveness of these nascent technologies compared to conventional fossil alternatives, where applicable, and highlight some of the tradeoffs associated with their expansion. Our findings indicate that enacted policies could lead to substantial capital cost reductions for electrolyzers. Nevertheless, electrolytic hydrogen production at $1-2/kg would still require some form of policy support. Given expected costs and experience curves, it is unlikely that liquid solvent DAC (L-DAC) scale-up will bring removal costs to stated targets of $100/tCO2, though a $200/tCO2 may eventually be within reach. We also underscore the importance of tackling methane leakage for natural gas-powered L-DAC: unmitigated leaks amplify net removal costs, exacerbate the investment requirements to reach targeted costs, and cast doubt on L-DAC's role in the clean energy transition. Lastly, despite reductions in electrolysis and L-DAC costs, e-kerosene remains considerably more expensive than fossil jet fuel. The economics of e-kerosene and the resources required for production raise questions about the fuel's ultimate viability as a decarbonization tool for aviation.\n",
            "Score: 2\n",
            "\n",
            "Document: 552|||| \n",
            "'arxiv_id': arXiv:2409.04751, \n",
            "'paper_link': https://arxiv.org/abs/2409.04751, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.04751, \n",
            "Title: Fisheye-GS: Lightweight and Extensible Gaussian Splatting Module for Fisheye Cameras \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Recently, 3D Gaussian Splatting (3DGS) has garnered attention for its high fidelity and real-time rendering. However, adapting 3DGS to different camera models, particularly fisheye lenses, poses challenges due to the unique 3D to 2D projection calculation. Additionally, there are inefficiencies in the tile-based splatting, especially for the extreme curvature and wide field of view of fisheye lenses, which are crucial for its broader real-life applications. To tackle these challenges, we introduce Fisheye-GS.This innovative method recalculates the projection transformation and its gradients for fisheye cameras. Our approach can be seamlessly integrated as a module into other efficient 3D rendering methods, emphasizing its extensibility, lightweight nature, and modular design. Since we only modified the projection component, it can also be easily adapted for use with different camera models. Compared to methods that train after undistortion, our approach demonstrates a clear improvement in visual quality.\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 3\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"distance measure\", 10),\n",
        "        (\"similarity measure\", 10),\n",
        "        (\"vector distance\", 10),\n",
        "        (\"distance metric\", 10),\n",
        "        (\"similarity metric\", 10),\n",
        "        (\"dimension reduction\", 10),\n",
        "\n",
        "        (\"similarity\", 1),\n",
        "        (\"distance\", 1),\n",
        "        (\"metric\", 1),\n",
        "    ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HOGJ_6VhZtO",
        "outputId": "a613ddd6-235b-42bc-ec72-226b1690057f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: distance measure\n",
            "Total Matches in Set: 105\n",
            "Matches Above Score-Floor in Set: 4\n",
            "2024-09-12__120155184812\n",
            "\n",
            "Showing 4 in top-45 out of 105 total results.     -> 4 of 45/105\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 3) -> 3\n",
            "\n",
            "\n",
            "Document: 123|||| \n",
            "'arxiv_id': arXiv:2409.06997, \n",
            "'paper_link': https://arxiv.org/abs/2409.06997, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06997, \n",
            "Title: What is the Right Notion of Distance between Predict-then-Optimize Tasks? \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Comparing datasets is a fundamental task in machine learning, essential for various learning paradigms; from evaluating train and test datasets for model generalization to using dataset similarity for detecting data drift. While traditional notions of dataset distances offer principled measures of similarity, their utility has largely been assessed through prediction error minimization. However, in Predict-then-Optimize (PtO) frameworks, where predictions serve as inputs for downstream optimization tasks, model performance is measured through decision regret minimization rather than prediction error minimization. In this work, we (i) show that traditional dataset distances, which rely solely on feature and label dimensions, lack informativeness in the PtO context, and (ii) propose a new dataset distance that incorporates the impacts of downstream decisions. Our results show that this decision-aware dataset distance effectively captures adaptation success in PtO contexts, providing a PtO adaptation bound in terms of dataset distance. Empirically, we show that our proposed distance measure accurately predicts transferability across three different PtO tasks from the literature.\n",
            "Score: 12\n",
            "\n",
            "Document: 192|||| \n",
            "'arxiv_id': arXiv:2409.07160, \n",
            "'paper_link': https://arxiv.org/abs/2409.07160, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07160, \n",
            "Title: Distance Measurement for UAVs in Deep Hazardous Tunnels \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: The localization of Unmanned aerial vehicles (UAVs) in deep tunnels is extremely challenging due to their inaccessibility and hazardous environment. Conventional outdoor localization techniques (such as using GPS) and indoor localization techniques (such as those based on WiFi, Infrared (IR), Ultra-Wideband, etc.) do not work in deep tunnels. We are developing a UAV-based system for the inspection of defects in the Deep Tunnel Sewerage System (DTSS) in Singapore. To enable the UAV localization in the DTSS, we have developed a distance measurement module based on the optical flow technique. However, the standard optical flow technique does not work well in tunnels with poor lighting and a lack of features. Thus, we have developed an enhanced optical flow algorithm with prediction, to improve the distance measurement for UAVs in deep hazardous tunnels.\n",
            "Score: 11\n",
            "\n",
            "Document: 58|||| \n",
            "'arxiv_id': arXiv:2409.06846, \n",
            "'paper_link': https://arxiv.org/abs/2409.06846, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06846, \n",
            "Title: Stratospheric aerosol source inversion: Noise, variability, and uncertainty quantification \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Stratospheric aerosols play an important role in the earth system and can affect the climate on timescales of months to years. However, estimating the characteristics of partially observed aerosol injections, such as those from volcanic eruptions, is fraught with uncertainties. This article presents a framework for stratospheric aerosol source inversion which accounts for background aerosol noise and earth system internal variability via a Bayesian approximation error approach. We leverage specially designed earth system model simulations using the Energy Exascale Earth System Model (E3SM). A comprehensive framework for data generation, data processing, dimension reduction, operator learning, and Bayesian inversion is presented where each component of the framework is designed to address particular challenges in stratospheric modeling on the global scale. We present numerical results using synthesized observational data to rigorously assess the ability of our approach to estimate aerosol sources and associate uncertainty with those estimates.\n",
            "Score: 10\n",
            "\n",
            "Document: 492|||| \n",
            "'arxiv_id': arXiv:2406.11346, \n",
            "'paper_link': https://arxiv.org/abs/2406.11346, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.11346, \n",
            "Title: WaDec: Decompiling WebAssembly Using Large Language Model \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: WebAssembly (abbreviated Wasm) has emerged as a cornerstone of web development, offering a compact binary format that allows high-performance applications to run at near-native speeds in web browsers. Despite its advantages, Wasm's binary nature presents significant challenges for developers and researchers, particularly regarding readability when debugging or analyzing web applications. Therefore, effective decompilation becomes crucial. Unfortunately, traditional decompilers often struggle with producing readable outputs. While some large language model (LLM)-based decompilers have shown good compatibility with general binary files, they still face specific challenges when dealing with Wasm. In this paper, we introduce a novel approach, WaDec, which is the first use of a fine-tuned LLM to interpret and decompile Wasm binary code into a higher-level, more comprehensible source code representation. The LLM was meticulously fine-tuned using a specialized dataset of wat-c code snippets, employing self-supervised learning techniques. This enables WaDec to effectively decompile not only complete wat functions but also finer-grained wat code snippets. Our experiments demonstrate that WaDec markedly outperforms current state-of-the-art tools, offering substantial improvements across several metrics. It achieves a code inflation rate of only 3.34%, a dramatic 97% reduction compared to the state-of-the-art's 116.94%. Unlike baselines' output that cannot be directly compiled or executed, WaDec maintains a recompilability rate of 52.11%, a re-execution rate of 43.55%, and an output consistency of 27.15%. Additionally, it significantly exceeds state-of-the-art performance in AST edit distance similarity by 185%, cyclomatic complexity by 8%, and cosine similarity by 41%, achieving an average code similarity above 50%.\n",
            "Score: 3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 3\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"parametric\", 10),\n",
        "    ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAncfvGn6URQ",
        "outputId": "9d800724-04cf-4386-922d-2fd98e7be10b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: parametric\n",
            "Total Matches in Set: 7\n",
            "Matches Above Score-Floor in Set: 7\n",
            "2024-09-12__120155506421\n",
            "\n",
            "Showing 7 in top-7 out of 7 total results.     -> 7 of 7/7\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 3) -> 3\n",
            "\n",
            "\n",
            "Document: 289|||| \n",
            "'arxiv_id': arXiv:2409.07394, \n",
            "'paper_link': https://arxiv.org/abs/2409.07394, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07394, \n",
            "Title: AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Our experiments across four models on six diverse question-answering (QA) datasets and three summarization tasks demonstrate that our training-free adaptive method consistently outperforms other decoding methods on QA, with average accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 5.59 (AlignScore). Furthermore, our analysis shows that while decoding with contrastive baselines hurts performance when conflict is absent, AdaCAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.\n",
            "Score: 10\n",
            "\n",
            "Document: 336|||| \n",
            "'arxiv_id': arXiv:2409.06890, \n",
            "'paper_link': https://arxiv.org/abs/2409.06890, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06890, \n",
            "Title: Learning Deep Kernels for Non-Parametric Independence Testing \n",
            "Subjects: Machine Learning (stat.ML) \n",
            "Abstract: The Hilbert-Schmidt Independence Criterion (HSIC) is a powerful tool for nonparametric detection of dependence between random variables. It crucially depends, however, on the selection of reasonable kernels; commonly-used choices like the Gaussian kernel, or the kernel that yields the distance covariance, are sufficient only for amply sized samples from data distributions with relatively simple forms of dependence. We propose a scheme for selecting the kernels used in an HSIC-based independence test, based on maximizing an estimate of the asymptotic test power. We prove that maximizing this estimate indeed approximately maximizes the true power of the test, and demonstrate that our learned kernels can identify forms of structured dependence between random variables in various experiments.\n",
            "Score: 10\n",
            "\n",
            "Document: 392|||| \n",
            "'arxiv_id': arXiv:2303.05471, \n",
            "'paper_link': https://arxiv.org/abs/2303.05471, \n",
            "'pdf_link': https://arxiv.org/pdf/2303.05471, \n",
            "Title: Exploring New Topologies for the Theory of Clones \n",
            "Subjects: Logic in Computer Science (cs.LO) \n",
            "Abstract: Clones of operations of arity omega (referred to as omega-operations) have been employed by Neumann to represent varieties of infinitary algebras defined by operations of at most arity omega. More recently, clone algebras have been introduced to study clones of functions, including omega-operations, within the framework of one-sorted universal algebra. Additionally, polymorphisms of arity omega, which are omega-operations preserving the relations of a given first-order structure, have recently been used to establish model theory results with applications in the field of complexity of CSP problems.\n",
            "In this paper, we undertake a topological and algebraic study of polymorphisms of arity omega and their corresponding invariant relations. Given a set A and a Boolean ideal X on the set of omega-sequences of elements of A, we propose a method to endow the set of omega-operations on A with a topology, which we refer to as X-topology. Notably, the topology of pointwise convergence can be retrieved as a special case of this approach. Polymorphisms and invariant relations are then defined parametrically, with respect to the X-topology. We characterise the X-closed clones of omega-operations in terms of polymorphisms and invariant relations of arity omega, and present a method to relate those infinitary invariant relation and polymorphisms to the classical (finitary) Inv-Pol.\n",
            "Score: 10\n",
            "\n",
            "Document: 446|||| \n",
            "'arxiv_id': arXiv:2403.12886, \n",
            "'paper_link': https://arxiv.org/abs/2403.12886, \n",
            "'pdf_link': https://arxiv.org/pdf/2403.12886, \n",
            "Title: EmoVOCA: Speech-Driven Emotional 3D Talking Heads \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available.\n",
            "Score: 10\n",
            "\n",
            "Document: 511|||| \n",
            "'arxiv_id': arXiv:2407.17992, \n",
            "'paper_link': https://arxiv.org/abs/2407.17992, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.17992, \n",
            "Title: Amortized Active Learning for Nonparametric Functions \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Active learning (AL) is a sequential learning scheme aiming to select the most informative data. AL reduces data consumption and avoids the cost of labeling large amounts of data. However, AL trains the model and solves an acquisition optimization for each selection. It becomes expensive when the model training or acquisition optimization is challenging. In this paper, we focus on active nonparametric function learning, where the gold standard Gaussian process (GP) approaches suffer from cubic time complexity. We propose an amortized AL method, where new data are suggested by a neural network which is trained up-front without any real data (Figure 1). Our method avoids repeated model training and requires no acquisition optimization during the AL deployment. We (i) utilize GPs as function priors to construct an AL simulator, (ii) train an AL policy that can zero-shot generalize from simulation to real learning problems of nonparametric functions and (iii) achieve real-time data selection and comparable learning performances to time-consuming baseline methods.\n",
            "Score: 10\n",
            "\n",
            "Document: 553|||| \n",
            "'arxiv_id': arXiv:2409.04760, \n",
            "'paper_link': https://arxiv.org/abs/2409.04760, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.04760, \n",
            "Title: Training-Free Point Cloud Recognition Based on Geometric and Semantic Information Fusion \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The trend of employing training-free methods for point cloud recognition is becoming increasingly popular due to its significant reduction in computational resources and time costs. However, existing approaches are limited as they typically extract either geometric or semantic features. To address this limitation, we are the first to propose a novel training-free method that integrates both geometric and semantic features. For the geometric branch, we adopt a non-parametric strategy to extract geometric features. In the semantic branch, we leverage a model aligned with text features to obtain semantic features. Additionally, we introduce the GFE module to complement the geometric information of point clouds and the MFF module to improve performance in few-shot settings. Experimental results demonstrate that our method outperforms existing state-of-the-art training-free approaches on mainstream benchmark datasets, including ModelNet and ScanObiectNN.\n",
            "Score: 10\n",
            "\n",
            "Document: 591|||| \n",
            "'arxiv_id': arXiv:2405.00385, \n",
            "'paper_link': https://arxiv.org/abs/2405.00385, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.00385, \n",
            "Title: Variational Bayesian Methods for a Tree-Structured Stick-Breaking Process Mixture of Gaussians by Application of the Bayes Codes for Context Tree Models \n",
            "Subjects: Machine Learning (stat.ML) \n",
            "Abstract: The tree-structured stick-breaking process (TS-SBP) mixture model is a non-parametric Bayesian model that can represent tree-like hierarchical structures among the mixture components. For TS-SBP mixture models, only a Markov chain Monte Carlo (MCMC) method has been proposed and any variational Bayesian (VB) methods has not been proposed. In general, MCMC methods are computationally more expensive than VB methods. Therefore, we require a large computational cost to learn the TS-SBP mixture model. In this paper, we propose a learning algorithm with less computational cost for the TS-SBP mixture of Gaussians by using the VB method under an assumption of finite tree width and depth. When constructing such VB method, the main challenge is efficient calculation of a sum over all possible trees. To solve this challenge, we utilizes a subroutine in the Bayes coding algorithm for context tree models. We confirm the computational efficiency of our VB method through an experiments on a benchmark dataset.\n",
            "Score: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"survey\", 1),\n",
        "        (\"election\", 1),\n",
        "        (\"voting\", 1),\n",
        "        (\"poll\", 1),\n",
        "        (\"vote\", 1),\n",
        "        (\"candidate\", 1),\n",
        "\n",
        "        (\"selection\", .5),\n",
        "        (\"coordination\", .5),\n",
        "        (\"consensus\", .5),\n",
        "        (\"campaign\", .5),\n",
        "\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Po_ipgoynfr",
        "outputId": "c5f5b66f-da41-4e12-f677-4037ad3cc87f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: survey\n",
            "Total Matches in Set: 64\n",
            "Matches Above Score-Floor in Set: 5\n",
            "2024-09-12__120155753414\n",
            "\n",
            "Showing 5 in top-45 out of 64 total results.     -> 5 of 45/64\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 149|||| \n",
            "'arxiv_id': arXiv:2409.07057, \n",
            "'paper_link': https://arxiv.org/abs/2409.07057, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07057, \n",
            "Title: A Novel Voting System for Medical Catalogues in National Health Insurance \n",
            "Subjects: Social and Information Networks (cs.SI) \n",
            "Abstract: This study explores the conceptual development of a medical insurance catalogue voting system. The methodology is centred on creating a model where doctors would vote on treatment inclusions, aiming to demonstrate transparency and integrity. The results from Monte Carlo simulations suggest a robust consensus on the selection of medicines and treatments. Further theoretical investigations propose incorporating a patient outcome-based incentive mechanism. This conceptual approach could enhance decision-making in healthcare by aligning stakeholder interests with patient outcomes, aiming for an optimised, equitable insurance catalogue with potential blockchain-based smart-contracts to ensure transparency and integrity.\n",
            "Score: 4.0\n",
            "\n",
            "Document: 38|||| \n",
            "'arxiv_id': arXiv:2409.06801, \n",
            "'paper_link': https://arxiv.org/abs/2409.06801, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06801, \n",
            "Title: Understanding and Mitigating the Impacts of Differentially Private Census Data on State Level Redistricting \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Data from the Decennial Census is published only after applying a disclosure avoidance system (DAS). Data users were shaken by the adoption of differential privacy in the 2020 DAS, a radical departure from past methods. The change raises the question of whether redistricting law permits, forbids, or requires taking account of the effect of disclosure avoidance. Such uncertainty creates legal risks for redistricters, as Alabama argued in a lawsuit seeking to prevent the 2020 DAS's deployment. We consider two redistricting settings in which a data user might be concerned about the impacts of privacy preserving noise: drawing equal population districts and litigating voting rights cases. What discrepancies arise if the user does nothing to account for disclosure avoidance? How might the user adapt her analyses to mitigate those discrepancies? We study these questions by comparing the official 2010 Redistricting Data to the 2010 Demonstration Data -- created using the 2020 DAS -- in an analysis of millions of algorithmically generated state legislative redistricting plans. In both settings, we observe that an analyst may come to incorrect conclusions if they do not account for noise. With minor adaptations, though, the underlying policy goals remain achievable: tweaking selection criteria enables a redistricter to draw balanced plans, and illustrative plans can still be used as evidence of the maximum number of majority-minority districts that are possible in a geography. At least for state legislatures, Alabama's claim that differential privacy ``inhibits a State's right to draw fair lines'' appears unfounded.\n",
            "Score: 2.5\n",
            "\n",
            "Document: 419|||| \n",
            "'arxiv_id': arXiv:2311.18402, \n",
            "'paper_link': https://arxiv.org/abs/2311.18402, \n",
            "'pdf_link': https://arxiv.org/pdf/2311.18402, \n",
            "Title: MV-CLIP: Multi-View CLIP for Zero-shot 3D Shape Recognition \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Large-scale pre-trained models have demonstrated impressive performance in vision and language tasks within open-world scenarios. Due to the lack of comparable pre-trained models for 3D shapes, recent methods utilize language-image pre-training to realize zero-shot 3D shape recognition. However, due to the modality gap, pretrained language-image models are not confident enough in the generalization to 3D shape recognition. Consequently, this paper aims to improve the confidence with view selection and hierarchical prompts. Leveraging the CLIP model as an example, we employ view selection on the vision side by identifying views with high prediction confidence from multiple rendered views of a 3D shape. On the textual side, the strategy of hierarchical prompts is proposed for the first time. The first layer prompts several classification candidates with traditional class-level descriptions, while the second layer refines the prediction based on function-level descriptions or further distinctions between the candidates. Remarkably, without the need for additional training, our proposed method achieves impressive zero-shot 3D classification accuracies of 84.44%, 91.51%, and 66.17% on ModelNet40, ModelNet10, and ShapeNet Core55, respectively. Furthermore, we will make the code publicly available to facilitate reproducibility and further research in this area.\n",
            "Score: 2.5\n",
            "\n",
            "Document: 19|||| \n",
            "'arxiv_id': arXiv:2409.06730, \n",
            "'paper_link': https://arxiv.org/abs/2409.06730, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06730, \n",
            "Title: Urban context and delivery performance: Modelling service time for cargo bikes and vans across diverse urban environments \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Light goods vehicles (LGV) used extensively in the last mile of delivery are one of the leading polluters in cities. Cargo-bike logistics and Light Electric Vehicles (LEVs) have been put forward as a high impact candidate for replacing LGVs. Studies have estimated over half of urban van deliveries being replaceable by cargo-bikes, due to their faster speeds, shorter parking times and more efficient routes across cities. However, the logistics sector suffers from a lack of publicly available data, particularly pertaining to cargo-bike deliveries, thus limiting the understanding of their potential benefits. Specifically, service time (which includes cruising for parking, and walking to destination) is a major, but often overlooked component of delivery time modelling. The aim of this study is to establish a framework for measuring the performance of delivery vehicles, with an initial focus on modelling service times of vans and cargo-bikes across diverse urban environments. We introduce two datasets that allow for in-depth analysis and modelling of service times of cargo bikes and use existing datasets to reason about differences in delivery performance across vehicle types. We introduce a modelling framework to predict the service times of deliveries based on urban context. We employ Uber's H3 index to divide cities into hexagonal cells and aggregate OpenStreetMap tags for each cell, providing a detailed assessment of urban context. Leveraging this spatial grid, we use GeoVex to represent micro-regions as points in a continuous vector space, which then serve as input for predicting vehicle service times. We show that geospatial embeddings can effectively capture urban contexts and facilitate generalizations to new contexts and cities. Our methodology addresses the challenge of limited comparative data available for different vehicle types within the same urban settings.\n",
            "Score: 2\n",
            "\n",
            "Document: 207|||| \n",
            "'arxiv_id': arXiv:2409.07194, \n",
            "'paper_link': https://arxiv.org/abs/2409.07194, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07194, \n",
            "Title: Cyber Deception: State of the art, Trends and Open challenges \n",
            "Subjects: Cryptography and Security (cs.CR) \n",
            "Abstract: The growing interest in cybersecurity has significantly increased articles designing and implementing various Cyber Deception (CYDEC) mechanisms. This trend reflects the urgent need for new strategies to address cyber threats effectively. Since its emergence, CYDEC has established itself as an innovative defense against attackers, thanks to its proactive and reactive capabilities, finding applications in numerous real-life scenarios. Despite the considerable work devoted to CYDEC, the literature still presents significant gaps. In particular, there has not been (i) a comprehensive analysis of the main components characterizing CYDEC, (ii) a generic classification covering all types of solutions, nor (iii) a survey of the current state of the literature in various contexts. This article aims to fill these gaps through a detailed review of the main features that comprise CYDEC, developing a comprehensive classification taxonomy. In addition, the different frameworks used to generate CYDEC are reviewed, presenting a more comprehensive one. Existing solutions in the literature using CYDEC, both without Artificial Intelligence (AI) and with AI, are studied and compared. Finally, the most salient trends of the current state of the art are discussed, offering a list of pending challenges for future research.\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 1\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"disinformation\", 1),\n",
        "        (\"manipulate public opinion\", 1),\n",
        "        (\"conspiracy\", 1),\n",
        "        (\"radicalization\", 1),\n",
        "        (\"conspiracy theories\", 1),\n",
        "        (\"violent extremism\", 2),\n",
        "\n",
        "        (\"extremism\", 1),\n",
        "        (\"extremist\", 1),\n",
        "        (\"extreme views\", 1),\n",
        "        (\"extreme beliefs\", 1),\n",
        "        (\"extreme action\", 1),\n",
        "        (\"ideology\", .5),        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdmCM3canYCW",
        "outputId": "0d4ea788-1b43-421d-f24e-b51737ed8e1e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: disinformation\n",
            "Total Matches in Set: 0\n",
            "Matches Above Score-Floor in Set: 0\n",
            "2024-09-12__120156017248\n",
            "\n",
            "Showing 0 in top-0 out of 0 total results.     -> 0 of 0/0\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 1) -> 1\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 1\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Speech-LLM\", 1),\n",
        "\n",
        "        (\"spoken language understanding\", 1),\n",
        "\n",
        "        (\"speech to text\", 1),\n",
        "        (\"text to speech\", 1),\n",
        "\n",
        "        (\"audio modality\", .5),\n",
        "        (\"speech encoder\", .5),\n",
        "        (\"SLU\", .5),\n",
        "        (\"stt\", .5),\n",
        "        (\"tts\", .5),\n",
        "\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHUhj_4zWLME",
        "outputId": "465e82a8-e12b-4bb4-942b-c2e29c221895"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Speech-LLM\n",
            "Total Matches in Set: 90\n",
            "Matches Above Score-Floor in Set: 5\n",
            "2024-09-12__120156354866\n",
            "\n",
            "Showing 5 in top-45 out of 90 total results.     -> 5 of 45/90\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 1) -> 1\n",
            "\n",
            "\n",
            "Document: 237|||| \n",
            "'arxiv_id': arXiv:2409.07265, \n",
            "'paper_link': https://arxiv.org/abs/2409.07265, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07265, \n",
            "Title: Cross-Dialect Text-To-Speech in Pitch-Accent Language Incorporating Multi-Dialect Phoneme-Level BERT \n",
            "Subjects: Sound (cs.SD) \n",
            "Abstract: We explore cross-dialect text-to-speech (CD-TTS), a task to synthesize learned speakers' voices in non-native dialects, especially in pitch-accent languages. CD-TTS is important for developing voice agents that naturally communicate with people across regions. We present a novel TTS model comprising three sub-modules to perform competitively at this task. We first train a backbone TTS model to synthesize dialect speech from a text conditioned on phoneme-level accent latent variables (ALVs) extracted from speech by a reference encoder. Then, we train an ALV predictor to predict ALVs tailored to a target dialect from input text leveraging our novel multi-dialect phoneme-level BERT. We conduct multi-dialect TTS experiments and evaluate the effectiveness of our model by comparing it with a baseline derived from conventional dialect TTS methods. The results show that our model improves the dialectal naturalness of synthetic speech in CD-TTS.\n",
            "Score: 2.0\n",
            "\n",
            "Document: 356|||| \n",
            "'arxiv_id': arXiv:2409.07151, \n",
            "'paper_link': https://arxiv.org/abs/2409.07151, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07151, \n",
            "Title: Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment \n",
            "Subjects: Audio and Speech Processing (eess.AS) \n",
            "Abstract: Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).\n",
            "Score: 2.0\n",
            "\n",
            "Document: 566|||| \n",
            "'arxiv_id': arXiv:2409.05674, \n",
            "'paper_link': https://arxiv.org/abs/2409.05674, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.05674, \n",
            "Title: Evaluation of real-time transcriptions using end-to-end ASR models \n",
            "Subjects: Sound (cs.SD) \n",
            "Abstract: Automatic Speech Recognition (ASR) or Speech-to-text (STT) has greatly evolved in the last few years. Traditional architectures based on pipelines have been replaced by joint end-to-end (E2E) architectures that simplify and streamline the model training process. In addition, new AI training methods, such as weak-supervised learning have reduced the need for high-quality audio datasets for model training. However, despite all these advancements, little to no research has been done on real-time transcription. In real-time scenarios, the audio is not pre-recorded, and the input audio must be fragmented to be processed by the ASR systems. To achieve real-time requirements, these fragments must be as short as possible to reduce latency. However, audio cannot be split at any point as dividing an utterance into two separate fragments will generate an incorrect transcription. Also, shorter fragments provide less context for the ASR model. For this reason, it is necessary to design and test different splitting algorithms to optimize the quality and delay of the resulting transcription. In this paper, three audio splitting algorithms are evaluated with different ASR models to determine their impact on both the quality of the transcription and the end-to-end delay. The algorithms are fragmentation at fixed intervals, voice activity detection (VAD), and fragmentation with feedback. The results are compared to the performance of the same model, without audio fragmentation, to determine the effects of this division. The results show that VAD fragmentation provides the best quality with the highest delay, whereas fragmentation at fixed intervals provides the lowest quality and the lowest delay. The newly proposed feedback algorithm exchanges a 2-4% increase in WER for a reduction of 1.5-2s delay, respectively, to the VAD splitting.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 195|||| \n",
            "'arxiv_id': arXiv:2409.07165, \n",
            "'paper_link': https://arxiv.org/abs/2409.07165, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07165, \n",
            "Title: Linear Time Complexity Conformers with SummaryMixing for Streaming Speech Recognition \n",
            "Subjects: Sound (cs.SD) \n",
            "Abstract: Automatic speech recognition (ASR) with an encoder equipped with self-attention, whether streaming or non-streaming, takes quadratic time in the length of the speech utterance. This slows down training and decoding, increase their cost, and limit the deployment of the ASR in constrained devices. SummaryMixing is a promising linear-time complexity alternative to self-attention for non-streaming speech recognition that, for the first time, preserves or outperforms the accuracy of self-attention models. Unfortunately, the original definition of SummaryMixing is not suited to streaming speech recognition. Hence, this work extends SummaryMixing to a Conformer Transducer that works in both a streaming and an offline mode. It shows that this new linear-time complexity speech encoder outperforms self-attention in both scenarios while requiring less compute and memory during training and decoding.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 287|||| \n",
            "'arxiv_id': arXiv:2409.07390, \n",
            "'paper_link': https://arxiv.org/abs/2409.07390, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07390, \n",
            "Title: D-CAPTCHA++: A Study of Resilience of Deepfake CAPTCHA under Transferable Imperceptible Adversarial Attack \n",
            "Subjects: Cryptography and Security (cs.CR) \n",
            "Abstract: The advancements in generative AI have enabled the improvement of audio synthesis models, including text-to-speech and voice conversion. This raises concerns about its potential misuse in social manipulation and political interference, as synthetic speech has become indistinguishable from natural human speech. Several speech-generation programs are utilized for malicious purposes, especially impersonating individuals through phone calls. Therefore, detecting fake audio is crucial to maintain social security and safeguard the integrity of information. Recent research has proposed a D-CAPTCHA system based on the challenge-response protocol to differentiate fake phone calls from real ones. In this work, we study the resilience of this system and introduce a more robust version, D-CAPTCHA++, to defend against fake calls. Specifically, we first expose the vulnerability of the D-CAPTCHA system under transferable imperceptible adversarial attack. Secondly, we mitigate such vulnerability by improving the robustness of the system by using adversarial training in D-CAPTCHA deepfake detectors and task classifiers.\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = .5\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"multiple agents\", 1),\n",
        "        (\"Multiagent Systems\", 1),\n",
        "        (\"Multiagent\", 1),\n",
        "        (\"(cs.MA)\", 1),\n",
        "        (\"multi-agent and multi-rack path finding\", 1),  #  (MARPF)\n",
        "\n",
        "        (\"agent interactions\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdHdGAdC9U1a",
        "outputId": "70b20b60-ee2a-40dc-f856-06809b4a76d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: multiple agents\n",
            "Total Matches in Set: 16\n",
            "Matches Above Score-Floor in Set: 16\n",
            "2024-09-12__120156631456\n",
            "\n",
            "Showing 16 in top-16 out of 16 total results.     -> 16 of 16/16\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 0.5) -> 0.5\n",
            "\n",
            "\n",
            "Document: 26|||| \n",
            "'arxiv_id': arXiv:2409.06750, \n",
            "'paper_link': https://arxiv.org/abs/2409.06750, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06750, \n",
            "Title: Can Agents Spontaneously Form a Society? Introducing a Novel Architecture for Generative Multi-Agents to Elicit Social Emergence \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: Generative agents have demonstrated impressive capabilities in specific tasks, but most of these frameworks focus on independent tasks and lack attention to social interactions. We introduce a generative agent architecture called ITCMA-S, which includes a basic framework for individual agents and a framework called LTRHA that supports social interactions among multi-agents. This architecture enables agents to identify and filter out behaviors that are detrimental to social interactions, guiding them to choose more favorable actions. We designed a sandbox environment to simulate the natural evolution of social relationships among multiple identity-less agents for experimental evaluation. The results showed that ITCMA-S performed well on multiple evaluation indicators, demonstrating its ability to actively explore the environment, recognize new agents, and acquire new information through continuous actions and dialogue. Observations show that as agents establish connections with each other, they spontaneously form cliques with internal hierarchies around a selected leader and organize collective activities.\n",
            "Score: 3\n",
            "\n",
            "Document: 72|||| \n",
            "'arxiv_id': arXiv:2409.06888, \n",
            "'paper_link': https://arxiv.org/abs/2409.06888, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06888, \n",
            "Title: A Quality Diversity Approach to Automatically Generate Multi-Agent Path Finding Benchmark Maps \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: We use the Quality Diversity (QD) algorithm with Neural Cellular Automata (NCA) to generate benchmark maps for Multi-Agent Path Finding (MAPF) algorithms. Previously, MAPF algorithms are tested using fixed, human-designed benchmark maps. However, such fixed benchmark maps have several problems. First, these maps may not cover all the potential failure scenarios for the algorithms. Second, when comparing different algorithms, fixed benchmark maps may introduce bias leading to unfair comparisons between algorithms. In this work, we take advantage of the QD algorithm and NCA with different objectives and diversity measures to generate maps with patterns to comprehensively understand the performance of MAPF algorithms and be able to make fair comparisons between two MAPF algorithms to provide further information on the selection between two algorithms. Empirically, we employ this technique to generate diverse benchmark maps to evaluate and compare the behavior of different types of MAPF algorithms such as bounded-suboptimal algorithms, suboptimal algorithms, and reinforcement-learning-based algorithms. Through both single-planner experiments and comparisons between algorithms, we identify patterns where each algorithm excels and detect disparities in runtime or success rates between different algorithms.\n",
            "Score: 3\n",
            "\n",
            "Document: 425|||| \n",
            "'arxiv_id': arXiv:2401.00605, \n",
            "'paper_link': https://arxiv.org/abs/2401.00605, \n",
            "'pdf_link': https://arxiv.org/pdf/2401.00605, \n",
            "Title: Distributed Multi-Object Tracking Under Limited Field of View Heterogeneous Sensors with Density Clustering \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: We consider the problem of tracking multiple, unknown, and time-varying numbers of objects using a distributed network of heterogeneous sensors. In an effort to derive a formulation for practical settings, we consider limited and unknown sensor field-of-views (FoVs), sensors with limited local computational resources and communication channel capacity. The resulting distributed multi-object tracking algorithm involves solving an NP-hard multidimensional assignment problem either optimally for small-size problems or sub-optimally for general practical problems. For general problems, we propose an efficient distributed multi-object tracking algorithm that performs track-to-track fusion using a clustering-based analysis of the state space transformed into a density space to mitigate the complexity of the assignment problem. The proposed algorithm can more efficiently group local track estimates for fusion than existing approaches. To ensure we achieve globally consistent identities for tracks across a network of nodes as objects move between FoVs, we develop a graph-based algorithm to achieve label consensus and minimise track segmentation. Numerical experiments with synthetic and real-world trajectory datasets demonstrate that our proposed method is significantly more computationally efficient than state-of-the-art solutions, achieving similar tracking accuracy and bandwidth requirements but with improved label consistency.\n",
            "Score: 3\n",
            "\n",
            "Document: 268|||| \n",
            "'arxiv_id': arXiv:2409.07335, \n",
            "'paper_link': https://arxiv.org/abs/2409.07335, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07335, \n",
            "Title: Explanation, Debate, Align: A Weak-to-Strong Framework for Language Model Generalization \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: The rapid advancement of artificial intelligence systems has brought the challenge of AI alignment to the forefront of research, particularly in complex decision-making and task execution. As these systems surpass human-level performance in sophisticated problems, ensuring their alignment with human values, intentions, and ethical guidelines becomes crucial. Building on previous work in explanation generation for human-agent alignment, we address the more complex dynamics of multi-agent systems and human-AI teams. This paper introduces a novel approach to model alignment through weak-to-strong generalization in the context of language models. We present a framework where a strong model facilitates the improvement of a weaker model, bridging the gap between explanation generation and model alignment. Our method, formalized as a facilitation function, allows for the transfer of capabilities from advanced models to less capable ones without direct access to extensive training data. Our results suggest that this facilitation-based approach not only enhances model performance but also provides insights into the nature of model alignment and the potential for scalable oversight of AI systems.\n",
            "Score: 2\n",
            "\n",
            "Document: 555|||| \n",
            "'arxiv_id': arXiv:2409.05106, \n",
            "'paper_link': https://arxiv.org/abs/2409.05106, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.05106, \n",
            "Title: Decentralized Control of Multi-Agent Systems Under Acyclic Spatio-Temporal Task Dependencies \n",
            "Subjects: Systems and Control (eess.SY) \n",
            "Abstract: We introduce a novel distributed sampled-data control method tailored for heterogeneous multi-agent systems under a global spatio-temporal task with acyclic dependencies. Specifically, we consider the global task as a conjunction of independent and collaborative tasks, defined over the absolute and relative states of agent pairs. Task dependencies in this form are then represented by a task graph, which we assume to be acyclic. From the given task graph, we provide an algorithmic approach to define a distributed sampled-data controller prioritizing the fulfilment of collaborative tasks as the primary objective, while fulfilling independent tasks unless they conflict with collaborative ones. Moreover, communication maintenance among collaborating agents is seamlessly enforced within the proposed control framework. A numerical simulation is provided to showcase the potential of our control framework.\n",
            "Score: 2\n",
            "\n",
            "Document: 570|||| \n",
            "'arxiv_id': arXiv:2409.05888, \n",
            "'paper_link': https://arxiv.org/abs/2409.05888, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.05888, \n",
            "Title: MA-CDMR: An Intelligent Cross-domain Multicast Routing Method based on Multiagent Deep Reinforcement Learning in Multi-domain SDWN \n",
            "Subjects: Networking and Internet Architecture (cs.NI) \n",
            "Abstract: The cross-domain multicast routing problem in a software-defined wireless network with multiple controllers is a classic NP-hard optimization problem. As the network size increases, designing and implementing cross-domain multicast routing paths in the network requires not only designing efficient solution algorithms to obtain the optimal cross-domain multicast tree but also ensuring the timely and flexible acquisition and maintenance of global network state information. However, existing solutions have a limited ability to sense the network traffic state, affecting the quality of service of multicast services. In addition, these methods have difficulty adapting to the highly dynamically changing network states and have slow convergence speeds. To this end, this paper aims to design and implement a multiagent deep reinforcement learning based cross-domain multicast routing method for SDWN with multicontroller domains. First, a multicontroller communication mechanism and a multicast group management module are designed to transfer and synchronize network information between different control domains of the SDWN, thus effectively managing the joining and classification of members in the cross-domain multicast group. Second, a theoretical analysis and proof show that the optimal cross-domain multicast tree includes an interdomain multicast tree and an intradomain multicast tree. An agent is established for each controller, and a cooperation mechanism between multiple agents is designed to effectively optimize cross-domain multicast routing and ensure consistency and validity in the representation of network state information for cross-domain multicast routing decisions. Third, a multiagent reinforcement learning-based method that combines online and offline training is designed to reduce the dependence on the real-time environment and increase the convergence speed of multiple agents.\n",
            "Score: 2\n",
            "\n",
            "Document: 175|||| \n",
            "'arxiv_id': arXiv:2409.07127, \n",
            "'paper_link': https://arxiv.org/abs/2409.07127, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07127, \n",
            "Title: DCMAC: Demand-aware Customized Multi-Agent Communication via Upper Bound Training \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: Efficient communication can enhance the overall performance of collaborative multi-agent reinforcement learning. A common approach is to share observations through full communication, leading to significant communication overhead. Existing work attempts to perceive the global state by conducting teammate model based on local information. However, they ignore that the uncertainty generated by prediction may lead to difficult training. To address this problem, we propose a Demand-aware Customized Multi-Agent Communication (DCMAC) protocol, which use an upper bound training to obtain the ideal policy. By utilizing the demand parsing module, agent can interpret the gain of sending local message on teammate, and generate customized messages via compute the correlation between demands and local observation using cross-attention mechanism. Moreover, our method can adapt to the communication resources of agents and accelerate the training progress by appropriating the ideal policy which is trained with joint observation. Experimental results reveal that DCMAC significantly outperforms the baseline algorithms in both unconstrained and communication constrained scenarios.\n",
            "Score: 1\n",
            "\n",
            "Document: 229|||| \n",
            "'arxiv_id': arXiv:2409.07246, \n",
            "'paper_link': https://arxiv.org/abs/2409.07246, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07246, \n",
            "Title: Propaganda to Hate: A Multimodal Analysis of Arabic Memes with Multi-Agent LLMs \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: In the past decade, social media platforms have been used for information dissemination and consumption. While a major portion of the content is posted to promote citizen journalism and public awareness, some content is posted to mislead users. Among different content types such as text, images, and videos, memes (text overlaid on images) are particularly prevalent and can serve as powerful vehicles for propaganda, hate, and humor. In the current literature, there have been efforts to individually detect such content in memes. However, the study of their intersection is very limited. In this study, we explore the intersection between propaganda and hate in memes using a multi-agent LLM-based approach. We extend the propagandistic meme dataset with coarse and fine-grained hate labels. Our finding suggests that there is an association between propaganda and hate in memes. We provide detailed experimental results that can serve as a baseline for future studies. We will make the experimental resources publicly available to the community.\n",
            "Score: 1\n",
            "\n",
            "Document: 269|||| \n",
            "'arxiv_id': arXiv:2409.07337, \n",
            "'paper_link': https://arxiv.org/abs/2409.07337, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07337, \n",
            "Title: Benchmarking 2D Egocentric Hand Pose Datasets \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Hand pose estimation from egocentric video has broad implications across various domains, including human-computer interaction, assistive technologies, activity recognition, and robotics, making it a topic of significant research interest. The efficacy of modern machine learning models depends on the quality of data used for their training. Thus, this work is devoted to the analysis of state-of-the-art egocentric datasets suitable for 2D hand pose estimation. We propose a novel protocol for dataset evaluation, which encompasses not only the analysis of stated dataset characteristics and assessment of data quality, but also the identification of dataset shortcomings through the evaluation of state-of-the-art hand pose estimation models. Our study reveals that despite the availability of numerous egocentric databases intended for 2D hand pose estimation, the majority are tailored for specific use cases. There is no ideal benchmark dataset yet; however, H2O and GANerated Hands datasets emerge as the most promising real and synthetic datasets, respectively.\n",
            "Score: 1\n",
            "\n",
            "Document: 291|||| \n",
            "'arxiv_id': arXiv:2409.07398, \n",
            "'paper_link': https://arxiv.org/abs/2409.07398, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07398, \n",
            "Title: The Complexity of Two-Team Polymatrix Games with Independent Adversaries \n",
            "Subjects: Computer Science and Game Theory (cs.GT) \n",
            "Abstract: Adversarial multiplayer games are an important object of study in multiagent learning. In particular, polymatrix zero-sum games are a multiplayer setting where Nash equilibria are known to be efficiently computable. Towards understanding the limits of tractability in polymatrix games, we study the computation of Nash equilibria in such games where each pair of players plays either a zero-sum or a coordination game. We are particularly interested in the setting where players can be grouped into a small number of teams of identical interest. While the three-team version of the problem is known to be PPAD-complete, the complexity for two teams has remained open. Our main contribution is to prove that the two-team version remains hard, namely it is CLS-hard. Furthermore, we show that this lower bound is tight for the setting where one of the teams consists of multiple independent adversaries. On the way to obtaining our main result, we prove hardness of finding any stationary point in the simplest type of non-convex-concave min-max constrained optimization problem, namely for a class of bilinear polynomial objective functions.\n",
            "Score: 1\n",
            "\n",
            "Document: 294|||| \n",
            "'arxiv_id': arXiv:2409.07406, \n",
            "'paper_link': https://arxiv.org/abs/2409.07406, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07406, \n",
            "Title: Trust Dynamics in Human-Autonomy Interaction: Uncover Associations between Trust Dynamics and Personal Characteristics \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: While personal characteristics influence people's snapshot trust towards autonomous systems, their relationships with trust dynamics remain poorly understood. We conducted a human-subject experiment with 130 participants performing a simulated surveillance task aided by an automated threat detector. A comprehensive pre-experimental survey collected data on participants' personal characteristics across 12 constructs and 28 dimensions. Based on data collected in the experiment, we clustered participants' trust dynamics into three types and assessed differences among the three clusters in terms of personal characteristics, behaviors, performance, and post-experiment ratings. Participants were clustered into three groups, namely Bayesian decision makers, disbelievers, and oscillators. Results showed that the clusters differ significantly in seven personal characteristics: masculinity, positive affect, extraversion, neuroticism, intellect, performance expectancy, and high expectations. The disbelievers tend to have high neuroticism and low performance expectancy. The oscillators tend to have higher scores in masculinity, positive affect, extraversion and intellect. We also found significant differences in the behaviors and post-experiment ratings among the three groups. The disbelievers are the least likely to blindly follow the recommendations made by the automated threat detector. Based on the significant personal characteristics, we developed a decision tree model to predict cluster types with an accuracy of 70%.\n",
            "Score: 1\n",
            "\n",
            "Document: 319|||| \n",
            "'arxiv_id': arXiv:2409.07453, \n",
            "'paper_link': https://arxiv.org/abs/2409.07453, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07453, \n",
            "Title: \"My Grade is Wrong!\": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: Interactive feedback, where feedback flows in both directions between teacher and student, is more effective than traditional one-way feedback. However, it is often too time-consuming for widespread use in educational practice. While Large Language Models (LLMs) have potential for automating feedback, they struggle with reasoning and interaction in an interactive setting. This paper introduces CAELF, a Contestable AI Empowered LLM Framework for automating interactive feedback. CAELF allows students to query, challenge, and clarify their feedback by integrating a multi-agent system with computational argumentation. Essays are first assessed by multiple Teaching-Assistant Agents (TA Agents), and then a Teacher Agent aggregates the evaluations through formal reasoning to generate feedback and grades. Students can further engage with the feedback to refine their understanding. A case study on 500 critical thinking essays with user studies demonstrates that CAELF significantly improves interactive feedback, enhancing the reasoning and interaction capabilities of LLMs. This approach offers a promising solution to overcoming the time and resource barriers that have limited the adoption of interactive feedback in educational settings.\n",
            "Score: 1\n",
            "\n",
            "Document: 330|||| \n",
            "'arxiv_id': arXiv:2409.06755, \n",
            "'paper_link': https://arxiv.org/abs/2409.06755, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06755, \n",
            "Title: A Systematic Approach to Crossing Numbers of Cartesian Products with Paths \n",
            "Subjects: Combinatorics (math.CO) \n",
            "Abstract: Determining the crossing numbers of Cartesian products of small graphs with arbitrarily large paths has been an ongoing topic of research since the 1970s. Doing so requires the establishment of coincident upper and lower bounds; the former is usually demonstrated by providing a suitable drawing procedure, while the latter often requires substantial theoretical arguments. Many such papers have been published, which typically focus on just one or two small graphs at a time, and use ad hoc arguments specific to those graphs. We propose a general approach which, when successful, establishes the required lower bound. This approach can be applied to the Cartesian product of any graph with arbitrarily large paths, and in each case involves solving a modified version of the crossing number problem on a finite number (typically only two or three) of small graphs. We demonstrate the potency of this approach by applying it to Cartesian products involving all 133 graphs $G$ of orders five or six, and show that it is successful in 128 cases. This includes 60 cases which a recent survey listed as either undetermined, or determined only in journals without adequate peer review.\n",
            "Score: 1\n",
            "\n",
            "Document: 474|||| \n",
            "'arxiv_id': arXiv:2405.07656, \n",
            "'paper_link': https://arxiv.org/abs/2405.07656, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.07656, \n",
            "Title: Non-Rigid Designators in Modal and Temporal Free Description Logics (Extended Version) \n",
            "Subjects: Logic in Computer Science (cs.LO) \n",
            "Abstract: Definite descriptions, such as 'the General Chair of KR 2024', are a semantically transparent device for object identification in knowledge representation. In first-order modal logic, definite descriptions have been widely investigated for their non-rigidity, which allows them to designate different objects (or none at all) at different states. We propose expressive modal description logics with non-rigid definite descriptions and names, and investigate decidability and complexity of the satisfaction problem. We first systematically link satisfiability for the one-variable fragment of first-order modal logic with counting to our modal description logics. Then, we prove a promising NEXPTIME-completeness result for concept satisfiability for the fundamental epistemic multi-agent logic $\\mathbf{S5}^{n}$ and its neighbours, and show that some expressive logics that are undecidable with constant domain become decidable (but Ackermann-hard) with expanding domains. Finally, we conduct a fine-grained analysis of decidability of temporal logics.\n",
            "Score: 1\n",
            "\n",
            "Document: 582|||| \n",
            "'arxiv_id': arXiv:2303.10615, \n",
            "'paper_link': https://arxiv.org/abs/2303.10615, \n",
            "'pdf_link': https://arxiv.org/pdf/2303.10615, \n",
            "Title: Counting Circuit Double Covers \n",
            "Subjects: Combinatorics (math.CO) \n",
            "Abstract: We study a counting version of Cycle Double Cover Conjecture. We discuss why it is more interesting to count circuits (i.e., graphs isomorphic to $C_k$ for some $k$) instead of cycles (graphs with all degrees even). We give an almost-exponential lower-bound for graphs with a surface embedding of representativity at least 4. We also prove an exponential lower-bound for planar graphs. We conjecture that any bridgeless cubic graph has at least $2^{n/2-1}$ circuit double covers and we show an infinite class of graphs for which this bound is tight.\n",
            "Score: 1\n",
            "\n",
            "Document: 602|||| \n",
            "'arxiv_id': arXiv:2408.12067, \n",
            "'paper_link': https://arxiv.org/abs/2408.12067, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.12067, \n",
            "Title: Distributed Noncoherent Joint Transmission Based on Multi-Agent Reinforcement Learning for Dense Small Cell MISO Systems \n",
            "Subjects: Signal Processing (eess.SP) \n",
            "Abstract: We consider a dense small cell (DSC) network where multi-antenna small cell base stations (SBSs) transmit data to single-antenna users over a shared frequency band. To enhance capacity, a state-of-the-art technique known as noncoherent joint transmission (JT) is applied, enabling users to receive data from multiple coordinated SBSs. However, the sum rate maximization problem with noncoherent JT is inherently nonconvex and NP-hard. While existing optimization-based noncoherent JT algorithms can provide near-optimal performance, they require global channel state information (CSI) and multiple iterations, which makes them difficult to be implemeted in DSC this http URL overcome these challenges, we first prove that the optimal beamforming structure is the same for both the power minimization problem and the sum rate maximization problem, and then mathematically derive the optimal beamforming structure for both problems by solving the power minimization problem.The optimal beamforming structure can effectively reduces the variable this http URL exploiting the optimal beamforming structure, we propose a deep deterministic policy gradient-based distributed noncoherent JT scheme to maximize the system sum this http URL the proposed scheme, each SBS utilizes global information for training and uses local CSI to determine beamforming vectors. Simulation results demonstrate that the proposed scheme achieves comparable performance with considerably lower computational complexity and information overhead compared to centralized iterative optimization-based techniques, making it more attractive for practical deployment.\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = .5\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Agents for Software Engineering\", .5),\n",
        "        (\"ai writing code\", .5),\n",
        "        (\"coding done by ai\", .5),\n",
        "        (\"AI-Generated Code\", .5),\n",
        "        (\"Generated Code\", .5),\n",
        "        (\"code generation\", .5),\n",
        "        (\"ai code writing\", .5),\n",
        "        (\"solutions to produce computer code\", .5),\n",
        "        (\"Generated Code\", .5),\n",
        "\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w24GXHdtcVgr",
        "outputId": "69526c78-dce6-4278-d028-6dffaa3380d8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Agents for Software Engineering\n",
            "Total Matches in Set: 3\n",
            "Matches Above Score-Floor in Set: 3\n",
            "2024-09-12__120157128039\n",
            "\n",
            "Showing 3 in top-3 out of 3 total results.     -> 3 of 3/3\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 0.5) -> 0.5\n",
            "\n",
            "\n",
            "Document: 280|||| \n",
            "'arxiv_id': arXiv:2409.07368, \n",
            "'paper_link': https://arxiv.org/abs/2409.07368, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07368, \n",
            "Title: Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code \n",
            "Subjects: Cryptography and Security (cs.CR) \n",
            "Abstract: This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: this http URL.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 99|||| \n",
            "'arxiv_id': arXiv:2409.06957, \n",
            "'paper_link': https://arxiv.org/abs/2409.06957, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06957, \n",
            "Title: Policy Filtration in RLHF to Fine-Tune LLM for Code Generation \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Reinforcement learning from human feedback (RLHF) is one of the key techniques that helps large language models (LLMs) to follow instructions and provide helpful and harmless responses. While direct policy optimization methods exist, state-of-the-art LLMs adopt RL-based methods (usually PPO) in RLHF to train the policy to generate good responses guided by a reward model learned from preference data. The main challenge of these methods is the inaccuracy of the intermediate reward model, especially in code generation tasks that require long and complex reasoning to score a response. We find that the reliability of the reward model varies across responses assigned with different rewards. This motivates us to filter the samples whose rewards may be unreliable to improve signal-to-noise ratio during policy learning, resulting in Policy Filtration for Proximal Policy Optimization (PF-PPO). To choose a proper policy filtration strategy for a given reward model, the coefficient of determination ($R^2$) between rewards and actual scores on filtered samples serves as a good metrics and helps us find several promising strategies. We provide extensive experiments to validate the effectiveness of PF-PPO in code generation tasks, and find that some variants of PF-PPO are highly effective and achieve new state-of-the-art performance across 7-billion-parameter models on HumanEval, MBPP, and a new and more challenging LeetCode Contest benchmark.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 177|||| \n",
            "'arxiv_id': arXiv:2409.07131, \n",
            "'paper_link': https://arxiv.org/abs/2409.07131, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07131, \n",
            "Title: Reranking Laws for Language Generation: A Communication-Theoretic Perspective \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: To ensure large language models (LLMs) are used safely, one must reduce their propensity to hallucinate or to generate unacceptable answers. A simple and often used strategy is to first let the LLM generate multiple hypotheses and then employ a reranker to choose the best one. In this paper, we draw a parallel between this strategy and the use of redundancy to decrease the error rate in noisy communication channels. We conceptualize the generator as a sender transmitting multiple descriptions of a message through parallel noisy channels. The receiver decodes the message by ranking the (potentially corrupted) descriptions and selecting the one found to be most reliable. We provide conditions under which this protocol is asymptotically error-free (i.e., yields an acceptable answer almost surely) even in scenarios where the reranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the channel distributions are statistically dependent. We use our framework to obtain reranking laws which we validate empirically on two real-world tasks using LLMs: text-to-code generation with DeepSeek-Coder 7B and machine translation of medical data with TowerInstruct 13B.\n",
            "Score: 0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = .5\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"e-Learners\", 1),\n",
        "        (\"educational content\", 1),\n",
        "        (\"learning styles\", 1),\n",
        "        (\"educational process\", 1),\n",
        "        (\"human learning\", 1),\n",
        "\n",
        "        (\"education\", .5),\n",
        "        (\"learner\", .5),\n",
        "        (\"individual needs\", .5),\n",
        "\n",
        "        (\"learning sciences\", .5),\n",
        "        (\"educational technology\", .5),\n",
        "        (\"human-computer interaction\", .5),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ1iMqUJt8l9",
        "outputId": "0f2c40ec-d711-40f1-a56f-2324ae728b40"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: e-Learners\n",
            "Total Matches in Set: 33\n",
            "Matches Above Score-Floor in Set: 33\n",
            "2024-09-12__120157393709\n",
            "\n",
            "Showing 33 in top-33 out of 33 total results.     -> 33 of 33/33\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 0.5) -> 0.5\n",
            "\n",
            "\n",
            "Document: 7|||| \n",
            "'arxiv_id': arXiv:2409.06712, \n",
            "'paper_link': https://arxiv.org/abs/2409.06712, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06712, \n",
            "Title: A Meta-analysis of College Students' Intention to Use Generative Artificial Intelligence \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: It is of critical importance to analyse the factors influencing college students' intention to use generative artificial intelligence (GenAI) to understand and predict learners' learning behaviours and academic outcomes. Nevertheless, a lack of congruity has been shown in extant research results. This study, therefore, conducted a meta-analysis of 27 empirical studies under an integrated theoretical framework, including 87 effect sizes of independent research and 33,833 sample data. The results revealed that the main variables are strongly correlated with students' behavioural intention to use GenAI. Among them, performance expectancy (r = 0.389) and attitudes (r = 0.576) play particularly critical roles, and effort expectancy and habit are moderated by locational factors. Gender, notably, only moderated attitudes on students' behavioural intention to use GenAI. This study provides valuable insights for addressing the debate regarding students' intention to use GenAI in existed research, improving educational technology, as well as offering support for school decision-makers and educators to apply GenAI in school settings.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 75|||| \n",
            "'arxiv_id': arXiv:2409.06898, \n",
            "'paper_link': https://arxiv.org/abs/2409.06898, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06898, \n",
            "Title: Mazed and Confused: A Dataset of Cybersickness, Working Memory, Mental Load, Physical Load, and Attention During a Real Walking Task in VR \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Virtual Reality (VR) is quickly establishing itself in various industries, including training, education, medicine, and entertainment, in which users are frequently required to carry out multiple complex cognitive and physical activities. However, the relationship between cognitive activities, physical activities, and familiar feelings of cybersickness is not well understood and thus can be unpredictable for developers. Researchers have previously provided labeled datasets for predicting cybersickness while users are stationary, but there have been few labeled datasets on cybersickness while users are physically walking. Thus, from 39 participants, we collected head orientation, head position, eye tracking, images, physiological readings from external sensors, and the self-reported cybersickness severity, physical load, and mental load in VR. Throughout the data collection, participants navigated mazes via real walking and performed tasks challenging their attention and working memory. To demonstrate the dataset's utility, we conducted a case study of training classifiers in which we achieved 95% accuracy for cybersickness severity classification. The noteworthy performance of the straightforward classifiers makes this dataset ideal for future researchers to develop cybersickness detection and reduction models. To better understand the features that helped with classification, we performed SHAP(SHapley Additive exPlanations) analysis, highlighting the importance of eye tracking and physiological measures for cybersickness prediction while walking. This open dataset can allow future researchers to study the connection between cybersickness and cognitive loads and develop prediction models. This dataset will empower future VR developers to design efficient and effective Virtual Environments by improving cognitive load management and minimizing cybersickness.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 94|||| \n",
            "'arxiv_id': arXiv:2409.06951, \n",
            "'paper_link': https://arxiv.org/abs/2409.06951, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06951, \n",
            "Title: A Comparative Study of Table Sized Physicalization and Digital Visualization \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Data physicalization is gaining popularity in public and educational contexts due to its potential to make abstract data more tangible and understandable. Despite its growing use, there remains a significant gap in our understanding of how large-size physical visualizations compare to their digital counterparts in terms of user comprehension and memory retention. This study aims to bridge this knowledge gap by comparing the effectiveness of visualizing school building history data on large digital screens versus large physical models. Our experimental approach involved 32 participants who were exposed to one of the visualization mediums. We assessed their user experience and immediate understanding of the content, measured through tests after exposure, and evaluated memory retention with follow-up tests seven days later. The results revealed notable differences between the two forms of visualization: physicalization not only facilitated better initial comprehension but also significantly enhanced long-term memory retention. Furthermore, user feedback on usability was also higher on physicalization. These findings underscore the substantial impact of physicalization in improving information comprehension and retention. This study contributes crucial insights into future visualization media selection in educational and public settings.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 174|||| \n",
            "'arxiv_id': arXiv:2409.07123, \n",
            "'paper_link': https://arxiv.org/abs/2409.07123, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07123, \n",
            "Title: Cross-Refine: Improving Natural Language Explanation Generation by Learning in Tandem \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Natural language explanations (NLEs) are vital for elucidating the reasoning behind large language model (LLM) decisions. Many techniques have been developed to generate NLEs using LLMs. However, like humans, LLMs might not always produce optimal NLEs on first attempt. Inspired by human learning processes, we introduce Cross-Refine, which employs role modeling by deploying two LLMs as generator and critic, respectively. The generator outputs a first NLE and then refines this initial explanation using feedback and suggestions provided by the critic. Cross-Refine does not require any supervised training data or additional training. We validate Cross-Refine across three NLP tasks using three state-of-the-art open-source LLMs through automatic and human evaluation. We select Self-Refine (Madaan et al., 2023) as the baseline, which only utilizes self-feedback to refine the explanations. Our findings from automatic evaluation and a user study indicate that Cross-Refine outperforms Self-Refine. Meanwhile, Cross-Refine can perform effectively with less powerful LLMs, whereas Self-Refine only yields strong results with ChatGPT. Additionally, we conduct an ablation study to assess the importance of feedback and suggestions. Both of them play an important role in refining explanations. We further evaluate Cross-Refine on a bilingual dataset in English and German.\n",
            "Score: 1\n",
            "\n",
            "Document: 9|||| \n",
            "'arxiv_id': arXiv:2409.06717, \n",
            "'paper_link': https://arxiv.org/abs/2409.06717, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06717, \n",
            "Title: Tailoring Chatbots for Higher Education: Some Insights and Experiences \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: The general availability of powerful Large Language Models had a powerful impact on higher education, yet general models may not always be useful for the associated specialized tasks. When using these models, oftentimes the need for particular domain knowledge becomes quickly apparent, and the desire for customized bots arises. Customization holds the promise of leading to more accurate and contextually relevant responses, enhancing the educational experience. The purpose of this short technical experience report is to describe what \"customizing\" Large Language Models means in practical terms for higher education institutions. This report thus relates insights and experiences from one particular technical university in Switzerland, ETH Zurich.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 10|||| \n",
            "'arxiv_id': arXiv:2409.06718, \n",
            "'paper_link': https://arxiv.org/abs/2409.06718, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06718, \n",
            "Title: Unsupervised Representation Learning of Complex Time Series for Maneuverability State Identification in Smart Mobility \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Multivariate Time Series (MTS) data capture temporal behaviors to provide invaluable insights into various physical dynamic phenomena. In smart mobility, MTS plays a crucial role in providing temporal dynamics of behaviors such as maneuver patterns, enabling early detection of anomalous behaviors while facilitating pro-activity in Prognostics and Health Management (PHM). In this work, we aim to address challenges associated with modeling MTS data collected from a vehicle using sensors. Our goal is to investigate the effectiveness of two distinct unsupervised representation learning approaches in identifying maneuvering states in smart mobility. Specifically, we focus on some bivariate accelerations extracted from 2.5 years of driving, where the dataset is non-stationary, long, noisy, and completely unlabeled, making manual labeling impractical. The approaches of interest are Temporal Neighborhood Coding for Maneuvering (TNC4Maneuvering) and Decoupled Local and Global Representation learner for Maneuvering (DLG4Maneuvering).\n",
            "The main advantage of these frameworks is that they capture transferable insights in a form of representations from the data that can be effectively applied in multiple subsequent tasks, such as time-series classification, clustering, and multi-linear regression, which are the quantitative measures and qualitative measures, including visualization of representations themselves and resulting reconstructed MTS, respectively. We compare their effectiveness, where possible, in order to gain insights into which approach is more effective in identifying maneuvering states in smart mobility.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 13|||| \n",
            "'arxiv_id': arXiv:2409.06721, \n",
            "'paper_link': https://arxiv.org/abs/2409.06721, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06721, \n",
            "Title: Students' Perceived Roles, Opportunities, and Challenges of a Generative AI-powered Teachable Agent: A Case of Middle School Math Class \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Ongoing advancements in Generative AI (GenAI) have boosted the potential of applying long-standing learning-by-teaching practices in the form of a teachable agent (TA). Despite the recognized roles and opportunities of TAs, less is known about how GenAI could create synergy or introduce challenges in TAs and how students perceived the application of GenAI in TAs. This study explored middle school students perceived roles, benefits, and challenges of GenAI-powered TAs in an authentic mathematics classroom. Through classroom observation, focus-group interviews, and open-ended surveys of 108 sixth-grade students, we found that students expected the GenAI-powered TA to serve as a learning companion, facilitator, and collaborative problem-solver. Students also expressed the benefits and challenges of GenAI-powered TAs. This study provides implications for the design of educational AI and AI-assisted instruction.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 23|||| \n",
            "'arxiv_id': arXiv:2409.06745, \n",
            "'paper_link': https://arxiv.org/abs/2409.06745, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06745, \n",
            "Title: Personalized Knowledge Tracing through Student Representation Reconstruction and Class Imbalance Mitigation \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Knowledge tracing is a technique that predicts students' future performance by analyzing their learning process through historical interactions with intelligent educational platforms, enabling a precise evaluation of their knowledge mastery. Recent studies have achieved significant progress by leveraging powerful deep neural networks. These models construct complex input representations using questions, skills, and other auxiliary information but overlook individual student characteristics, which limits the capability for personalized assessment. Additionally, the available datasets in the field exhibit class imbalance issues. The models that simply predict all responses as correct without substantial effort can yield impressive accuracy. In this paper, we propose PKT, a novel approach for personalized knowledge tracing. PKT reconstructs representations from sequences of interactions with a tutoring platform to capture latent information about the students. Moreover, PKT incorporates focal loss to improve prioritize minority classes, thereby achieving more balanced predictions. Extensive experimental results on four publicly available educational datasets demonstrate the advanced predictive performance of PKT in comparison with 16 state-of-the-art models. To ensure the reproducibility of our research, the code is publicly available at https://anonymous.4open.science/r/PKT.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 37|||| \n",
            "'arxiv_id': arXiv:2409.06800, \n",
            "'paper_link': https://arxiv.org/abs/2409.06800, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06800, \n",
            "Title: Adaptive Meta-Domain Transfer Learning (AMDTL): A Novel Approach for Knowledge Transfer in AI \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: This paper presents Adaptive Meta-Domain Transfer Learning (AMDTL), a novel methodology that combines principles of meta-learning with domain-specific adaptations to enhance the transferability of artificial intelligence models across diverse and unknown domains. AMDTL aims to address the main challenges of transfer learning, such as domain misalignment, negative transfer, and catastrophic forgetting, through a hybrid framework that emphasizes both generalization and contextual specialization. The framework integrates a meta-learner trained on a diverse distribution of tasks, adversarial training techniques for aligning domain feature distributions, and dynamic feature regulation mechanisms based on contextual domain embeddings. Experimental results on benchmark datasets demonstrate that AMDTL outperforms existing transfer learning methodologies in terms of accuracy, adaptation efficiency, and robustness. This research provides a solid theoretical and practical foundation for the application of AMDTL in various fields, opening new perspectives for the development of more adaptable and inclusive AI systems.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 44|||| \n",
            "'arxiv_id': arXiv:2409.06814, \n",
            "'paper_link': https://arxiv.org/abs/2409.06814, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06814, \n",
            "Title: \"Come to us first\": Centering Community Organizations in Artificial Intelligence for Social Good Partnerships \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Artificial Intelligence for Social Good (AI4SG) has emerged as a growing body of research and practice exploring the potential of AI technologies to tackle social issues. This area emphasizes interdisciplinary partnerships with community organizations, such as non-profits and government agencies. However, amidst excitement about new advances in AI and their potential impact, the needs, expectations, and aspirations of these community organizations--and whether they are being met--are not well understood. Understanding these factors is important to ensure that the considerable efforts by AI teams and community organizations can actually achieve the positive social impact they strive for. Drawing on the Data Feminism framework, we explored the perspectives of community organization members on their partnerships with AI teams through 16 semi-structured interviews. Our study highlights the pervasive influence of funding agendas and the optimism surrounding AI's potential. Despite the significant intellectual contributions and labor provided by community organization members, their goals were frequently sidelined in favor of other stakeholders, including AI teams. While many community organization members expected tangible project deployment, only two out of 14 projects we studied reached the deployment stage. However, community organization members sustained their belief in the potential of the projects, still seeing diminished goals as valuable. To enhance the efficacy of future collaborations, our participants shared their aspirations for success, calling for co-leadership starting from the early stages of projects. We propose data co-liberation as a grounding principle for approaching AI4SG moving forward, positing that community organizations' co-leadership is essential for fostering more effective, sustainable, and ethical development of AI.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 74|||| \n",
            "'arxiv_id': arXiv:2409.06892, \n",
            "'paper_link': https://arxiv.org/abs/2409.06892, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06892, \n",
            "Title: Formative Study for AI-assisted Data Visualization \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: This formative study investigates the impact of data quality on AI-assisted data visualizations, focusing on how uncleaned datasets influence the outcomes of these tools. By generating visualizations from datasets with inherent quality issues, the research aims to identify and categorize the specific visualization problems that arise. The study further explores potential methods and tools to address these visualization challenges efficiently and effectively. Although tool development has not yet been undertaken, the findings emphasize enhancing AI visualization tools to handle flawed data better. This research underscores the critical need for more robust, user-friendly solutions that facilitate quicker and easier correction of data and visualization errors, thereby improving the overall reliability and usability of AI-assisted data visualization processes.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 83|||| \n",
            "'arxiv_id': arXiv:2409.06926, \n",
            "'paper_link': https://arxiv.org/abs/2409.06926, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06926, \n",
            "Title: Minimum Viable Ethics: From Institutionalizing Industry AI Governance to Product Impact \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Across the technology industry, many companies have expressed their commitments to AI ethics and created dedicated roles responsible for translating high-level ethics principles into product. Yet it is unclear how effective this has been in leading to meaningful product changes. Through semi-structured interviews with 26 professionals working on AI ethics in industry, we uncover challenges and strategies of institutionalizing ethics work along with translation into product impact. We ultimately find that AI ethics professionals are highly agile and opportunistic, as they attempt to create standardized and reusable processes and tools in a corporate environment in which they have little traditional power. In negotiations with product teams, they face challenges rooted in their lack of authority and ownership over product, but can push forward ethics work by leveraging narratives of regulatory response and ethics as product quality assurance. However, this strategy leaves us with a minimum viable ethics, a narrowly scoped industry AI ethics that is limited in its capacity to address normative issues separate from compliance or product quality. Potential future regulation may help bridge this gap.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 128|||| \n",
            "'arxiv_id': arXiv:2409.07005, \n",
            "'paper_link': https://arxiv.org/abs/2409.07005, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07005, \n",
            "Title: Situated Visualization in Motion \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: We contribute a first design space on visualizations in motion and the design of a pilot study we plan to run in the fall. Visualizations can be useful in contexts where either the observation is in motion or the whole visualization is moving at various speeds. Imagine, for example, displays attached to an athlete or animal that show data about the wearer -- for example, captured from a fitness tracking band; or a visualization attached to a moving object such as a vehicle or a soccer ball. The ultimate goal of our research is to inform the design of visualizations under motion.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 129|||| \n",
            "'arxiv_id': arXiv:2409.07006, \n",
            "'paper_link': https://arxiv.org/abs/2409.07006, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07006, \n",
            "Title: Envisioning Situated Visualizations of Environmental Footprints in an Urban Environment \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: We present the results of a brainstorming exercise focused on how situated visualizations could be used to better understand the state of the environment and our personal behavioral impact on it. Specifically, we conducted a day long workshop in the French city of Bordeaux where we envisioned situated visualizations of urban environmental footprints. We explored the city and took photos and notes about possible situated visualizations of environmental footprints that could be embedded near places, people, or objects of interest. We found that our designs targeted four purposes and used four different methods that could be further explored to test situated visualizations for the protection of the environment.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 137|||| \n",
            "'arxiv_id': arXiv:2409.07031, \n",
            "'paper_link': https://arxiv.org/abs/2409.07031, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07031, \n",
            "Title: Situated Visualization in Motion for Video Games \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: We contribute a systematic review of situated visualizations in motion in the context of video games. Video games produce rich dynamic datasets during gameplay that are often visualized to help players succeed in a game. Often these visualizations are moving either because they are attached to moving game elements or due to camera changes. We want to understand to what extent this motion and contextual game factors impact how players can read these visualizations. In order to ground our work, we surveyed 160 visualizations in motion and their embeddings in the game world. Here, we report on our analysis and categorization of these visualizations.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 166|||| \n",
            "'arxiv_id': arXiv:2409.07105, \n",
            "'paper_link': https://arxiv.org/abs/2409.07105, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07105, \n",
            "Title: RSVP for VPSA : A Meta Design Study on Rapid Suggestive Visualization Prototyping for Visual Parameter Space Analysis \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Visual Parameter Space Analysis (VPSA) enables domain scientists to explore input-output relationships of computational models. Existing VPSA applications often feature multi-view visualizations designed by visualization experts for a specific scenario, making it hard for domain scientists to adapt them to their problems without professional help. We present RSVP, the Rapid Suggestive Visualization Prototyping system encoding VPSA knowledge to enable domain scientists to prototype custom visualization dashboards tailored to their specific needs. The system implements a task-oriented, multi-view visualization recommendation strategy over a visualization design space optimized for VPSA to guide users in meeting their analytical demands. We derived the VPSA knowledge implemented in the system by conducting an extensive meta design study over the body of work on VPSA. We show how this process can be used to perform a data and task abstraction, extract a common visualization design space, and derive a task-oriented VisRec strategy. User studies indicate that the system is user-friendly and can uncover novel insights.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 169|||| \n",
            "'arxiv_id': arXiv:2409.07110, \n",
            "'paper_link': https://arxiv.org/abs/2409.07110, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07110, \n",
            "Title: Bio-Eng-LMM AI Assist chatbot: A Comprehensive Tool for Research and Education \n",
            "Subjects: Systems and Control (eess.SY) \n",
            "Abstract: This article introduces Bio-Eng-LMM AI chatbot, a versatile platform designed to enhance user interaction for educational and research purposes. Leveraging cutting-edge open-source Large Language Models (LLMs), Bio-Eng-LMM operates as a sophisticated AI assistant, exploiting the capabilities of traditional models like ChatGPT. Central to Bio-Eng-LMM is its implementation of Retrieval Augmented Generation (RAG) through three primary methods: integration of preprocessed documents, real-time processing of user-uploaded files, and information retrieval from any specified website. Additionally, the chatbot incorporates image generation via a Stable Diffusion Model (SDM), image understanding and response generation through LLAVA, and search functionality on the internet powered by secure search engine such as DuckDuckGo. To provide comprehensive support, Bio-Eng-LMM offers text summarization, website content summarization, and both text and voice interaction. The chatbot maintains session memory to ensure contextually relevant and coherent responses. This integrated platform builds upon the strengths of RAG-GPT and Web-Based RAG Query (WBRQ) where the system fetches relevant information directly from the web to enhance the LLMs response generation.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 199|||| \n",
            "'arxiv_id': arXiv:2409.07178, \n",
            "'paper_link': https://arxiv.org/abs/2409.07178, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07178, \n",
            "Title: Identify Design Problems Through Questioning: Exploring Role-playing Interactions with Large Language Models to Foster Design Questioning Skills \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Identifying design problems is a crucial step for creating plausible solutions, but it is challenging for design novices due to their limited knowledge and experience. Questioning is a promising skill that enables students to independently identify design problems without being passive or relying on instructors. This study explores role-playing interactions with Large Language Model (LLM)-powered Conversational Agents (CAs) to foster the questioning skills of novice design students. We proposed an LLM-powered CA prototype and conducted a preliminary study with 16 novice design students engaged in a real-world design class to observe the interactions between students and the LLM-powered CAs. Our findings indicate that while the CAs stimulated questioning and reduced pressure to ask questions, it also inadvertently led to over-reliance on LLM responses. We proposed design considerations and future works for LLM-powered CA to foster questioning skills.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 212|||| \n",
            "'arxiv_id': arXiv:2409.07204, \n",
            "'paper_link': https://arxiv.org/abs/2409.07204, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07204, \n",
            "Title: Online Graph Filtering Over Expanding Graphs \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Graph filters are a staple tool for processing signals over graphs in a multitude of downstream tasks. However, they are commonly designed for graphs with a fixed number of nodes, despite real-world networks typically grow over time. This topological evolution is often known up to a stochastic model, thus, making conventional graph filters ill-equipped to withstand such topological changes, their uncertainty, as well as the dynamic nature of the incoming data. To tackle these issues, we propose an online graph filtering framework by relying on online learning principles. We design filters for scenarios where the topology is both known and unknown, including a learner adaptive to such evolution. We conduct a regret analysis to highlight the role played by the different components such as the online algorithm, the filter order, and the growing graph model. Numerical experiments with synthetic and real data corroborate the proposed approach for graph signal inference tasks and show a competitive performance w.r.t. baselines and state-of-the-art alternatives.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 230|||| \n",
            "'arxiv_id': arXiv:2409.07250, \n",
            "'paper_link': https://arxiv.org/abs/2409.07250, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07250, \n",
            "Title: Bridging Quantitative and Qualitative Methods for Visualization Research: A Data/Semantics Perspective in Light of Advanced AI \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: This paper revisits the role of quantitative and qualitative methods in visualization research in the context of advancements in artificial intelligence (AI). The focus is on how we can bridge between the different methods in an integrated process of analyzing user study data. To this end, a process model of - potentially iterated - semantic enrichment and transformation of data is proposed. This joint perspective of data and semantics facilitates the integration of quantitative and qualitative methods. The model is motivated by examples of own prior work, especially in the area of eye tracking user studies and coding data-rich observations. Finally, there is a discussion of open issues and research opportunities in the interplay between AI, human analyst, and qualitative and quantitative methods for visualization research.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 254|||| \n",
            "'arxiv_id': arXiv:2409.07306, \n",
            "'paper_link': https://arxiv.org/abs/2409.07306, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07306, \n",
            "Title: Visual Compositional Data Analytics for Spatial Transcriptomics \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: For the Bio+Med-Vis Challenge 2024, we propose a visual analytics system as a redesign for the scatter pie chart visualization of cell type proportions of spatial transcriptomics data. Our design uses three linked views: a view of the histological image of the tissue, a stacked bar chart showing cell type proportions of the spots, and a scatter plot showing a dimensionality reduction of the multivariate proportions. Furthermore, we apply a compositional data analysis framework, the Aitchison geometry, to the proportions for dimensionality reduction and $k$-means clustering. Leveraging brushing and linking, the system allows one to explore and uncover patterns in the cell type mixtures and relate them to their spatial locations on the cellular tissue. This redesign shifts the pattern recognition workload from the human visual system to computational methods commonly used in visual analytics. We provide the code and setup instructions of our visual analytics system on GitHub (this https URL).\n",
            "Score: 0.5\n",
            "\n",
            "Document: 269|||| \n",
            "'arxiv_id': arXiv:2409.07337, \n",
            "'paper_link': https://arxiv.org/abs/2409.07337, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07337, \n",
            "Title: Benchmarking 2D Egocentric Hand Pose Datasets \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Hand pose estimation from egocentric video has broad implications across various domains, including human-computer interaction, assistive technologies, activity recognition, and robotics, making it a topic of significant research interest. The efficacy of modern machine learning models depends on the quality of data used for their training. Thus, this work is devoted to the analysis of state-of-the-art egocentric datasets suitable for 2D hand pose estimation. We propose a novel protocol for dataset evaluation, which encompasses not only the analysis of stated dataset characteristics and assessment of data quality, but also the identification of dataset shortcomings through the evaluation of state-of-the-art hand pose estimation models. Our study reveals that despite the availability of numerous egocentric databases intended for 2D hand pose estimation, the majority are tailored for specific use cases. There is no ideal benchmark dataset yet; however, H2O and GANerated Hands datasets emerge as the most promising real and synthetic datasets, respectively.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 277|||| \n",
            "'arxiv_id': arXiv:2409.07362, \n",
            "'paper_link': https://arxiv.org/abs/2409.07362, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07362, \n",
            "Title: GitSEED: A Git-backed Automated Assessment Tool for Software Engineering and Programming Education \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Due to the substantial number of enrollments in programming courses, a key challenge is delivering personalized feedback to students. The nature of this feedback varies significantly, contingent on the subject and the chosen evaluation method. However, tailoring current Automated Assessment Tools (AATs) to integrate other program analysis tools is not straightforward. Moreover, AATs usually support only specific programming languages, providing feedback exclusively through dedicated websites based on test suites.\n",
            "This paper introduces GitSEED, a language-agnostic automated assessment tool designed for Programming Education and Software Engineering (SE) and backed by GitLab. The students interact with GitSEED through GitLab. Using GitSEED, students in Computer Science (CS) and SE can master the fundamentals of git while receiving personalized feedback on their programming assignments and projects. Furthermore, faculty members can easily tailor GitSEED's pipeline by integrating various code evaluation tools (e.g., memory leak detection, fault localization, program repair, etc.) to offer personalized feedback that aligns with the needs of each CS/SE course. Our experiments assess GitSEED's efficacy via comprehensive user evaluation, examining the impact of feedback mechanisms and features on student learning outcomes. Findings reveal positive correlations between GitSEED usage and student engagement.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 282|||| \n",
            "'arxiv_id': arXiv:2409.07372, \n",
            "'paper_link': https://arxiv.org/abs/2409.07372, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07372, \n",
            "Title: Awaking the Slides: A Tuning-free and Knowledge-regulated AI Tutoring System via Language Model Coordination \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: The vast pre-existing slides serve as rich and important materials to carry lecture knowledge. However, effectively leveraging lecture slides to serve students is difficult due to the multi-modal nature of slide content and the heterogeneous teaching actions. We study the problem of discovering effective designs that convert a slide into an interactive lecture. We develop Slide2Lecture, a tuning-free and knowledge-regulated intelligent tutoring system that can (1) effectively convert an input lecture slide into a structured teaching agenda consisting of a set of heterogeneous teaching actions; (2) create and manage an interactive lecture that generates responsive interactions catering to student learning demands while regulating the interactions to follow teaching actions. Slide2Lecture contains a complete pipeline for learners to obtain an interactive classroom experience to learn the slide. For teachers and developers, Slide2Lecture enables customization to cater to personalized demands. The evaluation rated by annotators and students shows that Slide2Lecture is effective in outperforming the remaining implementation. Slide2Lecture's online deployment has made more than 200K interaction with students in the 3K lecture sessions. We open source Slide2Lecture's implementation in https://anonymous.4open.science/r/slide2lecture-4210/.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 294|||| \n",
            "'arxiv_id': arXiv:2409.07406, \n",
            "'paper_link': https://arxiv.org/abs/2409.07406, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07406, \n",
            "Title: Trust Dynamics in Human-Autonomy Interaction: Uncover Associations between Trust Dynamics and Personal Characteristics \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: While personal characteristics influence people's snapshot trust towards autonomous systems, their relationships with trust dynamics remain poorly understood. We conducted a human-subject experiment with 130 participants performing a simulated surveillance task aided by an automated threat detector. A comprehensive pre-experimental survey collected data on participants' personal characteristics across 12 constructs and 28 dimensions. Based on data collected in the experiment, we clustered participants' trust dynamics into three types and assessed differences among the three clusters in terms of personal characteristics, behaviors, performance, and post-experiment ratings. Participants were clustered into three groups, namely Bayesian decision makers, disbelievers, and oscillators. Results showed that the clusters differ significantly in seven personal characteristics: masculinity, positive affect, extraversion, neuroticism, intellect, performance expectancy, and high expectations. The disbelievers tend to have high neuroticism and low performance expectancy. The oscillators tend to have higher scores in masculinity, positive affect, extraversion and intellect. We also found significant differences in the behaviors and post-experiment ratings among the three groups. The disbelievers are the least likely to blindly follow the recommendations made by the automated threat detector. Based on the significant personal characteristics, we developed a decision tree model to predict cluster types with an accuracy of 70%.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 311|||| \n",
            "'arxiv_id': arXiv:2409.07444, \n",
            "'paper_link': https://arxiv.org/abs/2409.07444, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07444, \n",
            "Title: Echoes of Privacy: Uncovering the Profiling Practices of Voice Assistants \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Many companies, including Google, Amazon, and Apple, offer voice assistants as a convenient solution for answering general voice queries and accessing their services. These voice assistants have gained popularity and can be easily accessed through various smart devices such as smartphones, smart speakers, smartwatches, and an increasing array of other devices. However, this convenience comes with potential privacy risks. For instance, while companies vaguely mention in their privacy policies that they may use voice interactions for user profiling, it remains unclear to what extent this profiling occurs and whether voice interactions pose greater privacy risks compared to other interaction modalities.\n",
            "In this paper, we conduct 1171 experiments involving a total of 24530 queries with different personas and interaction modalities over the course of 20 months to characterize how the three most popular voice assistants profile their users. We analyze factors such as the labels assigned to users, their accuracy, the time taken to assign these labels, differences between voice and web interactions, and the effectiveness of profiling remediation tools offered by each voice assistant. Our findings reveal that profiling can happen without interaction, can be incorrect and inconsistent at times, may take several days to weeks for changes to occur, and can be influenced by the interaction modality.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 319|||| \n",
            "'arxiv_id': arXiv:2409.07453, \n",
            "'paper_link': https://arxiv.org/abs/2409.07453, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07453, \n",
            "Title: \"My Grade is Wrong!\": A Contestable AI Framework for Interactive Feedback in Evaluating Student Essays \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: Interactive feedback, where feedback flows in both directions between teacher and student, is more effective than traditional one-way feedback. However, it is often too time-consuming for widespread use in educational practice. While Large Language Models (LLMs) have potential for automating feedback, they struggle with reasoning and interaction in an interactive setting. This paper introduces CAELF, a Contestable AI Empowered LLM Framework for automating interactive feedback. CAELF allows students to query, challenge, and clarify their feedback by integrating a multi-agent system with computational argumentation. Essays are first assessed by multiple Teaching-Assistant Agents (TA Agents), and then a Teacher Agent aggregates the evaluations through formal reasoning to generate feedback and grades. Students can further engage with the feedback to refine their understanding. A case study on 500 critical thinking essays with user studies demonstrates that CAELF significantly improves interactive feedback, enhancing the reasoning and interaction capabilities of LLMs. This approach offers a promising solution to overcoming the time and resource barriers that have limited the adoption of interactive feedback in educational settings.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 356|||| \n",
            "'arxiv_id': arXiv:2409.07151, \n",
            "'paper_link': https://arxiv.org/abs/2409.07151, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07151, \n",
            "Title: Zero-Shot Text-to-Speech as Golden Speech Generator: A Systematic Framework and its Applicability in Automatic Pronunciation Assessment \n",
            "Subjects: Audio and Speech Processing (eess.AS) \n",
            "Abstract: Second language (L2) learners can improve their pronunciation by imitating golden speech, especially when the speech that aligns with their respective speech characteristics. This study explores the hypothesis that learner-specific golden speech generated with zero-shot text-to-speech (ZS-TTS) techniques can be harnessed as an effective metric for measuring the pronunciation proficiency of L2 learners. Building on this exploration, the contributions of this study are at least two-fold: 1) design and development of a systematic framework for assessing the ability of a synthesis model to generate golden speech, and 2) in-depth investigations of the effectiveness of using golden speech in automatic pronunciation assessment (APA). Comprehensive experiments conducted on the L2-ARCTIC and Speechocean762 benchmark datasets suggest that our proposed modeling can yield significant performance improvements with respect to various assessment metrics in relation to some prior arts. To our knowledge, this study is the first to explore the role of golden speech in both ZS-TTS and APA, offering a promising regime for computer-assisted pronunciation training (CAPT).\n",
            "Score: 0.5\n",
            "\n",
            "Document: 428|||| \n",
            "'arxiv_id': arXiv:2401.15106, \n",
            "'paper_link': https://arxiv.org/abs/2401.15106, \n",
            "'pdf_link': https://arxiv.org/pdf/2401.15106, \n",
            "Title: Decision Theoretic Foundations for Experiments Evaluating Human Decisions \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: How well people use information displays to make decisions is of primary interest in human-centered AI, model explainability, data visualization, and related areas. However, what constitutes a decision problem, and what is required for a study to establish that human decisions could be improved remain open to speculation. We propose a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics as a standard for establishing when human decisions can be improved in HCI. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the utility-maximizing decision. As a demonstration, we evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve these criteria. We find that only 10 (26\\%) of 39 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making in at least one treatment condition. We motivate the value of studying well-defined decision problems by describing a characterization of performance losses they allow us to conceive. In contrast, the ambiguities of a poorly communicated decision problem preclude normative interpretation. We conclude with recommendations for practice.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 454|||| \n",
            "'arxiv_id': arXiv:2404.00047, \n",
            "'paper_link': https://arxiv.org/abs/2404.00047, \n",
            "'pdf_link': https://arxiv.org/pdf/2404.00047, \n",
            "Title: Foundational guidelines for enhancing neurotechnology research and development through end-user involvement \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Neurotechnologies are increasingly becoming integrated with our everyday lives, our bodies and our mental states. As the popularity and impact of neurotechnology grows, so does our responsibility to ensure we understand its particular implications on its end users, as well as broader ethical and societal implications. Enabling end-users and stakeholders to participate in the development of neurotechnology, from its earliest stages of conception, will help us better navigate our design around these considerations and deliver more impactful technologies. There are many terms and frameworks to articulate the concept of involving end users in the technology development lifecycle, for example: 'Public and Patient Involvement and Engagement' (PPIE), 'lived experience' and 'co-design'. Here we utilise the PPIE framework to develop clear guidelines for implementing a robust involvement process of current and future end-users in neurotechnology. We present best practice guidance for researchers and engineers who are interested in developing and conducting a PPI strategy for their neurotechnology. We provide advice from various online sources to orient individual teams (and funders) to carve up their own approach to meaningful involvement. After an introduction that coveys the tangible and conceptual benefits of user involvement, we guide the reader to develop a general strategy towards setting up their own process. We then help the reader map out their relevant stakeholders and provide advice on how to consider user diversity and representation. We also provide advice on how to quantify the outcomes of the engagement, as well as a check-list to ensure transparency and accountability at various stages. The aim is the establishment of gold-standard methodologies for ensuring that patient and public insights are at the forefront of our scientific inquiry and product development.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 473|||| \n",
            "'arxiv_id': arXiv:2405.07267, \n",
            "'paper_link': https://arxiv.org/abs/2405.07267, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.07267, \n",
            "Title: Fields, Bridges, and Foundations: How Researchers Browse Citation Network Visualizations \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Visualizing citation relations with network structures is widely used, but the visual complexity can make it challenging for individual researchers trying to navigate them. We collected data from 18 researchers with an interface that we designed using network simplification methods and analyzed how users browsed and identified important papers. Our analysis reveals six major patterns used for identifying papers of interest, which can be categorized into three key components: Fields, Bridges, and Foundations, each viewed from two distinct perspectives: layout-oriented and connection-oriented. The connection-oriented approach was found to be more reliable for selecting relevant papers, but the layout-oriented method was adopted more often, even though it led to unexpected results and user frustration. Our findings emphasize the importance of integrating these components and the necessity to balance visual layouts with meaningful connections to enhance the effectiveness of citation networks in academic browsing systems.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 478|||| \n",
            "'arxiv_id': arXiv:2405.12312, \n",
            "'paper_link': https://arxiv.org/abs/2405.12312, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.12312, \n",
            "Title: A Principled Approach for a New Bias Measure \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: The widespread use of machine learning and data-driven algorithms for decision making has been steadily increasing over many years. The areas in which this is happening are diverse: healthcare, employment, finance, education, the legal system to name a few; and the associated negative side effects are being increasingly harmful for society. Negative data \\emph{bias} is one of those, which tends to result in harmful consequences for specific groups of people. Any mitigation strategy or effective policy that addresses the negative consequences of bias must start with awareness that bias exists, together with a way to understand and quantify it. However, there is a lack of consensus on how to measure data bias and oftentimes the intended meaning is context dependent and not uniform within the research community. The main contributions of our work are: (1) The definition of Uniform Bias (UB), the first bias measure with a clear and simple interpretation in the full range of bias values. (2) A systematic study to characterize the flaws of existing measures in the context of anti employment discrimination rules used by the Office of Federal Contract Compliance Programs, additionally showing how UB solves open problems in this domain. (3) A framework that provides an efficient way to derive a mathematical formula for a bias measure based on an algorithmic specification of bias addition. Our results are experimentally validated using nine publicly available datasets and theoretically analyzed, which provide novel insights about the problem. Based on our approach, we also design a bias mitigation model that might be useful to policymakers.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 507|||| \n",
            "'arxiv_id': arXiv:2407.07786, \n",
            "'paper_link': https://arxiv.org/abs/2407.07786, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.07786, \n",
            "Title: The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Rapid progress in general-purpose AI has sparked significant interest in \"red teaming,\" a practice of adversarial testing originating in military and cybersecurity applications. AI red teaming raises many questions about the human factor, such as how red teamers are selected, biases and blindspots in how tests are conducted, and harmful content's psychological effects on red teamers. A growing body of HCI and CSCW literature examines related practices-including data labeling, content moderation, and algorithmic auditing. However, few, if any have investigated red teaming itself. Future studies may explore topics ranging from fairness to mental health and other areas of potential harm. We aim to facilitate a community of researchers and practitioners who can begin to meet these challenges with creativity, innovation, and thoughtful reflection.\n",
            "Score: 0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"collective behavior\", 1),\n",
        "        (\"collective\", 1),\n",
        "        (\"coordination\", 1),\n",
        "        (\"oganization\", 1),\n",
        "        (\"behavior\", 1),\n",
        "        (\"ants\", 1),\n",
        "        (\"insects\", 1),\n",
        "        (\"worms\", 1),\n",
        "        (\"swarm\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "id": "9zPAIaz5xzmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce2ac95d-75b6-4e74-86f1-e57b0b58ba70"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: collective behavior\n",
            "Total Matches in Set: 78\n",
            "Matches Above Score-Floor in Set: 5\n",
            "2024-09-12__120157772654\n",
            "\n",
            "Showing 5 in top-45 out of 78 total results.     -> 5 of 45/78\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 26|||| \n",
            "'arxiv_id': arXiv:2409.06750, \n",
            "'paper_link': https://arxiv.org/abs/2409.06750, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06750, \n",
            "Title: Can Agents Spontaneously Form a Society? Introducing a Novel Architecture for Generative Multi-Agents to Elicit Social Emergence \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: Generative agents have demonstrated impressive capabilities in specific tasks, but most of these frameworks focus on independent tasks and lack attention to social interactions. We introduce a generative agent architecture called ITCMA-S, which includes a basic framework for individual agents and a framework called LTRHA that supports social interactions among multi-agents. This architecture enables agents to identify and filter out behaviors that are detrimental to social interactions, guiding them to choose more favorable actions. We designed a sandbox environment to simulate the natural evolution of social relationships among multiple identity-less agents for experimental evaluation. The results showed that ITCMA-S performed well on multiple evaluation indicators, demonstrating its ability to actively explore the environment, recognize new agents, and acquire new information through continuous actions and dialogue. Observations show that as agents establish connections with each other, they spontaneously form cliques with internal hierarchies around a selected leader and organize collective activities.\n",
            "Score: 2\n",
            "\n",
            "Document: 294|||| \n",
            "'arxiv_id': arXiv:2409.07406, \n",
            "'paper_link': https://arxiv.org/abs/2409.07406, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07406, \n",
            "Title: Trust Dynamics in Human-Autonomy Interaction: Uncover Associations between Trust Dynamics and Personal Characteristics \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: While personal characteristics influence people's snapshot trust towards autonomous systems, their relationships with trust dynamics remain poorly understood. We conducted a human-subject experiment with 130 participants performing a simulated surveillance task aided by an automated threat detector. A comprehensive pre-experimental survey collected data on participants' personal characteristics across 12 constructs and 28 dimensions. Based on data collected in the experiment, we clustered participants' trust dynamics into three types and assessed differences among the three clusters in terms of personal characteristics, behaviors, performance, and post-experiment ratings. Participants were clustered into three groups, namely Bayesian decision makers, disbelievers, and oscillators. Results showed that the clusters differ significantly in seven personal characteristics: masculinity, positive affect, extraversion, neuroticism, intellect, performance expectancy, and high expectations. The disbelievers tend to have high neuroticism and low performance expectancy. The oscillators tend to have higher scores in masculinity, positive affect, extraversion and intellect. We also found significant differences in the behaviors and post-experiment ratings among the three groups. The disbelievers are the least likely to blindly follow the recommendations made by the automated threat detector. Based on the significant personal characteristics, we developed a decision tree model to predict cluster types with an accuracy of 70%.\n",
            "Score: 2\n",
            "\n",
            "Document: 340|||| \n",
            "'arxiv_id': arXiv:2409.07000, \n",
            "'paper_link': https://arxiv.org/abs/2409.07000, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07000, \n",
            "Title: Introducing UNIQuE: The Unconventional Noiseless Intermediate Quantum Emulator \n",
            "Subjects: Quantum Physics (quant-ph) \n",
            "Abstract: We implement the first open-source quantum computing emulator that includes arithmetic operations, the quantum Fourier transform, and quantum phase estimation. The emulator provides significant savings in both temporal and spatial resources compared to simulation, and these computational advantages are verified through comparison to the Intel Quantum Simulator. We also demonstrate how to use the emulator to implement Shor's algorithm and use it to solve a nontrivial factoring problem. This demonstrates that emulation can make quantum computing more accessible than simulation or noisy hardware by allowing researchers to study the behavior of algorithms on large problems in a noiseless environment.\n",
            "Score: 2\n",
            "\n",
            "Document: 406|||| \n",
            "'arxiv_id': arXiv:2308.05731, \n",
            "'paper_link': https://arxiv.org/abs/2308.05731, \n",
            "'pdf_link': https://arxiv.org/pdf/2308.05731, \n",
            "Title: The Integration of Prediction and Planning in Deep Learning Automated Driving Systems: A Review \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Automated driving has the potential to revolutionize personal, public, and freight mobility. Beside accurately perceiving the environment, automated vehicles must plan a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential, separate tasks. While this accounts for the influence of surrounding traffic on the ego vehicle, it fails to anticipate the reactions of traffic participants to the ego vehicle's behavior. Recent methods increasingly integrate prediction and planning in a joint or interdependent step to model bidirectional interactions. To date, a comprehensive overview of different integration principles is lacking. We systematically review state-of-the-art deep learning-based planning systems, and focus on how they integrate prediction. Different facets of the integration ranging from system architecture to high-level behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration principles. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.\n",
            "Score: 2\n",
            "\n",
            "Document: 428|||| \n",
            "'arxiv_id': arXiv:2401.15106, \n",
            "'paper_link': https://arxiv.org/abs/2401.15106, \n",
            "'pdf_link': https://arxiv.org/pdf/2401.15106, \n",
            "Title: Decision Theoretic Foundations for Experiments Evaluating Human Decisions \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: How well people use information displays to make decisions is of primary interest in human-centered AI, model explainability, data visualization, and related areas. However, what constitutes a decision problem, and what is required for a study to establish that human decisions could be improved remain open to speculation. We propose a widely applicable definition of a decision problem synthesized from statistical decision theory and information economics as a standard for establishing when human decisions can be improved in HCI. We argue that to attribute loss in human performance to forms of bias, an experiment must provide participants with the information that a rational agent would need to identify the utility-maximizing decision. As a demonstration, we evaluate the extent to which recent evaluations of decision-making from the literature on AI-assisted decisions achieve these criteria. We find that only 10 (26\\%) of 39 studies that claim to identify biased behavior present participants with sufficient information to characterize their behavior as deviating from good decision-making in at least one treatment condition. We motivate the value of studying well-defined decision problems by describing a characterization of performance losses they allow us to conceive. In contrast, the ambiguities of a poorly communicated decision problem preclude normative interpretation. We conclude with recommendations for practice.\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Retrieval-Augmented Systems\", 1),\n",
        "        (\"RAG systems\", 1),\n",
        "        (\"Retrieval-Augmented Generation\", 1),\n",
        "        (\"RAG evaluation metric \", 3),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oZ9kzdx6xhG",
        "outputId": "3588f281-2717-4450-8536-5cbe98c14614"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Retrieval-Augmented Systems\n",
            "Total Matches in Set: 2\n",
            "Matches Above Score-Floor in Set: 0\n",
            "2024-09-12__120158133029\n",
            "\n",
            "Showing 0 in top-2 out of 2 total results.     -> 0 of 2/2\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"mental health\", 5),\n",
        "        (\"psychological health\", 5),\n",
        "        (\"psycholog\", 2),  # stem vs. lemma\n",
        "        (\"mental health care\", 3),\n",
        "        (\"neuroscience\", 2),\n",
        "        (\"psychological assessment\", 2),\n",
        "        (\"personality assessment\", 2),\n",
        "        (\"personality inference\", 2),\n",
        "        (\"personality traits\", 2),\n",
        "        (\"personality dimensions\", 2),\n",
        "        (\"emotion\", 15),\n",
        "        (\"sports psychology\", 15),\n",
        "        (\"sentiment recognition\", 10),\n",
        "        (\"Emotion Recognition\", 5),\n",
        "        # (\"\", 5),\n",
        "        # (\"\", 5),\n",
        "\n",
        "        # disease terms\n",
        "        (\"depression\", 5),\n",
        "        (\"anxiety\", 5),\n",
        "        (\"mental disorders\", 2),\n",
        "        (\"social anxiety disorder\", 4),\n",
        "        (\"mental illness\", 2),\n",
        "        (\"Major Depressive Disorder\", 2),\n",
        "        (\"MDD\", 2),\n",
        "        (\"psychological stressors\", 2),\n",
        "        (\"cognitive impairment\", 2),\n",
        "        (\"mci\", 2),\n",
        "        (\"personality\", 1)\n",
        "        # (\"\", 2),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "id": "CixuXw-Fl3-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1673dc6-2301-446b-bdc5-38368b901e6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: mental health\n",
            "Total Matches in Set: 20\n",
            "Matches Above Score-Floor in Set: 20\n",
            "2024-09-12__120158374375\n",
            "\n",
            "Showing 20 in top-20 out of 20 total results.     -> 20 of 20/20\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 154|||| \n",
            "'arxiv_id': arXiv:2409.07078, \n",
            "'paper_link': https://arxiv.org/abs/2409.07078, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07078, \n",
            "Title: Multimodal Emotion Recognition with Vision-language Prompting and Modality Dropout \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: In this paper, we present our solution for the Second Multimodal Emotion Recognition Challenge Track 1(MER2024-SEMI). To enhance the accuracy and generalization performance of emotion recognition, we propose several methods for Multimodal Emotion Recognition. Firstly, we introduce EmoVCLIP, a model fine-tuned based on CLIP using vision-language prompt learning, designed for video-based emotion recognition tasks. By leveraging prompt learning on CLIP, EmoVCLIP improves the performance of pre-trained CLIP on emotional videos. Additionally, to address the issue of modality dependence in multimodal fusion, we employ modality dropout for robust information fusion. Furthermore, to aid Baichuan in better extracting emotional information, we suggest using GPT-4 as the prompt for Baichuan. Lastly, we utilize a self-training strategy to leverage unlabeled videos. In this process, we use unlabeled videos with high-confidence pseudo-labels generated by our model and incorporate them into the training set. Experimental results demonstrate that our model ranks 1st in the MER2024-SEMI track, achieving an accuracy of 90.15% on the test set.\n",
            "Score: 20\n",
            "\n",
            "Document: 286|||| \n",
            "'arxiv_id': arXiv:2409.07388, \n",
            "'paper_link': https://arxiv.org/abs/2409.07388, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07388, \n",
            "Title: Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Multimodal affective computing (MAC) has garnered increasing attention due to its broad applications in analyzing human behaviors and intentions, especially in text-dominated multimodal affective computing field. This survey presents the recent trends of multimodal affective computing from NLP perspective through four hot tasks: multimodal sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based sentiment analysis and multimodal multi-label emotion recognition. The goal of this survey is to explore the current landscape of multimodal affective research, identify development trends, and highlight the similarities and differences across various tasks, offering a comprehensive report on the recent progress in multimodal affective computing from an NLP perspective. This survey covers the formalization of tasks, provides an overview of relevant works, describes benchmark datasets, and details the evaluation metrics for each task. Additionally, it briefly discusses research in multimodal affective computing involving facial expressions, acoustic signals, physiological signals, and emotion causes. Additionally, we discuss the technical approaches, challenges, and future directions in multimodal affective computing. To support further research, we released a repository that compiles related works in multimodal affective computing, providing detailed resources and references for the community.\n",
            "Score: 20\n",
            "\n",
            "Document: 271|||| \n",
            "'arxiv_id': arXiv:2409.07341, \n",
            "'paper_link': https://arxiv.org/abs/2409.07341, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07341, \n",
            "Title: Online Decision MetaMorphFormer: A Casual Transformer-Based Reinforcement Learning Framework of Universal Embodied Intelligence \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Interactive artificial intelligence in the motion control field is an interesting topic, especially when universal knowledge is adaptive to multiple tasks and universal environments. Despite there being increasing efforts in the field of Reinforcement Learning (RL) with the aid of transformers, most of them might be limited by the offline training pipeline, which prohibits exploration and generalization abilities. To address this limitation, we propose the framework of Online Decision MetaMorphFormer (ODM) which aims to achieve self-awareness, environment recognition, and action planning through a unified model architecture. Motivated by cognitive and behavioral psychology, an ODM agent is able to learn from others, recognize the world, and practice itself based on its own experience. ODM can also be applied to any arbitrary agent with a multi-joint body, located in different environments, and trained with different types of tasks using large-scale pre-trained datasets. Through the use of pre-trained datasets, ODM can quickly warm up and learn the necessary knowledge to perform the desired task, while the target environment continues to reinforce the universal policy. Extensive online experiments as well as few-shot and zero-shot environmental tests are used to verify ODM's performance and generalization ability. The results of our study contribute to the study of general artificial intelligence in embodied and cognitive fields. Code, results, and video examples can be found on the website \\url{this https URL}.\n",
            "Score: 17\n",
            "\n",
            "Document: 67|||| \n",
            "'arxiv_id': arXiv:2409.06863, \n",
            "'paper_link': https://arxiv.org/abs/2409.06863, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06863, \n",
            "Title: Towards Understanding Human Emotional Fluctuations with Sparse Check-In Data \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Data sparsity is a key challenge limiting the power of AI tools across various domains. The problem is especially pronounced in domains that require active user input rather than measurements derived from automated sensors. It is a critical barrier to harnessing the full potential of AI in domains requiring active user engagement, such as self-reported mood check-ins, where capturing a continuous picture of emotional states is essential. In this context, sparse data can hinder efforts to capture the nuances of individual emotional experiences such as causes, triggers, and contributing factors. Existing methods for addressing data scarcity often rely on heuristics or large established datasets, favoring deep learning models that lack adaptability to new domains. This paper proposes a novel probabilistic framework that integrates user-centric feedback-based learning, allowing for personalized predictions despite limited data. Achieving 60% accuracy in predicting user states among 64 options (chance of 1/64), this framework effectively mitigates data sparsity. It is versatile across various applications, bridging the gap between theoretical AI research and practical deployment.\n",
            "Score: 15\n",
            "\n",
            "Document: 233|||| \n",
            "'arxiv_id': arXiv:2409.07255, \n",
            "'paper_link': https://arxiv.org/abs/2409.07255, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07255, \n",
            "Title: EMOdiffhead: Continuously Emotional Control in Talking Head Generation via Diffusion \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The task of audio-driven portrait animation involves generating a talking head video using an identity image and an audio track of speech. While many existing approaches focus on lip synchronization and video quality, few tackle the challenge of generating emotion-driven talking head videos. The ability to control and edit emotions is essential for producing expressive and realistic animations. In response to this challenge, we propose EMOdiffhead, a novel method for emotional talking head video generation that not only enables fine-grained control of emotion categories and intensities but also enables one-shot generation. Given the FLAME 3D model's linearity in expression modeling, we utilize the DECA method to extract expression vectors, that are combined with audio to guide a diffusion model in generating videos with precise lip synchronization and rich emotional expressiveness. This approach not only enables the learning of rich facial information from emotion-irrelevant data but also facilitates the generation of emotional videos. It effectively overcomes the limitations of emotional data, such as the lack of diversity in facial and background information, and addresses the absence of emotional details in emotion-irrelevant data. Extensive experiments and user studies demonstrate that our approach achieves state-of-the-art performance compared to other emotion portrait animation methods.\n",
            "Score: 15\n",
            "\n",
            "Document: 234|||| \n",
            "'arxiv_id': arXiv:2409.07256, \n",
            "'paper_link': https://arxiv.org/abs/2409.07256, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07256, \n",
            "Title: MRAC Track 1: 2nd Workshop on Multimodal, Generative and Responsible Affective Computing \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: With the rapid advancements in multimodal generative technology, Affective Computing research has provoked discussion about the potential consequences of AI systems equipped with emotional intelligence. Affective Computing involves the design, evaluation, and implementation of Emotion AI and related technologies aimed at improving people's lives. Designing a computational model in affective computing requires vast amounts of multimodal data, including RGB images, video, audio, text, and physiological signals. Moreover, Affective Computing research is deeply engaged with ethical considerations at various stages-from training emotionally intelligent models on large-scale human data to deploying these models in specific applications. Fundamentally, the development of any AI system must prioritize its impact on humans, aiming to augment and enhance human abilities rather than replace them, while drawing inspiration from human intelligence in a safe and responsible manner. The MRAC 2024 Track 1 workshop seeks to extend these principles from controlled, small-scale lab environments to real-world, large-scale contexts, emphasizing responsible development. The workshop also aims to highlight the potential implications of generative technology, along with the ethical consequences of its use, to researchers and industry professionals. To the best of our knowledge, this is the first workshop series to comprehensively address the full spectrum of multimodal, generative affective computing from a responsible AI perspective, and this is the second iteration of this workshop. Webpage: this https URL\n",
            "Score: 15\n",
            "\n",
            "Document: 308|||| \n",
            "'arxiv_id': arXiv:2409.07437, \n",
            "'paper_link': https://arxiv.org/abs/2409.07437, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07437, \n",
            "Title: A Suite for Acoustic Language Model Evaluation \n",
            "Subjects: Sound (cs.SD) \n",
            "Abstract: Speech language models have recently demonstrated great potential as universal speech processing systems. Such models have the ability to model the rich acoustic information existing in audio signals, beyond spoken content, such as emotion, background noise, etc. Despite this, evaluation benchmarks which evaluate awareness to a wide range of acoustic aspects, are lacking. To help bridge this gap, we introduce SALMon, a novel evaluation suite encompassing background noise, emotion, speaker identity and room impulse response. The proposed benchmarks both evaluate the consistency of the inspected element and how much it matches the spoken text. We follow a modelling based approach, measuring whether a model gives correct samples higher scores than incorrect ones. This approach makes the benchmark fast to compute even for large models. We evaluated several speech language models on SALMon, thus highlighting the strengths and weaknesses of each evaluated method. Code and data are publicly available at this https URL .\n",
            "Score: 15\n",
            "\n",
            "Document: 406|||| \n",
            "'arxiv_id': arXiv:2308.05731, \n",
            "'paper_link': https://arxiv.org/abs/2308.05731, \n",
            "'pdf_link': https://arxiv.org/pdf/2308.05731, \n",
            "Title: The Integration of Prediction and Planning in Deep Learning Automated Driving Systems: A Review \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Automated driving has the potential to revolutionize personal, public, and freight mobility. Beside accurately perceiving the environment, automated vehicles must plan a safe, comfortable, and efficient motion trajectory. To promote safety and progress, many works rely on modules that predict the future motion of surrounding traffic. Modular automated driving systems commonly handle prediction and planning as sequential, separate tasks. While this accounts for the influence of surrounding traffic on the ego vehicle, it fails to anticipate the reactions of traffic participants to the ego vehicle's behavior. Recent methods increasingly integrate prediction and planning in a joint or interdependent step to model bidirectional interactions. To date, a comprehensive overview of different integration principles is lacking. We systematically review state-of-the-art deep learning-based planning systems, and focus on how they integrate prediction. Different facets of the integration ranging from system architecture to high-level behavioral aspects are considered and related to each other. Moreover, we discuss the implications, strengths, and limitations of different integration principles. By pointing out research gaps, describing relevant future challenges, and highlighting trends in the research field, we identify promising directions for future research.\n",
            "Score: 15\n",
            "\n",
            "Document: 446|||| \n",
            "'arxiv_id': arXiv:2403.12886, \n",
            "'paper_link': https://arxiv.org/abs/2403.12886, \n",
            "'pdf_link': https://arxiv.org/pdf/2403.12886, \n",
            "Title: EmoVOCA: Speech-Driven Emotional 3D Talking Heads \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The domain of 3D talking head generation has witnessed significant progress in recent years. A notable challenge in this field consists in blending speech-related motions with expression dynamics, which is primarily caused by the lack of comprehensive 3D datasets that combine diversity in spoken sentences with a variety of facial expressions. Whereas literature works attempted to exploit 2D video data and parametric 3D models as a workaround, these still show limitations when jointly modeling the two motions. In this work, we address this problem from a different perspective, and propose an innovative data-driven technique that we used for creating a synthetic dataset, called EmoVOCA, obtained by combining a collection of inexpressive 3D talking heads and a set of 3D expressive sequences. To demonstrate the advantages of this approach, and the quality of the dataset, we then designed and trained an emotional 3D talking head generator that accepts a 3D face, an audio file, an emotion label, and an intensity value as inputs, and learns to animate the audio-synchronized lip movements with expressive traits of the face. Comprehensive experiments, both quantitative and qualitative, using our data and generator evidence superior ability in synthesizing convincing animations, when compared with the best performing methods in the literature. Our code and pre-trained model will be made available.\n",
            "Score: 15\n",
            "\n",
            "Document: 456|||| \n",
            "'arxiv_id': arXiv:2404.01900, \n",
            "'paper_link': https://arxiv.org/abs/2404.01900, \n",
            "'pdf_link': https://arxiv.org/pdf/2404.01900, \n",
            "Title: Automatic Derivation of an Optimal Task Frame for Learning and Controlling Contact-Rich Tasks \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: In previous work on learning and controlling contact-rich tasks, the procedure for choosing a proper reference frame to express learned signals for the motion and the interaction wrench is often implicit, requires expert insight, or starts from proposed frame candidates. This article presents an automatic method to derive the optimal reference frame, referred to as optimal task frame, directly from the recorded motion and wrench data of the demonstration. Using screw theory, several origin and orientation candidates are generated that maximize decoupling in the data. These candidates are then processed probabilistically, without needing hyperparameters, to obtain the optimal task frame. Its origin and orientation are independently fixed to either the world or the robot tool. The method works regardless of whether the task involves translation, rotation, force, or moment, or any combination thereof. The method was validated for various tasks, including surface following and manipulation of articulated objects, showing good agreement between derived and assumed expert task frames. To validate the robot's performance, a constraint-based controller was designed based on the data expressed in the derived task frames. These experiments demonstrated the approach's effectiveness and versatility. The automatic task frame derivation approach supports learning methods to design controllers for a wide range of contact-rich tasks.\n",
            "Score: 15\n",
            "\n",
            "Document: 515|||| \n",
            "'arxiv_id': arXiv:2407.21500, \n",
            "'paper_link': https://arxiv.org/abs/2407.21500, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.21500, \n",
            "Title: DIABLO: A 6-DoF Wheeled Bipedal Robot Composed Entirely of Direct-Drive Joints \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Wheeled bipedal robots offer the advantages of both wheeled and legged robots, combining the ability to traverse a wide range of terrains and environments with high efficiency. However, the conventional approach in existing wheeled bipedal robots involves motor-driven joints with high-ratio gearboxes. While this approach provides specific benefits, it also presents several challenges, including increased mechanical complexity, efficiency losses, noise, vibrations, and higher maintenance and lubrication requirements. Addressing the aforementioned concerns, we developed a direct-drive wheeled bipedal robot called DIABLO, which eliminates the use of gearboxes entirely. Our robotic system is simplified as a second-order inverted pendulum, and we have designed an LQR-based balance controller to ensure stability. Additionally, we implemented comprehensive motion controller, including yaw, split-angle, height, and roll controllers. Through expriments in simulations and real-world prototype, we have demonstrated that our platform achieves satisfactory performance.\n",
            "Score: 15\n",
            "\n",
            "Document: 538|||| \n",
            "'arxiv_id': arXiv:2409.00143, \n",
            "'paper_link': https://arxiv.org/abs/2409.00143, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.00143, \n",
            "Title: Semantic-Guided Multimodal Sentiment Decoding with Adversarial Temporal-Invariant Learning \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Multimodal sentiment analysis aims to learn representations from different modalities to identify human emotions. However, existing works often neglect the frame-level redundancy inherent in continuous time series, resulting in incomplete modality representations with noise. To address this issue, we propose temporal-invariant learning for the first time, which constrains the distributional variations over time steps to effectively capture long-term temporal dynamics, thus enhancing the quality of the representations and the robustness of the model. To fully exploit the rich semantic information in textual knowledge, we propose a semantic-guided fusion module. By evaluating the correlations between different modalities, this module facilitates cross-modal interactions gated by modality-invariant representations. Furthermore, we introduce a modality discriminator to disentangle modality-invariant and modality-specific subspaces. Experimental results on two public datasets demonstrate the superiority of our model. Our code is available at this https URL.\n",
            "Score: 15\n",
            "\n",
            "Document: 1|||| \n",
            "'arxiv_id': arXiv:2409.06706, \n",
            "'paper_link': https://arxiv.org/abs/2409.06706, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06706, \n",
            "Title: Discovering Long-Term Effects on Parameter Efficient Fine-tuning \n",
            "Subjects: Neural and Evolutionary Computing (cs.NE) \n",
            "Abstract: Pre-trained Artificial Neural Networks (ANNs) exhibit robust pattern recognition capabilities and share extensive similarities with the human brain, specifically Biological Neural Networks (BNNs). We are particularly intrigued by these models' ability to acquire new knowledge through fine-tuning. In this regard, Parameter-efficient Fine-tuning (PEFT) has gained widespread adoption as a substitute for full fine-tuning due to its cost reduction in training and mitigation of over-fitting risks by limiting the number of trainable parameters during adaptation. Since both ANNs and BNNs propagate information layer-by-layer, a common analogy can be drawn: weights in ANNs represent synapses in BNNs, while features (also known as latent variables or logits) in ANNs represent neurotransmitters released by neurons in BNNs. Mainstream PEFT methods aim to adjust feature or parameter values using only a limited number of trainable parameters (usually less than 1% of the total parameters), yet achieve surprisingly good results. Building upon this clue, we delve deeper into exploring the connections between feature adjustment and parameter adjustment, resulting in our proposed method Synapses & Neurons (SAN) that learns scaling matrices for features and propagates their effects towards posterior weight matrices. Our approach draws strong inspiration from well-known neuroscience phenomena - Long-term Potentiation (LTP) and Long-term Depression (LTD), which also reveal the relationship between synapse development and neurotransmitter release levels. We conducted extensive comparisons of PEFT on 26 datasets using attention-based networks as well as convolution-based networks, leading to significant improvements compared to other tuning methods (+8.5% over fully-finetune, +7% over Visual Prompt Tuning, and +3.2% over LoRA). The codes would be released.\n",
            "Score: 7\n",
            "\n",
            "Document: 507|||| \n",
            "'arxiv_id': arXiv:2407.07786, \n",
            "'paper_link': https://arxiv.org/abs/2407.07786, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.07786, \n",
            "Title: The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Rapid progress in general-purpose AI has sparked significant interest in \"red teaming,\" a practice of adversarial testing originating in military and cybersecurity applications. AI red teaming raises many questions about the human factor, such as how red teamers are selected, biases and blindspots in how tests are conducted, and harmful content's psychological effects on red teamers. A growing body of HCI and CSCW literature examines related practices-including data labeling, content moderation, and algorithmic auditing. However, few, if any have investigated red teaming itself. Future studies may explore topics ranging from fairness to mental health and other areas of potential harm. We aim to facilitate a community of researchers and practitioners who can begin to meet these challenges with creativity, innovation, and thoughtful reflection.\n",
            "Score: 7\n",
            "\n",
            "Document: 367|||| \n",
            "'arxiv_id': arXiv:2409.07347, \n",
            "'paper_link': https://arxiv.org/abs/2409.07347, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07347, \n",
            "Title: The Role of Explainable AI in Revolutionizing Human Health Monitoring \n",
            "Subjects: Signal Processing (eess.SP) \n",
            "Abstract: The complex nature of disease mechanisms and the variability of patient symptoms present significant obstacles in developing effective diagnostic tools. Although machine learning has made considerable advances in medical diagnosis, its decision-making processes frequently lack transparency, which can jeopardize patient outcomes. This underscores the critical need for Explainable AI (XAI), which not only offers greater clarity but also has the potential to significantly improve patient care. In this literature review, we conduct a detailed analysis of analyzing XAI methods identified through searches across various databases, focusing on chronic conditions such as Parkinson's, stroke, depression, cancer, heart disease, and Alzheimer's disease. The literature search revealed the application of 9 trending XAI algorithms in the field of healthcare and highlighted the pros and cons of each of them. Thus, the article is concluded with a critical appraisal of the challenges and future research opportunities for XAI in human health monitoring.\n",
            "Score: 5\n",
            "\n",
            "Document: 557|||| \n",
            "'arxiv_id': arXiv:2409.05292, \n",
            "'paper_link': https://arxiv.org/abs/2409.05292, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.05292, \n",
            "Title: Mpox Narrative on Instagram: A Labeled Multilingual Dataset of Instagram Posts on Mpox for Sentiment, Hate Speech, and Anxiety Analysis \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: The world is currently experiencing an outbreak of mpox, which has been declared a Public Health Emergency of International Concern by WHO. No prior work related to social media mining has focused on the development of a dataset of Instagram posts about the mpox outbreak. The work presented in this paper aims to address this research gap and makes two scientific contributions to this field. First, it presents a multilingual dataset of 60,127 Instagram posts about mpox, published between July 23, 2022, and September 5, 2024. The dataset, available at this https URL, contains Instagram posts about mpox in 52 languages. For each of these posts, the Post ID, Post Description, Date of publication, language, and translated version of the post (translation to English was performed using the Google Translate API) are presented as separate attributes in the dataset. After developing this dataset, sentiment analysis, hate speech detection, and anxiety or stress detection were performed. This process included classifying each post into (i) one of the sentiment classes, i.e., fear, surprise, joy, sadness, anger, disgust, or neutral, (ii) hate or not hate, and (iii) anxiety/stress detected or no anxiety/stress detected. These results are presented as separate attributes in the dataset. Second, this paper presents the results of performing sentiment analysis, hate speech analysis, and anxiety or stress analysis. The variation of the sentiment classes - fear, surprise, joy, sadness, anger, disgust, and neutral were observed to be 27.95%, 2.57%, 8.69%, 5.94%, 2.69%, 1.53%, and 50.64%, respectively. In terms of hate speech detection, 95.75% of the posts did not contain hate and the remaining 4.25% of the posts contained hate. Finally, 72.05% of the posts did not indicate any anxiety/stress, and the remaining 27.95% of the posts represented some form of anxiety/stress.\n",
            "Score: 5\n",
            "\n",
            "Document: 18|||| \n",
            "'arxiv_id': arXiv:2409.06729, \n",
            "'paper_link': https://arxiv.org/abs/2409.06729, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.06729, \n",
            "Title: How will advanced AI systems impact democracy? \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Advanced AI systems capable of generating humanlike text and multimodal content are now widely available. In this paper, we discuss the impacts that generative artificial intelligence may have on democratic processes. We consider the consequences of AI for citizens' ability to make informed choices about political representatives and issues (epistemic impacts). We ask how AI might be used to destabilise or support democratic mechanisms like elections (material impacts). Finally, we discuss whether AI will strengthen or weaken democratic principles (foundational impacts). It is widely acknowledged that new AI systems could pose significant challenges for democracy. However, it has also been argued that generative AI offers new opportunities to educate and learn from citizens, strengthen public discourse, help people find common ground, and to reimagine how democracies might work better.\n",
            "Score: 2\n",
            "\n",
            "Document: 186|||| \n",
            "'arxiv_id': arXiv:2409.07148, \n",
            "'paper_link': https://arxiv.org/abs/2409.07148, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.07148, \n",
            "Title: Jump Restore Light Transport \n",
            "Subjects: Graphics (cs.GR) \n",
            "Abstract: Markov chain Monte Carlo (MCMC) algorithms come to rescue when sampling from a complex, high-dimensional distribution by a conventional method is intractable. Even though MCMC is a powerful tool, it is also hard to control and tune in practice. Simultaneously achieving both \\emph{local exploration} of the state space and \\emph{global discovery} of the target distribution is a challenging task. In this work, we present a MCMC formulation that subsumes all existing MCMC samplers employed in rendering. We then present a novel framework for \\emph{adjusting} an arbitrary Markov chain, making it exhibit invariance with respect to a specified target distribution. To showcase the potential of the proposed framework, we focus on a first simple application in light transport simulation. As a by-product, we introduce continuous-time MCMC sampling to the computer graphics community. We show how any existing MCMC-based light transport algorithm can be embedded into our framework. We empirically and theoretically prove that this embedding is superior to running the standalone algorithm. In fact, our approach will convert any existing algorithm into a highly parallelizable variant with shorter running time, smaller error and less variance.\n",
            "Score: 2\n",
            "\n",
            "Document: 400|||| \n",
            "'arxiv_id': arXiv:2305.12992, \n",
            "'paper_link': https://arxiv.org/abs/2305.12992, \n",
            "'pdf_link': https://arxiv.org/pdf/2305.12992, \n",
            "Title: Antithetic multilevel Monte Carlo method for approximations of SDEs with non-globally Lipschitz continuous coefficients \n",
            "Subjects: Numerical Analysis (math.NA) \n",
            "Abstract: In the field of computational finance, one is commonly interested in the expected value of a financial derivative whose payoff depends on the solution of stochastic differential equations (SDEs). For multi-dimensional SDEs with non-commutative diffusion coefficients in the globally Lipschitz setting, a kind of one-half order truncated Milstein-type scheme without Lévy areas was recently introduced by Giles and Szpruch (2014), which combined with the antithetic multilevel Monte Carlo (MLMC) gives the optimal overall computational cost $\\mathcal{O}(\\epsilon^{-2})$ for the required target accuracy $\\epsilon$. Nevertheless, many nonlinear SDEs in applications have non-globally Lipschitz continuous coefficients and the corresponding theoretical guarantees for antithetic MLMC are absent in the literature. In the present work, we aim to fill the gap and analyze antithetic MLMC in a non-globally Lipschitz setting. First, we propose a family of modified Milstein-type schemes without Lévy areas to approximate SDEs with non-globally Lipschitz continuous coefficients. The expected one-half order of strong convergence is recovered in a non-globally Lipschitz setting, where even the diffusion coefficients are allowed to grow superlinearly. This then helps us to analyze the relevant variance of the multilevel estimator and the optimal computational cost is finally achieved for the antithetic MLMC. Since getting rid of the Lévy areas destroys the martingale properties of the scheme, the analysis of both the convergence rate and the desired variance becomes highly non-trivial in the non-globally Lipschitz setting. By introducing an auxiliary approximation process, we develop non-standard arguments to overcome the essential difficulties. Numerical experiments are provided to confirm the theoretical findings.\n",
            "Score: 2\n",
            "\n",
            "Document: 606|||| \n",
            "'arxiv_id': arXiv:2409.01978, \n",
            "'paper_link': https://arxiv.org/abs/2409.01978, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.01978, \n",
            "Title: Application of Langevin Dynamics to Advance the Quantum Natural Gradient Optimization Algorithm \n",
            "Subjects: Quantum Physics (quant-ph) \n",
            "Abstract: A Quantum Natural Gradient (QNG) algorithm for optimization of variational quantum circuits has been proposed recently. In this study, we employ the Langevin equation with a QNG stochastic force to demonstrate that its discrete-time solution gives a generalized form of the above-specified algorithm, which we call Momentum-QNG. Similar to other optimization algorithms with the momentum term, such as the Stochastic Gradient Descent with momentum, RMSProp with momentum and Adam, Momentum-QNG is more effective to escape local minima and plateaus in the variational parameter space and, therefore, achieves a better convergence behavior compared to the basic QNG. Our open-source code is available at this https URL\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Timer"
      ],
      "metadata": {
        "id": "BBVy06WenHk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_time_whole_single_task = datetime.now()\n",
        "duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "print(f\"Duration to run -> {duration_time}\")"
      ],
      "metadata": {
        "id": "cCnRP_8anHbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ce9830-5dd8-4211-a900-eb24f7428c98"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run -> 0_min__9.7_sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See files\n",
        "print(\"List of results saved:\")\n",
        "!ls\n",
        "print(f\"All Articles-Found Results Count = {len(all_results_json_list)}\")"
      ],
      "metadata": {
        "id": "B7IHdEI-v2jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7379e50f-e47b-4b7c-cf5e-2288c250cbd6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of results saved:\n",
            "'Agents for Software Engineering_articles2024-09-12__120157341459.html'\n",
            "'Agents for Software Engineering_articles_2024-09-12__120157341459.json'\n",
            " all_arxiv_article_dicts_2024-09-12__120154531943.json\n",
            " all_arxiv_results_2024-09-12__120154702432.json\n",
            "'collective behavior_articles2024-09-12__120158027208.html'\n",
            "'collective behavior_articles_2024-09-12__120158027208.json'\n",
            " disinformation_articles2024-09-12__120156304703.html\n",
            " disinformation_articles_2024-09-12__120156304703.json\n",
            "'distance measure_articles2024-09-12__120155467598.html'\n",
            "'distance measure_articles_2024-09-12__120155467598.json'\n",
            " e-Learners_articles2024-09-12__120157674038.html\n",
            " e-Learners_articles_2024-09-12__120157674038.json\n",
            "'Manifold Approximation_articles2024-09-12__120155081352.html'\n",
            "'Manifold Approximation_articles_2024-09-12__120155081352.json'\n",
            "'mental health_articles2024-09-12__120158561274.html'\n",
            "'mental health_articles_2024-09-12__120158561274.json'\n",
            "'multiple agents_articles2024-09-12__120156998009.html'\n",
            "'multiple agents_articles_2024-09-12__120156998009.json'\n",
            " parametric_articles2024-09-12__120155693903.html\n",
            " parametric_articles_2024-09-12__120155693903.json\n",
            "'Retrieval-Augmented Systems_articles2024-09-12__120158320863.html'\n",
            "'Retrieval-Augmented Systems_articles_2024-09-12__120158320863.json'\n",
            " sample_data\n",
            " Speech-LLM_articles2024-09-12__120156538826.html\n",
            " Speech-LLM_articles_2024-09-12__120156538826.json\n",
            " survey_articles2024-09-12__120155969637.html\n",
            " survey_articles_2024-09-12__120155969637.json\n",
            "All Articles-Found Results Count = 540\n"
          ]
        }
      ]
    }
  ]
}