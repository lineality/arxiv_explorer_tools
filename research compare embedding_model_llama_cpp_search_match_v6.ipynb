{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495c97f5-b6b7-4c43-8e12-55fea78fc93d",
   "metadata": {},
   "source": [
    "https://huggingface.co/CompendiumLabs/bge-large-en-v1.5-gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e7a29-5a13-4c90-b3a3-f4409a9013b2",
   "metadata": {},
   "source": [
    "- https://pypi.org/project/llama-cpp-python/\n",
    "- https://pypi.org/project/scikit-learn/\n",
    "- https://pypi.org/project/beautifulsoup4/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdea8fa-7a5d-4d32-a88b-1b1f8619e1b3",
   "metadata": {},
   "source": [
    "requirements.txt ->\n",
    "llama-cpp-python\n",
    "scikit-learn \n",
    "scipy \n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a78344-196a-4f51-8003-60c1796051a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q llama-cpp-python\n",
    "# !pip install -q scikit-learn scipy numpy\n",
    "# !pip install -q beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3287d066-1139-4982-9eee-275664aa9576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bge-large-en-v1.5-f32.gguf\n",
      " bge-large-en-v1.5-q4_k_m.gguf\n",
      "'embedding_model_llama_cpp_search_match_v1 (copy).ipynb'\n",
      " embedding_model_llama_cpp_search_match_v1.ipynb\n",
      " embedding_model_llama_cpp_search_match_v2.ipynb\n",
      " embedding_model_llama_cpp_search_match_v3.ipynb\n",
      " embedding_model_llama_cpp_search_match_v4.ipynb\n",
      " embedding_model_llama_cpp_search_match_v5.ipynb\n",
      " embedding_model_llama_cpp_search_match_v6.ipynb\n",
      " embedding_model_llama_cpp_test.ipynb\n",
      " env\n",
      " requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "454642d6-8675-4b5f-bb4b-78d53c2ecd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"bge-large-en-v1.5-f32.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cf9e36-cfe9-4056-a7e6-3432ccd1ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_embed = \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156e8f2f-6772-4049-92f7-40b8ebf84b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 389 tensors from bge-large-en-v1.5-f32.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = bert\n",
      "llama_model_loader: - kv   1:                               general.name str              = bge-large-en-v1.5\n",
      "llama_model_loader: - kv   2:                           bert.block_count u32              = 24\n",
      "llama_model_loader: - kv   3:                        bert.context_length u32              = 512\n",
      "llama_model_loader: - kv   4:                      bert.embedding_length u32              = 1024\n",
      "llama_model_loader: - kv   5:                   bert.feed_forward_length u32              = 4096\n",
      "llama_model_loader: - kv   6:                  bert.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv   7:          bert.attention.layer_norm_epsilon f32              = 0.000000\n",
      "llama_model_loader: - kv   8:                          general.file_type u32              = 0\n",
      "llama_model_loader: - kv   9:                      bert.attention.causal bool             = false\n",
      "llama_model_loader: - kv  10:                          bert.pooling_type u32              = 2\n",
      "llama_model_loader: - kv  11:            tokenizer.ggml.token_type_count u32              = 2\n",
      "llama_model_loader: - kv  12:                tokenizer.ggml.bos_token_id u32              = 101\n",
      "llama_model_loader: - kv  13:                tokenizer.ggml.eos_token_id u32              = 102\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,30522]   = [\"[PAD]\", \"[unused0]\", \"[unused1]\", \"...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100\n",
      "llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.cls_token_id u32              = 101\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103\n",
      "llama_model_loader: - type  f32:  389 tensors\n",
      "llm_load_vocab: special tokens cache size = 5\n",
      "llm_load_vocab: token to piece cache size = 0.2032 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = bert\n",
      "llm_load_print_meta: vocab type       = WPM\n",
      "llm_load_print_meta: n_vocab          = 30522\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 512\n",
      "llm_load_print_meta: n_embd           = 1024\n",
      "llm_load_print_meta: n_head           = 16\n",
      "llm_load_print_meta: n_head_kv        = 16\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 1.0e-12\n",
      "llm_load_print_meta: f_norm_rms_eps   = 0.0e+00\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4096\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 0\n",
      "llm_load_print_meta: pooling type     = 2\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 512\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 335M\n",
      "llm_load_print_meta: model ftype      = all F32\n",
      "llm_load_print_meta: model params     = 334.09 M\n",
      "llm_load_print_meta: model size       = 1.24 GiB (32.00 BPW) \n",
      "llm_load_print_meta: general.name     = bge-large-en-v1.5\n",
      "llm_load_print_meta: BOS token        = 101 '[CLS]'\n",
      "llm_load_print_meta: EOS token        = 102 '[SEP]'\n",
      "llm_load_print_meta: UNK token        = 100 '[UNK]'\n",
      "llm_load_print_meta: SEP token        = 102 '[SEP]'\n",
      "llm_load_print_meta: PAD token        = 0 '[PAD]'\n",
      "llm_load_print_meta: CLS token        = 101 '[CLS]'\n",
      "llm_load_print_meta: MASK token       = 103 '[MASK]'\n",
      "llm_load_print_meta: LF token         = 0 '[PAD]'\n",
      "llm_load_tensors: ggml ctx size =    0.18 MiB\n",
      "llm_load_tensors:        CPU buffer size =  1274.46 MiB\n",
      "................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    48.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    25.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 849\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.mask_token_id': '103', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.seperator_token_id': '102', 'tokenizer.ggml.unknown_token_id': '100', 'tokenizer.ggml.model': 'bert', 'tokenizer.ggml.eos_token_id': '102', 'general.architecture': 'bert', 'bert.block_count': '24', 'bert.attention.layer_norm_epsilon': '0.000000', 'bert.context_length': '512', 'bert.feed_forward_length': '4096', 'bert.embedding_length': '1024', 'tokenizer.ggml.cls_token_id': '101', 'tokenizer.ggml.token_type_count': '2', 'bert.attention.head_count': '16', 'tokenizer.ggml.bos_token_id': '101', 'general.file_type': '0', 'general.name': 'bge-large-en-v1.5', 'bert.attention.causal': 'false', 'bert.pooling_type': '2'}\n",
      "Using fallback chat format: llama-2\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "model = Llama(model_path, embedding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf06e99-72f8-4cf1-99bf-e6cdd8bd1f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     105.34 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     100.97 ms /     4 tokens (   25.24 ms per token,    39.62 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     104.68 ms /     5 tokens\n"
     ]
    }
   ],
   "source": [
    "embed = model.embed(text_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2894671-e38f-4b84-89fe-0aea23d0a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inspection to see if pipeline is working\n",
    "# print(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e8c0d1a-0367-4259-8a39-c1108a262753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     105.34 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     174.63 ms /     3 tokens (   58.21 ms per token,    17.18 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     176.83 ms /     4 tokens\n",
      "\n",
      "llama_print_timings:        load time =     105.34 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      93.18 ms /     3 tokens (   31.06 ms per token,    32.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      96.44 ms /     4 tokens\n"
     ]
    }
   ],
   "source": [
    "embedding1 = model.embed(\"flower\")\n",
    "embedding2 = model.embed(\"blossom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5b2a4e0-b561-4a40-996e-0340c2ee4fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# get_vector function to modularize pipeline\n",
    "#############################################\n",
    "def get_vector(text_to_vectorize):\n",
    "    return model.embed(text_to_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba7382b1-3c53-411a-a348-fe4e60a34e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: embed the search-phrase\n",
    "# step 2: embed each text\n",
    "# step 3: get scores\n",
    "# step 4: evaluates if score is succss or fail\n",
    "# step 5: if success: do stuff with text, else: move on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35e4a063-a36c-4944-997d-f21e2ed165e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cosine_similarity_distance(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Cosine Similarity: This is a common method for measuring the similarity\n",
    "    between two vectors. It measures the cosine of the angle between\n",
    "    two vectors and the result is a value between -1 and 1.\n",
    "    A value of 1 means the vectors are identical,\n",
    "    0 means they are orthogonal (or completely dissimilar),\n",
    "    and -1 means they are diametrically opposed.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "\n",
    "    similarity = similarity[0][0]\n",
    "    \n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "def euclidean_distance(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Euclidean Distance: This is another common method for measuring\n",
    "     the similarity between two vectors.\n",
    "     It calculates the straight-line distance between two points in a space.\n",
    "     The smaller the distance, the more similar the vectors.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = 1 / (1 + euclidean(embedding1, embedding2))\n",
    "    \n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dot_product(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Dot Product: This is a simple method that calculates\n",
    "    the sum of the products of the corresponding entries of the\n",
    "    two sequences of numbers. If the vectors are normalized,\n",
    "    the dot product is equal to the cosine similarity.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    dot_product = np.dot(embedding1, embedding2)\n",
    "    normalized_dot_product = dot_product / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "    similarity = normalized_dot_product\n",
    "    \n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "def manhattan_distance(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Manhattan Distance: This is a measure of the distance between\n",
    "    two vectors in a grid-based system.\n",
    "    It calculates the sum of the absolute differences of their coordinates.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = 1 / (1 + cityblock(embedding1, embedding2))\n",
    "\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def pearson_correlation(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Pearson Correlation: This is a measure of the linear correlation\n",
    "    between two vectors. It ranges from -1 (perfectly negatively correlated)\n",
    "     to 1 (perfectly positively correlated).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity, _ = pearsonr(embedding1, embedding2)\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def spearmans_rank_correlation(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Spearman's Rank Correlation: This is a non-parametric\n",
    "     measure of the monotonicity of the relationship between\n",
    "     two datasets. Unlike the Pearson correlation, the Spearman\n",
    "      correlation does not assume that the relationship between\n",
    "       the two variables is linear.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity, _ = spearmanr(embedding1, embedding2)\n",
    "\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "def kendalls_rank_correlation(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Kendall's Rank Correlation: This is another non-parametric\n",
    "    measure of the ordinal association between two variables.\n",
    "    It is a measure of the correspondence between two rankings.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity, _ = kendalltau(embedding1, embedding2)\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import minkowski\n",
    "\n",
    "\n",
    "def minkowski_distance(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Minkowski Distance: This is a generalization of\n",
    "    both the Euclidean distance and the Manhattan distance.\n",
    "    It is defined as the p-th root of the sum of the p-th powers\n",
    "    of the differences of the coordinates.\n",
    "    When p=1, this is the Manhattan distance,\n",
    "    and when p=2, this is the Euclidean distance.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = 1 / (1 + minkowski(embedding1, embedding2, p=2))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import chebyshev\n",
    "def chebyshev_distance(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Chebyshev Distance: This is a measure of the distance between\n",
    "    two vectors in a vector space.\n",
    "    It is the maximum of the absolute differences of their coordinates.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = 1 / (1 + chebyshev(embedding1, embedding2))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def mahalanobis_distance(embedding1, embedding2, boolean=False, threshold=0.415):\n",
    "    \"\"\"Mahalanobis Distance: This is a measure of the distance between \n",
    "    a point P and a distribution D, introduced by P. C. Mahalanobis in 1936.\n",
    "    It is a multivariate generalization of the Euclidean distance.\n",
    "    It is based on correlations between dimensions of the data, \n",
    "    and thus takes into account the structure of the data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Assuming embedding1 and embedding2 are your vectors\n",
    "    data = np.array([embedding1, embedding2])\n",
    "\n",
    "    # Calculate the covariance matrix with a small regularization term\n",
    "    cov = np.cov(data, rowvar=False) + np.eye(data.shape[1])# * 1e-6\n",
    "\n",
    "    # Calculate the Mahalanobis distance\n",
    "    distance = mahalanobis(embedding1, embedding2, inv(cov))\n",
    "\n",
    "    # Calculate the similarity score\n",
    "    similarity = 1 / (1 + distance)\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import braycurtis\n",
    "def bray_curtis_distance_dissimilarity(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"Bray-Curtis Distance: This is a measure of dissimilarity\n",
    "    between two vectors. It is used in ecology to compare species\n",
    "    composition in different samples. It is defined as the sum of\n",
    "    the absolute differences between the vectors, divided by the sum of their sums.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = 1 / (1 + braycurtis(embedding1, embedding2))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import canberra\n",
    "def canberra_distance(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    dissimilarity\n",
    "    Canberra Distance: This is a measure of the dissimilarity\n",
    "    between two vectors. It is defined as the sum of the absolute\n",
    "    differences between the vectors, divided by the sum of their absolute values.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = 1 / (1 + canberra(embedding1, embedding2))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "def correlation_distance_dissimilarity_measure(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    dissimilarity\n",
    "    Correlation Distance: This is a measure of the dissimilarity\n",
    "    between two vectors. It is defined as 1 - the absolute value of\n",
    "    the Pearson correlation coefficient between the vectors.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    correlation, _ = pearsonr(embedding1, embedding2)\n",
    "    similarity = 1 / (1 + (1 - abs(correlation)))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import sqeuclidean\n",
    "def squared_euclidean_distance_dissimilarity_measure(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    dissimilarity\n",
    "    Squared Euclidean Distance: This is a measure of the dissimilarity\n",
    "    between two vectors. It is defined as the sum of the squared differences\n",
    "    between the vectors. It is similar to the Euclidean distance,\n",
    "    but it does not take the square root, which can make it faster to compute.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your embeddings\n",
    "    similarity = 1 / (1 + sqeuclidean(embedding1, embedding2))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import hamming\n",
    "def hamming_distance_dissimilarity_measure(embedding1, embedding2):\n",
    "    \"\"\"\n",
    "    Hamming Distance: This is a measure of the minimum number\n",
    "    of substitutions required to change one vector into the other.\n",
    "    It is used in information theory to measure the difference between\n",
    "    two binary vectors.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your binary vectors\n",
    "    similarity = 1 / (1 + hamming(embedding1, embedding2))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# not for vectors\n",
    "# dissimilarity\n",
    "# Jensen-Shannon Distance: This is a measure of the dissimilarity\n",
    "# between two probability distributions. It is defined as the square root\n",
    "# of the Jensen-Shannon divergence. It is a symmetric and smooth measure\n",
    "# of dissimilarity that is always greater than or equal to 0.\n",
    "# \"\"\"\n",
    "# from scipy.spatial.distance import jensenshannon\n",
    "# import numpy as np\n",
    "# def jensen_shannon_distance_dissimilarity_measure(embedding1, embedding2):\n",
    "#     # Assuming embedding1 and embedding2 are your probability distributions\n",
    "#     # Compute the average of the two distributions\n",
    "#     average = 0.5 * np.add(embedding1, embedding2)\n",
    "#     # Compute the Jensen-Shannon divergence\n",
    "#     jsd = 0.5 * jensenshannon(embedding1, average) + 0.5 * jensenshannon(embedding2, average)\n",
    "#     # Compute the Jensen-Shannon distance\n",
    "#     dissimilarity = np.sqrt(jsd)\n",
    "#     return dissimilarity\n",
    "\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# not for vectors\n",
    "# dissimilarity\n",
    "# Kullback-Leibler Divergence:\n",
    "# This is a measure of the dissimilarity between two probability distributions.\n",
    "#  It is not symmetric, meaning that the divergence from\n",
    "#  distribution P to distribution Q is not\n",
    "#  necessarily the same as the divergence from Q to P.\n",
    "# \"\"\"\n",
    "# from scipy.special import kl_div\n",
    "# def kullback_leibler_distance_dissimilarity_measure(embedding1, embedding2):\n",
    "#     # Assuming embedding1 and embedding2 are your probability distributions\n",
    "#     # Compute the Kullback-Leibler divergence\n",
    "#     kld = np.sum(kl_div(embedding1, embedding2))\n",
    "#     return kld\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import wasserstein_distance\n",
    "def total_variation_distance_dissimilarity_measure(embedding1, embedding2, boolean=False, threshold=0.5):\n",
    "    \"\"\"\n",
    "    dissimilarity\n",
    "    Total Variation Distance: This is a measure of the dissimilarity\n",
    "    between two probability distributions.\n",
    "    It is defined as half the sum of the absolute differences\n",
    "    between the corresponding probabilities in the two distributions.\n",
    "    \"\"\"\n",
    "    # Assuming embedding1 and embedding2 are your probability distributions\n",
    "    similarity = 1 / (1 + wasserstein_distance(embedding1, embedding2))\n",
    "\n",
    "    if not boolean:\n",
    "        # print(similarity)\n",
    "        return similarity\n",
    "\n",
    "    else:\n",
    "        if simialirity < threshold:\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4476bfc1-410d-41a6-92db-eb628f466399",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # step 1: embed the search-phrase\n",
    "# # step 2: embed each text\n",
    "# # step 3: get scores\n",
    "# # step 4: evaluates if score is succss or fail\n",
    "# # step 5: if success: do stuff with text, else: move on\n",
    "\n",
    "# # arxiv inspector\n",
    "\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import json\n",
    "\n",
    "# ##########################################\n",
    "# # Make comparison phrase and vectorize it\n",
    "# ##########################################\n",
    "# comparison_phrase = \"Neural Networks\"\n",
    "\n",
    "# embedding1 = get_vector(comparison_phrase)\n",
    "\n",
    "\n",
    "# # Make a request to the website\n",
    "# r = requests.get('https://arxiv.org/list/cs/new')\n",
    "# soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "# # Find all the articles\n",
    "# articles = soup.find_all('div', class_='meta')\n",
    "\n",
    "# # List to hold all article data\n",
    "# article_data = []\n",
    "\n",
    "# for article in articles:\n",
    "#     # Extract the title\n",
    "#     title = article.find('div', class_='list-title mathjax').text.strip().replace('Title:', '')\n",
    "\n",
    "#     # Extract the abstract\n",
    "#     abstract = article.find('p', class_='mathjax').text.strip()\n",
    "\n",
    "#     # Extract the link\n",
    "#     link_element = article.find('a', title='Abstract')\n",
    "#     if link_element:\n",
    "#         link = 'https://arxiv.org' + link_element['href']\n",
    "#     else:\n",
    "#         link = ''\n",
    "\n",
    "\n",
    "#     extracted_article_string = title + \" \" + abstract\n",
    "\n",
    "\n",
    "#     ##################################\n",
    "#     # Do embedding search here:\n",
    "#     ##################################\n",
    "\n",
    "#     embedding2 = get_vector(extracted_article_string)\n",
    "\n",
    "\n",
    "#     ##################################\n",
    "#     # Do basic embedding search here:\n",
    "#     ##################################\n",
    "\n",
    "#     # List of functions\n",
    "#     list_of_comparison_functions = [\n",
    "#         cosine_similarity_distance,\n",
    "#         euclidean_distance,\n",
    "#         dot_product,\n",
    "#         manhattan_distance,\n",
    "#         pearson_correlation,\n",
    "#         spearmans_rank_correlation,\n",
    "#         kendalls_rank_correlation,\n",
    "#         minkowski_distance,\n",
    "#         chebyshev_distance,\n",
    "#         mahalanobis_distance,\n",
    "#         bray_curtis_distance_dissimilarity,\n",
    "#         canberra_distance,\n",
    "#         correlation_distance_dissimilarity_measure,\n",
    "#         squared_euclidean_distance_dissimilarity_measure,\n",
    "#         hamming_distance_dissimilarity_measure,\n",
    "#         jensen_shannon_distance_dissimilarity_measure,\n",
    "#         kullback_leibler_distance_dissimilarity_measure,\n",
    "#         total_variation_distance_dissimilarity_measure,\n",
    "#         ]\n",
    "\n",
    "#     # Arguments to pass to the functions\n",
    "#     arguments = (embedding1, embedding2)\n",
    "\n",
    "#     print(f\"\\n\\n For {comparison_phrase} vs. {extracted_article_string[:200]}\")\n",
    "    \n",
    "#     # Iterate through the functions and call each one with the arguments\n",
    "#     for this_function in list_of_comparison_functions:\n",
    "#         raw_score = this_function(*arguments)\n",
    "\n",
    "#         print(raw_score)\n",
    "\n",
    "#     input(\"PointBreak\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3af479c-405a-43ec-9851-0cacef9114a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     105.34 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      59.90 ms /    10 tokens (    5.99 ms per token,   166.96 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      61.12 ms /    11 tokens\n",
      "\n",
      "llama_print_timings:        load time =     105.34 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =      93.54 ms /    10 tokens (    9.35 ms per token,   106.91 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =      95.70 ms /    11 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For I will not be coming to dinner. vs. I will not be coming to dinner.\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normalized_dot_product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Iterate through the functions and call each one with the arguments\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m this_function \u001b[38;5;129;01min\u001b[39;00m list_of_comparison_functions:\n\u001b[0;32m--> 102\u001b[0m     raw_score \u001b[38;5;241m=\u001b[39m \u001b[43mthis_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(raw_score)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# input(\"PointBreak\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m, in \u001b[0;36meuclidean_distance\u001b[0;34m(embedding1, embedding2, boolean, threshold)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Assuming embedding1 and embedding2 are your embeddings\u001b[39;00m\n\u001b[1;32m     39\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m euclidean(embedding1, embedding2))\n\u001b[0;32m---> 42\u001b[0m similarity \u001b[38;5;241m=\u001b[39m \u001b[43mnormalized_dot_product\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m boolean:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# print(similarity)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m similarity\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalized_dot_product' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# step 1: embed the search-phrase\n",
    "# step 2: embed each text\n",
    "# step 3: get scores\n",
    "# step 4: evaluates if score is succss or fail\n",
    "# step 5: if success: do stuff with text, else: move on\n",
    "\n",
    "# arxiv inspector\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "##########################################\n",
    "# Make comparison phrase and vectorize it\n",
    "##########################################\n",
    "comparison_phrase = \"I will not be coming to dinner.\"\n",
    "\n",
    "embedding1 = get_vector(comparison_phrase)\n",
    "\n",
    "\n",
    "\n",
    "articles = [\n",
    "    \"I will not be coming to dinner.\",\n",
    "    \"I should not attend supper\", \n",
    "    \"When you dine tonight I will not be there\",\n",
    "\n",
    "    \"For lunch people go to eat food.\",\n",
    "    \"I will be eating eggs for breakfast\", \n",
    "    \n",
    "    \"dogs dance\",\n",
    "    \"rocks\",\n",
    "    \"velocity\",\n",
    "]\n",
    "\n",
    "# List to hold all article data\n",
    "article_data = []\n",
    "\n",
    "for article in articles:\n",
    "\n",
    "    extracted_article_string = article\n",
    "\n",
    "\n",
    "    ##################################\n",
    "    # Do embedding search here:\n",
    "    ##################################\n",
    "\n",
    "    embedding2 = get_vector(extracted_article_string)\n",
    "\n",
    "\n",
    "    ##################################\n",
    "    # Do basic embedding search here:\n",
    "    ##################################\n",
    "\n",
    "    # List of functions\n",
    "    list_of_comparison_functions = [\n",
    "        mahalanobis_distance,\n",
    "        canberra_distance,\n",
    "        euclidean_distance,\n",
    "        \n",
    "        manhattan_distance,\n",
    "        minkowski_distance,\n",
    "\n",
    "        \n",
    "        squared_euclidean_distance_dissimilarity_measure,\n",
    "\n",
    "\n",
    "        chebyshev_distance,\n",
    "        kendalls_rank_correlation,\n",
    "        bray_curtis_distance_dissimilarity,\n",
    "        cosine_similarity_distance,\n",
    "        dot_product,\n",
    "\n",
    "\n",
    "\n",
    "        spearmans_rank_correlation,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        pearson_correlation,\n",
    "\n",
    "        # hamming_distance_dissimilarity_measure,\n",
    "\n",
    "        correlation_distance_dissimilarity_measure,\n",
    "\n",
    "        total_variation_distance_dissimilarity_measure,\n",
    "        # jensen_shannon_distance_dissimilarity_measure,\n",
    "        # kullback_leibler_distance_dissimilarity_measure,\n",
    "        ]\n",
    "\n",
    "    # Arguments to pass to the functions\n",
    "    arguments = (embedding1, embedding2)\n",
    "\n",
    "    print(f\"For {comparison_phrase} vs. {extracted_article_string[:200]}\")\n",
    "    \n",
    "    # Iterate through the functions and call each one with the arguments\n",
    "    for this_function in list_of_comparison_functions:\n",
    "        raw_score = this_function(*arguments)\n",
    "\n",
    "        print(raw_score)\n",
    "\n",
    "    # input(\"PointBreak\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26e922-f4d6-44df-9a9e-2779d633fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 1: embed the search-phrase\n",
    "# step 2: embed each text\n",
    "# step 3: get scores\n",
    "# step 4: evaluates if score is succss or fail\n",
    "# step 5: if success: do stuff with text, else: move on\n",
    "\n",
    "# arxiv inspector\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "##########################################\n",
    "# Make comparison phrase and vectorize it\n",
    "##########################################\n",
    "comparison_phrase = \"computer vision\"\n",
    "\n",
    "embedding1 = get_vector(comparison_phrase)\n",
    "\n",
    "\n",
    "\n",
    "articles = [\n",
    "    \"\"\"\n",
    "    Principles of Designing Robust Remote Face Anti-Spoofing Systems Xiang Xu, Tianchen Zhao, Zheng Zhang, Zhihua Li, Jon Wu, Alessandro Achille, Mani Srivastava Comments: Under review Subjects: Computer Vision and Pattern Recognition (cs.CV); Cryptography and Security (cs.CR)\n",
    "Protecting digital identities of human face from various attack vectors is paramount, and face anti-spoofing plays a crucial role in this endeavor. Current approaches primarily focus on detecting spoofing attempts within individual frames to detect presentation attacks. However, the emergence of hyper-realistic generative models capable of real-time operation has heightened the risk of digitally generated attacks. In light of these evolving threats, this paper aims to address two key aspects. First, it sheds light on the vulnerabilities of state-of-the-art face anti-spoofing methods against digital attacks. Second, it presents a comprehensive taxonomy of common threats encountered in face anti-spoofing systems. Through a series of experiments, we demonstrate the limitations of current face anti-spoofing detection techniques and their failure to generalize to novel digital attack scenarios. Notably, the existing models struggle with digital injection attacks including adversarial noise, realistic deepfake attacks, and digital replay attacks. To aid in the design and implementation of robust face anti-spoofing systems resilient to these emerging vulnerabilities, the paper proposes key design principles from model accuracy and robustness to pipeline robustness and even platform robustness. Especially, we suggest to implement the proactive face anti-spoofing system using active sensors to significant reduce the risks for unseen attack vectors and improve the user experience.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Principles of Designing Robust Remote Face Anti-Spoofing Systems Xiang Xu, Tianchen Zhao, Zheng Zhang, Zhihua Li, Jon Wu, Alessandro Achille, Mani Srivastava Comments: Under review Subjects: Computer Vision and Pattern Recognition (cs.CV); Cryptography and Security (cs.CR)\n",
    "\n",
    "Protecting digital identities of human face from various attack vectors is paramount, and face anti-spoofing plays a crucial role in this endeavor. Current approaches primarily focus on detecting spoofing attempts within individual frames to detect presentation attacks. However, the emergence of hyper-realistic generative models capable of real-time operation has heightened the risk of digitally generated attacks. In light of these evolving threats, this paper aims to address two key aspects. First, it sheds light on the vulnerabilities of state-of-the-art face anti-spoofing methods against digital attacks. Second, it presents a comprehensive taxonomy of common threats encountered in face anti-spoofing systems. Through a series of experiments, we demonstrate the limitations of current face anti-spoofing detection techniques and their failure to generalize to novel digital attack scenarios. Notably, the existing models struggle with digital injection attacks including adversarial noise, realistic deepfake attacks, and digital replay attacks. To aid in the design and implementation of robust face anti-spoofing systems resilient to these emerging vulnerabilities, the paper proposes key design principles from model accuracy and robustness to pipeline robustness and even platform robustness. Especially, we suggest to implement the proactive face anti-spoofing system using active sensors to significant reduce the risks for unseen attack vectors and improve the user experience.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    A Geometric View of Data Complexity: Efficient Local Intrinsic Dimension Estimation with Diffusion Models\n",
    "Hamidreza Kamkari, Brendan Leigh Ross, Rasa Hosseinzadeh, Jesse C. Cresswell, Gabriel Loaiza-Ganem\n",
    "Comments: 10 pages\n",
    "Subjects: Machine Learning (cs.LG); Artificial Intelligence (cs.AI); Machine Learning (stat.ML)\n",
    "\n",
    "High-dimensional data commonly lies on low-dimensional submanifolds, and estimating the local intrinsic dimension (LID) of a datum -- i.e. the dimension of the submanifold it belongs to -- is a longstanding problem. LID can be understood as the number of local factors of variation: the more factors of variation a datum has, the more complex it tends to be. Estimating this quantity has proven useful in contexts ranging from generalization in neural networks to detection of out-of-distribution data, adversarial examples, and AI-generated text. The recent successes of deep generative models present an opportunity to leverage them for LID estimation, but current methods based on generative models produce inaccurate estimates, require more than a single pre-trained model, are computationally intensive, or do not exploit the best available deep generative models, i.e. diffusion models (DMs). In this work, we show that the Fokker-Planck equation associated with a DM can provide a LID estimator which addresses all the aforementioned deficiencies. Our estimator, called FLIPD, is compatible with all popular DMs, and outperforms existing baselines on LID estimation benchmarks. We also apply FLIPD on natural images where the true LID is unknown. Compared to competing estimators, FLIPD exhibits a higher correlation with non-LID measures of complexity, better matches a qualitative assessment of complexity, and is the only estimator to remain tractable with high-resolution images at the scale of Stable Diffusion.\n",
    "    \"\"\",\n",
    "\n",
    "    \"For lunch people go to eat food.\",\n",
    "    \"I will be eating eggs for breakfast\", \n",
    "    \n",
    "    \"dogs dance\",\n",
    "    \"rocks\",\n",
    "    \"velocity\",\n",
    "]\n",
    "\n",
    "# List to hold all article data\n",
    "article_data = []\n",
    "\n",
    "for article in articles:\n",
    "\n",
    "    extracted_article_string = article\n",
    "\n",
    "\n",
    "    ##################################\n",
    "    # Do embedding search here:\n",
    "    ##################################\n",
    "\n",
    "    embedding2 = get_vector(extracted_article_string)\n",
    "\n",
    "\n",
    "    ##################################\n",
    "    # Do basic embedding search here:\n",
    "    ##################################\n",
    "\n",
    "    # List of functions\n",
    "    list_of_comparison_functions = [\n",
    "\n",
    "        mahalanobis_distance,\n",
    "        \n",
    "        canberra_distance,\n",
    "        \n",
    "        euclidean_distance,\n",
    "        \n",
    "        manhattan_distance,\n",
    "        \n",
    "        minkowski_distance,\n",
    "\n",
    "    \n",
    "        squared_euclidean_distance_dissimilarity_measure,\n",
    "\n",
    "\n",
    "        chebyshev_distance,\n",
    "        \n",
    "        kendalls_rank_correlation,\n",
    "        \n",
    "        bray_curtis_distance_dissimilarity,\n",
    "        \n",
    "        cosine_similarity_distance,\n",
    "        \n",
    "        dot_product,\n",
    "\n",
    "        spearmans_rank_correlation,\n",
    "\n",
    "        pearson_correlation,\n",
    "\n",
    "        correlation_distance_dissimilarity_measure,\n",
    "\n",
    "        total_variation_distance_dissimilarity_measure,\n",
    "\n",
    "        ]\n",
    "\n",
    "    # Arguments to pass to the functions\n",
    "    arguments = (embedding1, embedding2)\n",
    "\n",
    "    print(f\"For {comparison_phrase} vs. {extracted_article_string[:100]}\")\n",
    "    \n",
    "    # Iterate through the functions and call each one with the arguments\n",
    "    for this_function in list_of_comparison_functions:\n",
    "        raw_score = this_function(*arguments)\n",
    "\n",
    "        print(raw_score)\n",
    "\n",
    "    # input(\"PointBreak\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eeb571-509d-4bc2-b8d7-43ba7d7fb1f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
