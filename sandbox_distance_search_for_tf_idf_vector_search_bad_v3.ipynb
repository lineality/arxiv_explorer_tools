{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1be4a439-8fc9-44ea-aa2a-b2b06980cc7f",
        "SOMfhOwlr-zu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba77c8fe-bdc2-4e48-91f2-a942055118eb"
      },
      "source": [
        "# Arxiv Explorer Tools - minimal TF-IDF Vector Search\n",
        "- extract articles on topics of interest from the too-many-to-look-through loads of articles that come out each day.\n",
        "- adjust how strict of loose your filters are\n",
        "- saves results to json and html\n",
        "- minimal TF-IDF is vanilla python (no additional packages or libraries)\n",
        "- arxiv reading uses 'beautiful soup'\n",
        "- various classic distance metrics use:\n",
        "    - scikit-learn\n",
        "    - scipy\n",
        "    - numpy\n",
        "\n",
        "### Setup & Install:\n",
        "- have python installed and use an python env\n",
        "- use a jupyter notebook or script, etc.\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11e7a29-5a13-4c90-b3a3-f4409a9013b2"
      },
      "source": [
        "\n",
        "- https://pypi.org/project/beautifulsoup4/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdea8fa-7a5d-4d32-a88b-1b1f8619e1b3"
      },
      "source": [
        "requirements.txt ->\n",
        "```\n",
        "scikit-learn\n",
        "scipy\n",
        "numpy\n",
        "beautifulsoup4\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4c5c9be-949c-4c72-b2cf-b26df5316aa2",
        "outputId": "096a287d-9833-4d5e-9be5-708aa16ff420",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run -> 0_min__0.0_sec\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "start_time_whole_single_task = datetime.now()\n",
        "end_time_whole_single_task = datetime.now()\n",
        "\n",
        "\n",
        "def duration_min_sec(start_time, end_time):\n",
        "\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    duration_seconds = duration.total_seconds()\n",
        "\n",
        "    minutes = int(duration_seconds // 60)\n",
        "    seconds = duration_seconds % 60\n",
        "    time_message = f\"{minutes}_min__{seconds:.1f}_sec\"\n",
        "\n",
        "    return time_message\n",
        "\n",
        "duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "print(f\"Duration to run -> {duration_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba7382b1-3c53-411a-a348-fe4e60a34e6f"
      },
      "outputs": [],
      "source": [
        "# step 1: make corpus vector-matrix\n",
        "# step 2: get vector of the search-phrase\n",
        "# step 3: get vector of  each text\n",
        "# step 4: get scores\n",
        "# step 5: evaluates if score is succss or fail\n",
        "# step 6: if success: do stuff with text, else: move on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1be4a439-8fc9-44ea-aa2a-b2b06980cc7f"
      },
      "source": [
        "# Distance Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "997a75ee-ef11-47c5-bf36-f292d3043efd"
      },
      "source": [
        "## score report functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d41f4ee-1a88-4da4-aef6-2cfb6b71e820"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "# Functions\n",
        "############\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cosine_similarity_distance(embedding1, embedding2, boolean=False, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Cosine Similarity: This is a common method for measuring the similarity\n",
        "    between two vectors. It measures the cosine of the angle between\n",
        "    two vectors and the result is a value between -1 and 1.\n",
        "    A value of 1 means the vectors are identical,\n",
        "    0 means they are orthogonal (or completely dissimilar),\n",
        "    and -1 means they are diametrically opposed.\n",
        "\n",
        "    if not surprisingly, this looks solid: gold standard?\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = cosine_similarity([embedding1], [embedding2])\n",
        "\n",
        "    similarity = similarity[0][0]\n",
        "\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "from scipy.spatial.distance import euclidean\n",
        "\n",
        "def euclidean_distance(embedding1, embedding2, boolean=False, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Euclidean Distance: This is another common method for measuring\n",
        "     the similarity between two vectors.\n",
        "     It calculates the straight-line distance between two points in a space.\n",
        "     The smaller the distance, the more similar the vectors.\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = 1 / (1 + euclidean(embedding1, embedding2))\n",
        "\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def normalized_dot_product(embedding1, embedding2, boolean=False, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Dot Product: This is a simple method that calculates\n",
        "    the sum of the products of the corresponding entries of the\n",
        "    two sequences of numbers. If the vectors are normalized,\n",
        "    the dot product is equal to the cosine similarity.\n",
        "\n",
        "    0.5 ok? seems good\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    dot_product = np.dot(embedding1, embedding2)\n",
        "    normalized_dot_product = dot_product / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
        "\n",
        "    similarity = normalized_dot_product\n",
        "\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "from scipy.spatial.distance import cityblock\n",
        "\n",
        "def manhattan_distance(embedding1, embedding2, boolean=False, threshold=0.0024):\n",
        "    \"\"\"\n",
        "    Manhattan Distance: This is a measure of the distance between\n",
        "    two vectors in a grid-based system.\n",
        "    It calculates the sum of the absolute differences of their coordinates.\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = 1 / (1 + cityblock(embedding1, embedding2))\n",
        "\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def pearson_correlation(embedding1, embedding2, boolean=False, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Pearson Correlation: This is a measure of the linear correlation\n",
        "    between two vectors. It ranges from -1 (perfectly negatively correlated)\n",
        "     to 1 (perfectly positively correlated).\n",
        "\n",
        "    maybe decent around 0.6?\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity, _ = pearsonr(embedding1, embedding2)\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "def spearmans_rank_correlation(embedding1, embedding2, boolean=False, threshold=0.6):\n",
        "    \"\"\"\n",
        "    Spearman's Rank Correlation: This is a non-parametric\n",
        "     measure of the monotonicity of the relationship between\n",
        "     two datasets. Unlike the Pearson correlation, the Spearman\n",
        "      correlation does not assume that the relationship between\n",
        "       the two variables is linear.\n",
        "\n",
        "    more strict measure?\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity, _ = spearmanr(embedding1, embedding2)\n",
        "\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "from scipy.stats import kendalltau\n",
        "def kendalls_rank_correlation(embedding1, embedding2, boolean=False, threshold=0.7):\n",
        "\n",
        "    \"\"\"\n",
        "    Kendall's Rank Correlation: This is another non-parametric\n",
        "    measure of the ordinal association between two variables.\n",
        "    It is a measure of the correspondence between two rankings.\n",
        "\n",
        "    0.3 may match the subject generally\n",
        "    0.5 may most closely match meaning\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity, _ = kendalltau(embedding1, embedding2)\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "from scipy.spatial.distance import minkowski\n",
        "\n",
        "\n",
        "def minkowski_distance(embedding1, embedding2, boolean=False, threshold=0.055):\n",
        "    \"\"\"\n",
        "    Minkowski Distance: This is a generalization of\n",
        "    both the Euclidean distance and the Manhattan distance.\n",
        "    It is defined as the p-th root of the sum of the p-th powers\n",
        "    of the differences of the coordinates.\n",
        "    When p=1, this is the Manhattan distance,\n",
        "    and when p=2, this is the Euclidean distance.\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = 1 / (1 + minkowski(embedding1, embedding2, p=2))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "from scipy.spatial.distance import chebyshev\n",
        "def chebyshev_distance(embedding1, embedding2, boolean=False, threshold=0.4):\n",
        "    \"\"\"\n",
        "    Chebyshev Distance: This is a measure of the distance between\n",
        "    two vectors in a vector space.\n",
        "    It is the maximum of the absolute differences of their coordinates.\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = 1 / (1 + chebyshev(embedding1, embedding2))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from numpy.linalg import inv\n",
        "\n",
        "def mahalanobis_distance(embedding1, embedding2, boolean=False, threshold=0.415):\n",
        "    \"\"\"Mahalanobis Distance: This is a measure of the distance between\n",
        "    a point P and a distribution D, introduced by P. C. Mahalanobis in 1936.\n",
        "    It is a multivariate generalization of the Euclidean distance.\n",
        "    It is based on correlations between dimensions of the data,\n",
        "    and thus takes into account the structure of the data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming embedding1 and embedding2 are your vectors\n",
        "    data = np.array([embedding1, embedding2])\n",
        "\n",
        "    # Calculate the covariance matrix with a small regularization term\n",
        "    cov = np.cov(data, rowvar=False) + np.eye(data.shape[1])# * 1e-6\n",
        "\n",
        "    # Calculate the Mahalanobis distance\n",
        "    distance = mahalanobis(embedding1, embedding2, inv(cov))\n",
        "\n",
        "    # Calculate the similarity score\n",
        "    similarity = 1 / (1 + distance)\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "\n",
        "from scipy.spatial.distance import braycurtis\n",
        "def bray_curtis_distance_dissimilarity(embedding1, embedding2, boolean=False, threshold=0.75):\n",
        "    \"\"\"Bray-Curtis Distance: This is a measure of dissimilarity\n",
        "    between two vectors. It is used in ecology to compare species\n",
        "    composition in different samples. It is defined as the sum of\n",
        "    the absolute differences between the vectors, divided by the sum of their sums.\n",
        "\n",
        "    0.75 is maybe a stricker-yes\n",
        "\n",
        "    but total no is still .6+\n",
        "    \"\"\"\n",
        "\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = 1 / (1 + braycurtis(embedding1, embedding2))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "from scipy.spatial.distance import canberra\n",
        "def canberra_distance(embedding1, embedding2, boolean=False, threshold=0.002):\n",
        "    \"\"\"\n",
        "    dissimilarity\n",
        "    Canberra Distance: This is a measure of the dissimilarity\n",
        "    between two vectors. It is defined as the sum of the absolute\n",
        "    differences between the vectors, divided by the sum of their absolute values.\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = 1 / (1 + canberra(embedding1, embedding2))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "def correlation_distance_dissimilarity_measure(embedding1, embedding2, boolean=False, threshold=0.7):\n",
        "    \"\"\"\n",
        "    dissimilarity\n",
        "    Correlation Distance: This is a measure of the dissimilarity\n",
        "    between two vectors. It is defined as 1 - the absolute value of\n",
        "    the Pearson correlation coefficient between the vectors.\n",
        "\n",
        "    even no is hight... maybe .7 ok?\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    correlation, _ = pearsonr(embedding1, embedding2)\n",
        "    similarity = 1 / (1 + (1 - abs(correlation)))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "\n",
        "from scipy.spatial.distance import sqeuclidean\n",
        "def squared_euclidean_distance_dissimilarity_measure(embedding1, embedding2, boolean=False, threshold=0.005):\n",
        "    \"\"\"\n",
        "    dissimilarity\n",
        "    Squared Euclidean Distance: This is a measure of the dissimilarity\n",
        "    between two vectors. It is defined as the sum of the squared differences\n",
        "    between the vectors. It is similar to the Euclidean distance,\n",
        "    but it does not take the square root, which can make it faster to compute.\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your embeddings\n",
        "    similarity = 1 / (1 + sqeuclidean(embedding1, embedding2))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "from scipy.spatial.distance import hamming\n",
        "def hamming_distance_dissimilarity_measure(embedding1, embedding2):\n",
        "    \"\"\"\n",
        "    Hamming Distance: This is a measure of the minimum number\n",
        "    of substitutions required to change one vector into the other.\n",
        "    It is used in information theory to measure the difference between\n",
        "    two binary vectors.\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your binary vectors\n",
        "    similarity = 1 / (1 + hamming(embedding1, embedding2))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# not for vectors\n",
        "# dissimilarity\n",
        "# Jensen-Shannon Distance: This is a measure of the dissimilarity\n",
        "# between two probability distributions. It is defined as the square root\n",
        "# of the Jensen-Shannon divergence. It is a symmetric and smooth measure\n",
        "# of dissimilarity that is always greater than or equal to 0.\n",
        "# \"\"\"\n",
        "# from scipy.spatial.distance import jensenshannon\n",
        "# import numpy as np\n",
        "# def jensen_shannon_distance_dissimilarity_measure(embedding1, embedding2):\n",
        "#     # Assuming embedding1 and embedding2 are your probability distributions\n",
        "#     # Compute the average of the two distributions\n",
        "#     average = 0.5 * np.add(embedding1, embedding2)\n",
        "#     # Compute the Jensen-Shannon divergence\n",
        "#     jsd = 0.5 * jensenshannon(embedding1, average) + 0.5 * jensenshannon(embedding2, average)\n",
        "#     # Compute the Jensen-Shannon distance\n",
        "#     dissimilarity = np.sqrt(jsd)\n",
        "#     return dissimilarity\n",
        "\n",
        "\n",
        "\n",
        "# \"\"\"\n",
        "# not for vectors\n",
        "# dissimilarity\n",
        "# Kullback-Leibler Divergence:\n",
        "# This is a measure of the dissimilarity between two probability distributions.\n",
        "#  It is not symmetric, meaning that the divergence from\n",
        "#  distribution P to distribution Q is not\n",
        "#  necessarily the same as the divergence from Q to P.\n",
        "# \"\"\"\n",
        "# from scipy.special import kl_div\n",
        "# def kullback_leibler_distance_dissimilarity_measure(embedding1, embedding2):\n",
        "#     # Assuming embedding1 and embedding2 are your probability distributions\n",
        "#     # Compute the Kullback-Leibler divergence\n",
        "#     kld = np.sum(kl_div(embedding1, embedding2))\n",
        "#     return kld\n",
        "\n",
        "\n",
        "\n",
        "from scipy.stats import wasserstein_distance\n",
        "def total_variation_distance_dissimilarity_measure(embedding1, embedding2, boolean=False, threshold=0.97):\n",
        "    \"\"\"\n",
        "    dissimilarity\n",
        "    Total Variation Distance: This is a measure of the dissimilarity\n",
        "    between two probability distributions.\n",
        "    It is defined as half the sum of the absolute differences\n",
        "    between the corresponding probabilities in the two distributions.\n",
        "\n",
        "    all scores high, maybe .97 is strict enough?\n",
        "    \"\"\"\n",
        "    # Assuming embedding1 and embedding2 are your probability distributions\n",
        "    similarity = 1 / (1 + wasserstein_distance(embedding1, embedding2))\n",
        "\n",
        "    threshold_difference = similarity - threshold\n",
        "\n",
        "    boolean_result = None\n",
        "\n",
        "    if similarity < threshold:\n",
        "        boolean_result = False\n",
        "\n",
        "    else:\n",
        "        boolean_result = True\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    return profile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minimal weighted matching"
      ],
      "metadata": {
        "id": "SOMfhOwlr-zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # import math\n",
        "# # from collections import Counter\n",
        "\n",
        "\n",
        "# # And an even more simplistic basic key word search (with optional weights)\n",
        "\n",
        "# import re\n",
        "\n",
        "# def rank_documents_on_weighted_matches(documents, keyword_weights):\n",
        "#     \"\"\"\n",
        "#     Ranks documents based on the presence of weighted keywords-phrases.\n",
        "#     comparison looks at text without:\n",
        "#     - captialization\n",
        "#     - spaces\n",
        "#     - newlines\n",
        "#     - special symbols\n",
        "\n",
        "#     Parameters:\n",
        "#     documents (list of str): The list of documents to be ranked.\n",
        "#     keyword_weights (list of tuple): A list of tuples, where the first element is the keyword and the\n",
        "#     second element is the corresponding weight.\n",
        "\n",
        "#     Returns:\n",
        "#     list of (str, float): A list of tuples, where the first element is the document and the\n",
        "#     second element is the ranking score.\n",
        "#     \"\"\"\n",
        "\n",
        "#     ranked_documents = []\n",
        "\n",
        "#     for document in documents:\n",
        "#         score = 0\n",
        "#         # Make the document lowercase and strip all symbols, spaces, and newline characters\n",
        "#         match_document = re.sub(r'[^\\w\\s]', '', document.lower()).replace('\\n', '').replace(' ','')\n",
        "#         # print(match_document)\n",
        "#         for keyword, weight in keyword_weights:\n",
        "\n",
        "#             # Make the keyword lowercase and strip all symbols, spaces, and newline characters\n",
        "#             match_keyword = re.sub(r'[^\\w\\s]', '', keyword.lower()).replace('\\n', '').replace(' ','')\n",
        "#             # print(match_keyword)\n",
        "#             # Check if the keyword-phrase is in the document\n",
        "#             if match_keyword in match_document:\n",
        "#                 # If the keyword-phrase is in the document, add its weight to the score\n",
        "#                 score += weight\n",
        "\n",
        "#         ranked_documents.append((document, score))\n",
        "\n",
        "#     # Sort the documents by their ranking scores in descending order\n",
        "#     ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "#     return ranked_documents\n",
        "\n",
        "\n",
        "# ################\n",
        "# # Example usage\n",
        "# ################\n",
        "# documents = [\n",
        "#     \"This is the first document about machine learning.\",\n",
        "#     \"The second document discusses data analysis and visualization.\",\n",
        "#     \"The third document focuses on natural language processing.\",\n",
        "#     \"The fourth document talks about deep learning and neural networks.\",\n",
        "#     \"\"\"to test line breaks\n",
        "#     Emotion mining\n",
        "#      data\n",
        "#     analysis\n",
        "#     Keywords: emotion mining, sentiment analysis, natural disasters, psychology, technological disasters\"\"\",\n",
        "# ]\n",
        "\n",
        "# keyword_weights = [(\"machine learning\", 3), (\"data analysis\", 2), (\"natural language processing\", 4), (\"deep learning\", 5), {\"neural networks\", 6}]\n",
        "\n",
        "# ranked_documents = rank_documents_on_weighted_matches(documents, keyword_weights)\n",
        "\n",
        "# for document, score in ranked_documents:\n",
        "#     print(f\"Document: {document}\\nScore: {score}\\n\")\n"
      ],
      "metadata": {
        "id": "bqy_ZPvpr-6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv Explorerer\n"
      ],
      "metadata": {
        "id": "YepU-A4Fr_J3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19bd0781-5480-4ec0-9709-07330763fd06"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# Arxiv Explorerer\n",
        "###################\n",
        "\n",
        "# step 1: embed the search-phrase\n",
        "# step 2: embed each text\n",
        "# step 3: get scores\n",
        "# step 4: evaluates if score is succss or fail\n",
        "# step 5: if success: do stuff with text\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "start_time_whole_single_task = datetime.now()\n",
        "\n",
        "\n",
        "# ##########################################\n",
        "# # Make comparison phrase and vectorize it\n",
        "# ##########################################\n",
        "# comparison_phrase = \"computer vision resolution enhancement\"\n",
        "# # comparison_phrase = \"cyber security\"\n",
        "# # comparison_phrase = \"natural language processing\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hght1gb699Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Article Corpus"
      ],
      "metadata": {
        "id": "ItIQ_onG-IXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_segment_time = datetime.now()\n",
        "\n",
        "#####################\n",
        "# Get Article Corpus\n",
        "#####################\n",
        "\n",
        "# List to hold all article data\n",
        "article_data = []\n",
        "\n",
        "# # Make a request to the website\n",
        "r = requests.get('https://arxiv.org/list/cs/new')\n",
        "\n",
        "url = \"https://arxiv.org/list/cs/new\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# # Find all the articles\n",
        "articles = soup.find_all('dt')\n",
        "\n",
        "# # Find all the titles\n",
        "articles_title = soup.find_all('div', {'class': 'list-title mathjax'})\n",
        "\n",
        "# Find all the subject on the page\n",
        "articles_subject = soup.find_all('dd')\n",
        "\n",
        "\n",
        "###############\n",
        "# make corpus\n",
        "###############\n",
        "\n",
        "corpus = []\n",
        "report_list = []\n",
        "\n",
        "for this_index, article in enumerate(articles):\n",
        "\n",
        "    ################################################\n",
        "    # Extract each field of data about each article\n",
        "    ################################################\n",
        "\n",
        "    # Extract the title\n",
        "    title = articles_title[this_index].text.split('Title:')[1].strip()\n",
        "\n",
        "    # Extract the subjects\n",
        "    subjects = articles_subject[this_index].find('span', {'class': 'primary-subject'}).text\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "\n",
        "    abstract_p = article.find_next_sibling('dd').find('p', {'class': 'mathjax'})\n",
        "\n",
        "    # Extract the abstract\n",
        "    if abstract_p:\n",
        "        abstract = abstract_p.text.strip()\n",
        "    else:\n",
        "        abstract = \"\"\n",
        "\n",
        "    pdf_link_segment = article.find('a', {'title': 'Download PDF'})['href']\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "    pdf_link = f\"https://arxiv.org{pdf_link_segment}\"\n",
        "    paper_link = f\"https://arxiv.org/abs/{arxiv_id[6:]}\"\n",
        "\n",
        "    extracted_article_string = title + \" \" + abstract + \" \" + str(subjects)\n",
        "\n",
        "    # assemble corpus\n",
        "    article_characters = \"\"\n",
        "\n",
        "    article_characters += f\"'arxiv_id': {arxiv_id}, \"\n",
        "    article_characters += f\"'paper_link': {paper_link}, \"\n",
        "    article_characters += f\"'pdf_link': {pdf_link}, \"\n",
        "\n",
        "    article_characters += title + \" \"\n",
        "    article_characters += subjects + \" \"\n",
        "    article_characters += abstract + \" \"\n",
        "\n",
        "    # add to corpus: just the meaningful text\n",
        "    corpus.append(extracted_article_string)\n",
        "\n",
        "    # add to simple report_list: includes link and article ID info\n",
        "    report_list.append(article_characters)\n",
        "\n",
        "\n",
        "# # Segment Timer\n",
        "# start_segment_time = datetime.now()\n",
        "end_segment_time = datetime.now()\n",
        "duration_time = duration_min_sec(start_segment_time, end_segment_time)\n",
        "print(f\"Duration to run segment -> {duration_time}\")"
      ],
      "metadata": {
        "id": "e8FPqO0u-IXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d0bc42-3e3c-41c4-871d-53dc6dd941aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run segment -> 0_min__1.4_sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (size of corpus)\n",
        "len(corpus)"
      ],
      "metadata": {
        "id": "bve1wNfDBC-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35a4357a-eb9b-4351-8506-e54c20fd2cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "611"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (size of report_list)\n",
        "len(report_list)"
      ],
      "metadata": {
        "id": "5mwhGxD3ESCv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4f71ed-be0c-4151-9bc4-ac7a8958af65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "611"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (sample of corpus)\n",
        "corpus[0]"
      ],
      "metadata": {
        "id": "Wt_6aZx8EVCH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "6b984efd-8fa2-4108-efa3-c1f00dfe66e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Real-Time Automated donning and doffing detection of PPE based on Yolov4-tiny Maintaining patient safety and the safety of healthcare workers (HCWs) in hospitals and clinics highly depends on following the proper protocol for donning and taking off personal protective equipment (PPE). HCWs can benefit from a feedback system during the putting on and removal process because the process is cognitively demanding and errors are common. Centers for Disease Control and Prevention (CDC) provided guidelines for correct PPE use which should be followed. A real time object detection along with a unique sequencing algorithms are used to identify and determine the donning and doffing process in real time. The purpose of this technical research is two-fold: The user gets real time alert to the step they missed in the sequence if they don't follow the proper procedure during donning or doffing. Secondly, the use of tiny machine learning (yolov4-tiny) in embedded system architecture makes it feasible and cost-effective to deploy in different healthcare settings. Computer Vision and Pattern Recognition (cs.CV)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (sample of report_list)\n",
        "report_list[0]"
      ],
      "metadata": {
        "id": "qt7IREx_C9g6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "cc49f920-d340-42b8-c2e4-371c20edc1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'arxiv_id': arXiv:2407.17471, 'paper_link': https://arxiv.org/abs/2407.17471, 'pdf_link': https://arxiv.org/pdf/2407.17471, Real-Time Automated donning and doffing detection of PPE based on Yolov4-tiny Computer Vision and Pattern Recognition (cs.CV) Maintaining patient safety and the safety of healthcare workers (HCWs) in hospitals and clinics highly depends on following the proper protocol for donning and taking off personal protective equipment (PPE). HCWs can benefit from a feedback system during the putting on and removal process because the process is cognitively demanding and errors are common. Centers for Disease Control and Prevention (CDC) provided guidelines for correct PPE use which should be followed. A real time object detection along with a unique sequencing algorithms are used to identify and determine the donning and doffing process in real time. The purpose of this technical research is two-fold: The user gets real time alert to the step they missed in the sequence if they don't follow the proper procedure during donning or doffing. Secondly, the use of tiny machine learning (yolov4-tiny) in embedded system architecture makes it feasible and cost-effective to deploy in different healthcare settings. \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Model: TF-IDF\n",
        "- olde schoole"
      ],
      "metadata": {
        "id": "kUW5FmNv5qXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"computer vision\""
      ],
      "metadata": {
        "id": "vOyEN_TVE8-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "454642d6-8675-4b5f-bb4b-78d53c2ecd01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcf9f1bb-faf7-4b15-a881-90839badf269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run segment -> 2_min__56.4_sec\n"
          ]
        }
      ],
      "source": [
        "start_segment_time = datetime.now()\n",
        "\"\"\"\n",
        "vanilla_TF-IDF_v8.ipynb\n",
        "\n",
        "This notebook is based on:\n",
        "- https://medium.com/@coldstart_coder/understanding-and-implementing-tf-idf-in-python-a325d1301484\n",
        "- https://www.kaggle.com/code/tylerpoff/understanding-and-implementing-tf-idf-in-python/notebook\n",
        "\n",
        "\n",
        "instructions:\n",
        "1. set query string\n",
        "2. set corpus list of strings\n",
        "3. create TF-IDF vector-matrix (pick an inverse_document_frequency variant)\n",
        "4. search/sort for top-N results: tfidf_vector_search_top_n()\n",
        "5. print results etc.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "\n",
        "\"\"\"# query\"\"\"\n",
        "query = query\n",
        "\n",
        "corpus_unsplit = corpus\n",
        "\n",
        "\n",
        "\n",
        "def term_frequency(word, document):\n",
        "    return document.count(word) / len(document)\n",
        "tf = term_frequency\n",
        "\n",
        "# # non-plus-1 variant (\"unsafe\" variant)\n",
        "# def inverse_document_frequency_unsafe(word, corpus):\n",
        "#     count_of_documents = len(corpus)\n",
        "#     count_of_documents_with_word = sum([1 for doc in corpus if word in doc])\n",
        "#     idf = math.log10(count_of_documents/count_of_documents_with_word)\n",
        "#     return idf\n",
        "\n",
        "# sklearn variant\n",
        "def inverse_document_frequency(word, corpus):\n",
        "    count_of_documents = len(corpus) + 1\n",
        "    # count_of_documents_with_word = sum([1 for doc in corpus if word in doc]) + 1\n",
        "    count_of_documents_with_word = 0\n",
        "\n",
        "    for doc in corpus:\n",
        "        count_of_documents_with_word\n",
        "        # print(f\"doc -> {doc}\")\n",
        "        # print(f\"word -> {word}\")\n",
        "\n",
        "        if word in doc:\n",
        "            # print(f\"count_of_documents_with_word -> {count_of_documents_with_word}\")\n",
        "            count_of_documents_with_word += 1\n",
        "\n",
        "    idf = math.log10(count_of_documents/count_of_documents_with_word) + 1\n",
        "    return idf\n",
        "\n",
        "idf = inverse_document_frequency\n",
        "\n",
        "def TF_IDF(word, document, corpus):\n",
        "    return tf(word, document) * idf(word, corpus)\n",
        "\n",
        "\n",
        "\"\"\"# corpus of documents\"\"\"\n",
        "split_corpus = [c.split() for c in corpus_unsplit]\n",
        "num_documents = len(split_corpus)\n",
        "\n",
        "\n",
        "\"\"\"### Optional Sample Target Word Analysis\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "optional\n",
        "\"\"\"\n",
        "# target_word = \"Expert\"\n",
        "\n",
        "# print(\"searching for the word '%s'\"%target_word)\n",
        "# for i, document in enumerate(split_corpus):\n",
        "#     tf_score = tf(target_word, document)\n",
        "#     idf_score = idf(target_word, split_corpus)\n",
        "#     tf_idf_score = TF_IDF(target_word, document, split_corpus)\n",
        "\n",
        "#     print(\"document %s: '%s'\\n    tf score: %s\\n    idf score: %s\\n    tf_idf score:%s\"%(i, document, tf_score, idf_score, tf_idf_score))\n",
        "#     print(\"-\"*30)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"## word to vector mappings\"\"\"\n",
        "\n",
        "\"\"\"\n",
        " create the word to vector mappings,\n",
        " we want each word to map to a unique point in our word vectors.\n",
        "combine the complete corpus into a single list of words; remove duplicates.\n",
        "use position in this list as the index for a word vector\n",
        "\"\"\"\n",
        "word_set = list(set(sum(split_corpus, [])))\n",
        "# create a lookup for each word to it's index,\n",
        "word_to_index = {word:i for i, word in enumerate(word_set)}\n",
        "\n",
        "num_words = len(word_set)\n",
        "\n",
        "\n",
        "import math\n",
        "\n",
        "def get_tfidf_vector(query, word_set, split_corpus, word_to_index):\n",
        "\n",
        "    # Create an empty list to store the word vectors\n",
        "    word_vectors = []\n",
        "\n",
        "    # Calculate the TF-IDF score for each word in the query\n",
        "    query_keywords = query.split()\n",
        "    query_vector = [0 for _ in range(len(word_set))]\n",
        "    for word in query_keywords:\n",
        "        if word in word_set:\n",
        "            word_index = word_to_index[word]\n",
        "            query_vector[word_index] = TF_IDF(word, query_keywords, split_corpus)\n",
        "\n",
        "    return query_vector\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"## create the word vectors\"\"\"\n",
        "# create an empty list to store our word vectors\n",
        "word_vectors = []\n",
        "for document in split_corpus:\n",
        "    # for our new document create a new word vector\n",
        "    new_word_vector = [0 for i in range(num_words)]\n",
        "\n",
        "    # now we loop through each word in our document and compute the tf-idf score and populate our vector with it,\n",
        "    # we only care about words in this document because words outside of it will remain zero\n",
        "    for word in document:\n",
        "        # get the score\n",
        "        tf_idf_score = TF_IDF(word, document, split_corpus)\n",
        "        # next get the index for this word in our word vector\n",
        "        word_index = word_to_index[word]\n",
        "        # populate the vector\n",
        "        new_word_vector[word_index] = tf_idf_score\n",
        "\n",
        "    # add new word vector to list of existing word_vectors\n",
        "    word_vectors.append(new_word_vector)\n",
        "\n",
        "\n",
        "\"\"\"## one word vector in comparision to document\"\"\"\n",
        "# # inspection\n",
        "# print(corpus_unsplit[0])\n",
        "# print(word_vectors[0])\n",
        "\n",
        "\n",
        "#############\n",
        "# Exmple Use\n",
        "#############\n",
        "\n",
        "########################################\n",
        "## Searching with TF-IDF Sparse Vectors\n",
        "########################################\n",
        "query_keywords = query.split()\n",
        "\n",
        "# now we loop through each documents word vector, get the tf-idf score for each keyword, sum them up and that is our tf-idf for that document,\n",
        "# we keep track of the best document and return that as our result,\n",
        "tf_idf_scores = []\n",
        "best_document_index = 0\n",
        "best_tf_idf = 0\n",
        "\n",
        "for i, word_vector in enumerate(word_vectors):\n",
        "    document_tf_idf_score_for_query = 0\n",
        "    for word in query_keywords:\n",
        "        # first do a check, does this word appear in our split_corpus of documents?\n",
        "        # if not skip this keyword\n",
        "        if word not in word_set:\n",
        "            continue\n",
        "\n",
        "        # get the index for this keyword and directly pull it from the word vector\n",
        "        word_index = word_to_index[word]\n",
        "        document_tf_idf_score_for_query += word_vector[word_index]\n",
        "    tf_idf_scores.append(document_tf_idf_score_for_query) # keep track of all tf_idf scores, just in case we want to review them,\n",
        "\n",
        "\n",
        "    # optional:\n",
        "    # top N list...TODO\n",
        "\n",
        "    # does this tf_idf score for this document beat our previous best?\n",
        "    if document_tf_idf_score_for_query > best_tf_idf:\n",
        "        best_tf_idf = document_tf_idf_score_for_query\n",
        "        best_document_index = i\n",
        "\n",
        "\n",
        "\n",
        "# Inspection & Study\n",
        "\n",
        "# from pprint import pprint\n",
        "# # then print out our results\n",
        "# # print(\"results of query: \", query)\n",
        "# print(\"best tf_idf score sum for query: \", best_tf_idf)\n",
        "# print(\"best document: \", corpus_unsplit[best_document_index])\n",
        "# print(\"complete list of tf_idf scores: \", tf_idf_scores)\n",
        "# from pprint import pprint\n",
        "# print(\"tf_idf_scores -> \")\n",
        "# pprint(tf_idf_scores)\n",
        "##  pprint(corpus_unsplit)\n",
        "\n",
        "\n",
        "\n",
        "def tfidf_vector_search_top_n(query, corpus, n):\n",
        "    query_keywords = query.split()\n",
        "\n",
        "    tf_idf_scores = []\n",
        "    for i, word_vector in enumerate(word_vectors):\n",
        "        document_tf_idf_score_for_query = 0\n",
        "        for word in query_keywords:\n",
        "            if word not in word_set:\n",
        "                continue\n",
        "\n",
        "            word_index = word_to_index[word]\n",
        "            document_tf_idf_score_for_query += word_vector[word_index]\n",
        "        tf_idf_scores.append((document_tf_idf_score_for_query, i))\n",
        "\n",
        "    # Sort the TF-IDF scores in descending order\n",
        "    tf_idf_scores.sort(reverse=True)\n",
        "\n",
        "    # Extract the document indices from the top-N results\n",
        "    top_n_document_indices = [index for _, index in tf_idf_scores[:n]]\n",
        "\n",
        "    # Return the top-N documents and their TF-IDF scores\n",
        "    top_n_documents = [corpus[index] for index in top_n_document_indices]\n",
        "    top_n_tf_idf_scores = [score for score, _ in tf_idf_scores[:n]]\n",
        "\n",
        "    return top_n_documents, top_n_tf_idf_scores\n",
        "\n",
        "\n",
        "# # Set This\n",
        "# how_many_results = 5\n",
        "\n",
        "# # Search\n",
        "# top_n = how_many_results\n",
        "# start_time = time.monotonic()  # timer\n",
        "# top_n_documents, top_n_tf_idf_scores = tfidf_vector_search_top_n(query, corpus_unsplit, top_n)\n",
        "# end_time = time.monotonic()  # timer\n",
        "# elapsed_time = end_time - start_time  # timer\n",
        "\n",
        "# print(f\"Top-{top_n} results for query: {query}\")\n",
        "# for i, (document, score) in enumerate(zip(top_n_documents, top_n_tf_idf_scores)):\n",
        "#     print(f\"Result {i+1}:\")\n",
        "#     print(f\"TF-IDF score: {score}\")\n",
        "#     print(f\"Document: {document}\\n\")\n",
        "# # timer\n",
        "# print(f\"Elapsed time: {elapsed_time} seconds\")\n",
        "\n",
        "\n",
        "\n",
        "# # Segment Timer\n",
        "# start_segment_time = datetime.now()\n",
        "end_segment_time = datetime.now()\n",
        "duration_time = duration_min_sec(start_segment_time, end_segment_time)\n",
        "print(f\"Duration to run segment -> {duration_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # inspection\n",
        "# print(corpus_unsplit[0])\n",
        "# print(word_vectors[0])"
      ],
      "metadata": {
        "id": "CLFfVc8YN43w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tfidf_vector(query, word_set, split_corpus, word_to_index):\n",
        "\n",
        "    # Create an empty list to store the word vectors\n",
        "    word_vectors = []\n",
        "\n",
        "    # Calculate the TF-IDF score for each word in the query\n",
        "    query_keywords = query.split()\n",
        "    query_vector = [0 for _ in range(len(word_set))]\n",
        "    for word in query_keywords:\n",
        "        if word in word_set:\n",
        "            word_index = word_to_index[word]\n",
        "            query_vector[word_index] = TF_IDF(word, query_keywords, split_corpus)\n",
        "\n",
        "    return query_vector\n"
      ],
      "metadata": {
        "id": "W4LZ5C5BRFLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_tfidf_vector(query, corpus)"
      ],
      "metadata": {
        "id": "ou0bjnwuRIoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1ywewAwMsZg",
        "outputId": "e6fbd2c3-a28d-4d0f-bb49-5f65b98a5859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "611"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word_vectors[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq5RqPunMxwg",
        "outputId": "aa49d936-6a2f-4d40-8f3d-99b99e66afa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18422"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1kELE5XjM6aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Top-N TF-IDF"
      ],
      "metadata": {
        "id": "QXYDuqnGbmf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set This\n",
        "how_many_results = 10\n",
        "\n",
        "# Search\n",
        "top_n = how_many_results\n",
        "start_tfidf_time = time.monotonic()  # timer\n",
        "top_n_documents, top_n_tf_idf_scores = tfidf_vector_search_top_n(query, corpus_unsplit, top_n)\n",
        "end_tfidf_time = time.monotonic()  # timer\n",
        "elapsed_time = end_tfidf_time - start_tfidf_time  # timer\n",
        "\n",
        "print(f\"Top-{top_n} results for query: {query}\")\n",
        "for i, (document, score) in enumerate(zip(top_n_documents, top_n_tf_idf_scores)):\n",
        "    print(f\"Result {i+1}:\")\n",
        "    print(f\"TF-IDF score: {score}\")\n",
        "    print(f\"Document: {document}\\n\")\n",
        "# timer\n",
        "print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "oWftLrBNFDjB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af444678-7aae-42c9-d989-63c79ff8792f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-10 results for query: computer vision\n",
            "Result 1:\n",
            "TF-IDF score: 0.0857940024139748\n",
            "Document: Mpox Detection Advanced: Rapid Epidemic Response Through Synthetic Data Rapid development of disease detection models using computer vision is crucial in responding to medical emergencies, such as epidemics or bioterrorism events. Traditional data collection methods are often too slow in these scenarios, requiring innovative approaches for quick, reliable model generation from minimal data. Our study introduces a novel approach by constructing a comprehensive computer vision model to detect Mpox lesions using only synthetic data. Initially, these models generated a diverse set of synthetic images representing Mpox lesions on various body parts (face, back, chest, leg, neck, arm) across different skin tones as defined by the Fitzpatrick scale (fair, brown, dark skin). Subsequently, we trained and tested a vision model with this synthetic dataset to evaluate the diffusion models' efficacy in producing high-quality training data and its impact on the vision model's medical image recognition performance. The results were promising; the vision model achieved a 97% accuracy rate, with 96% precision and recall for Mpox cases, and similarly high metrics for normal and other skin disorder cases, demonstrating its ability to correctly identify true positives and minimize false positives. The model achieved an F1-Score of 96% for Mpox cases and 98% for normal and other skin disorders, reflecting a balanced precision-recall relationship, thus ensuring reliability and robustness in its predictions. Our proposed SynthVision methodology indicates the potential to develop accurate computer vision models with minimal data input for future medical emergencies. Computer Vision and Pattern Recognition (cs.CV)\n",
            "\n",
            "Result 2:\n",
            "TF-IDF score: 0.07472414417142281\n",
            "Document: Exploring the Effect of Dataset Diversity in Self-Supervised Learning for Surgical Computer Vision Over the past decade, computer vision applications in minimally invasive surgery have rapidly increased. Despite this growth, the impact of surgical computer vision remains limited compared to other medical fields like pathology and radiology, primarily due to the scarcity of representative annotated data. Whereas transfer learning from large annotated datasets such as ImageNet has been conventionally the norm to achieve high-performing models, recent advancements in self-supervised learning (SSL) have demonstrated superior performance. In medical image analysis, in-domain SSL pretraining has already been shown to outperform ImageNet-based initialization. Although unlabeled data in the field of surgical computer vision is abundant, the diversity within this data is limited. This study investigates the role of dataset diversity in SSL for surgical computer vision, comparing procedure-specific datasets against a more heterogeneous general surgical dataset across three different downstream surgical applications. The obtained results show that using solely procedure-specific data can lead to substantial improvements of 13.8%, 9.5%, and 36.8% compared to ImageNet pretraining. However, extending this data with more heterogeneous surgical data further increases performance by an additional 5.0%, 5.2%, and 2.5%, suggesting that increasing diversity within SSL data is beneficial for model performance. The code and pretrained model weights are made publicly available at this https URL. Computer Vision and Pattern Recognition (cs.CV)\n",
            "\n",
            "Result 3:\n",
            "TF-IDF score: 0.057513617442902475\n",
            "Document: Tool-Assisted Learning of Computational Reductions Computational reductions are an important and powerful concept in computer science. However, they are difficult for many students to grasp. In this paper, we outline a concept for how the learning of reductions can be supported by educational support systems. We present an implementation of the concept within such a system, concrete web-based and interactive learning material for reductions, and report on our experiences using the material in a large introductory course on theoretical computer science. Computers and Society (cs.CY)\n",
            "\n",
            "Result 4:\n",
            "TF-IDF score: 0.04568838768828701\n",
            "Document: Improving engagement, diversity, and retention in computer science with RadGrad: Results of a case study RadGrad is a curriculum initiative implemented via an application that combines features of social networks, degree planners, individual learning plans, and serious games. RadGrad redefines traditional meanings of \"progress\" and \"success\" in the undergraduate computer science degree program in an attempt to improve engagement, retention, and diversity. In this paper, we describe the RadGrad Project and report on an evaluation study designed to assess the impact of RadGrad on student engagement, diversity, and retention. We also present opportunities and challenges that result from the use of the system. Computers and Society (cs.CY)\n",
            "\n",
            "Result 5:\n",
            "TF-IDF score: 0.0398295669717197\n",
            "Document: Exploring Scaling Trends in LLM Robustness Language model capabilities predictably improve from scaling a model's size and training data. Motivated by this, increasingly large language models have been trained, yielding an array of impressive capabilities. Yet these models are vulnerable to adversarial prompts, such as \"jailbreaks\" that hijack models to perform undesired behaviors, posing a significant risk of misuse. Prior work indicates that computer vision models become more robust with model and data scaling, raising the question: does language model robustness also improve with scale? We study this question empirically, finding that larger models respond substantially better to adversarial training, but there is little to no benefit from model scale in the absence of explicit defenses. Machine Learning (cs.LG)\n",
            "\n",
            "Result 6:\n",
            "TF-IDF score: 0.03782316235454803\n",
            "Document: Universal Approximation Theory: The basic theory for deep learning-based computer vision models Computer vision (CV) is one of the most crucial fields in artificial intelligence. In recent years, a variety of deep learning models based on convolutional neural networks (CNNs) and Transformers have been designed to tackle diverse problems in CV. These algorithms have found practical applications in areas such as robotics and facial recognition. Despite the increasing power of current CV models, several fundamental questions remain unresolved: Why do CNNs require deep layers? What ensures the generalization ability of CNNs? Why do residual-based networks outperform fully convolutional networks like VGG? What is the fundamental difference between residual-based CNNs and Transformer-based networks? Why can CNNs utilize LoRA and pruning techniques? The root cause of these questions lies in the lack of a robust theoretical foundation for deep learning models in CV. To address these critical issues and techniques, we employ the Universal Approximation Theorem (UAT) to provide a theoretical basis for convolution- and Transformer-based models in CV. By doing so, we aim to elucidate these questions from a theoretical perspective. Computer Vision and Pattern Recognition (cs.CV)\n",
            "\n",
            "Result 7:\n",
            "TF-IDF score: 0.034850871100254734\n",
            "Document: Geometry Fidelity for Spherical Images Spherical or omni-directional images offer an immersive visual format appealing to a wide range of computer vision applications. However, geometric properties of spherical images pose a major challenge for models and metrics designed for ordinary 2D images. Here, we show that direct application of Fréchet Inception Distance (FID) is insufficient for quantifying geometric fidelity in spherical images. We introduce two quantitative metrics accounting for geometric constraints, namely Omnidirectional FID (OmniFID) and Discontinuity Score (DS). OmniFID is an extension of FID tailored to additionally capture field-of-view requirements of the spherical format by leveraging cubemap projections. DS is a kernel-based seam alignment score of continuity across borders of 2D representations of spherical images. In experiments, OmniFID and DS quantify geometry fidelity issues that are undetected by FID. Computer Vision and Pattern Recognition (cs.CV)\n",
            "\n",
            "Result 8:\n",
            "TF-IDF score: 0.03291471159468502\n",
            "Document: Enhancing Environmental Monitoring through Multispectral Imaging: The WasteMS Dataset for Semantic Segmentation of Lakeside Waste Environmental monitoring of lakeside green areas is crucial for environmental protection. Compared to manual inspections, computer vision technologies offer a more efficient solution when deployed on-site. Multispectral imaging provides diverse information about objects under different spectrums, aiding in the differentiation between waste and lakeside lawn environments. This study introduces WasteMS, the first multispectral dataset established for the semantic segmentation of lakeside waste. WasteMS includes a diverse range of waste types in lawn environments, captured under various lighting conditions. We implemented a rigorous annotation process to label waste in images. Representative semantic segmentation frameworks were used to evaluate segmentation accuracy using WasteMS. Challenges encountered when using WasteMS for segmenting waste on lakeside lawns were discussed. The WasteMS dataset is available at this https URL. Computer Vision and Pattern Recognition (cs.CV)\n",
            "\n",
            "Result 9:\n",
            "TF-IDF score: 0.031833068768986124\n",
            "Document: TiCoSS: Tightening the Coupling between Semantic Segmentation and Stereo Matching within A Joint Learning Framework Semantic segmentation and stereo matching, respectively analogous to the ventral and dorsal streams in our human brain, are two key components of autonomous driving perception systems. Addressing these two tasks with separate networks is no longer the mainstream direction in developing computer vision algorithms, particularly with the recent advances in large vision models and embodied artificial intelligence. The trend is shifting towards combining them within a joint learning framework, especially emphasizing feature sharing between the two tasks. The major contributions of this study lie in comprehensively tightening the coupling between semantic segmentation and stereo matching. Specifically, this study introduces three novelties: (1) a tightly coupled, gated feature fusion strategy, (2) a hierarchical deep supervision strategy, and (3) a coupling tightening loss function. The combined use of these technical contributions results in TiCoSS, a state-of-the-art joint learning framework that simultaneously tackles semantic segmentation and stereo matching. Through extensive experiments on the KITTI and vKITTI2 datasets, along with qualitative and quantitative analyses, we validate the effectiveness of our developed strategies and loss function, and demonstrate its superior performance compared to prior arts, with a notable increase in mIoU by over 9%. Our source code will be publicly available at mias.group/TiCoSS upon publication. Computer Vision and Pattern Recognition (cs.CV)\n",
            "\n",
            "Result 10:\n",
            "TF-IDF score: 0.029257521417497802\n",
            "Document: Wasserstein approximation schemes based on Voronoi partitions We consider structured approximation of measures in Wasserstein space $\\mathrm{W}_p(\\mathbb{R}^d)$ for $p\\in[1,\\infty)$ using general measure approximants compactly supported on Voronoi regions derived from a scaled Voronoi partition of $\\mathbb{R}^d$. We show that if a full rank lattice $\\Lambda$ is scaled by a factor of $h\\in(0,1]$, then approximation of a measure based on the Voronoi partition of $h\\Lambda$ is $O(h)$ regardless of $d$ or $p$. We then use a covering argument to show that $N$-term approximations of compactly supported measures is $O(N^{-\\frac1d})$ which matches known rates for optimal quantizers and empirical measure approximation in most instances. Additionally, we generalize our construction to nonuniform Voronoi partitions, highlighting the flexibility and robustness of our approach for various measure approximation scenarios. Finally, we extend these results to noncompactly supported measures with sufficient decay. Our findings are pertinent to applications in computer vision and machine learning where measures are used to represent structured data such as images. Machine Learning (stat.ML)\n",
            "\n",
            "Elapsed time: 0.32162186299956375 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Distance Checking"
      ],
      "metadata": {
        "id": "jWXPQq7BEmDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "query = \"\"\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "g9TSQrxUb238"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# Embedding 1, is the vector of the query\n",
        "##########################################\n",
        "embedding1 = get_tfidf_vector(query, word_set, split_corpus, word_to_index)\n",
        "\n",
        "\n",
        "for this_index, article in enumerate(articles):\n",
        "\n",
        "    ############################\n",
        "    # Do search here:\n",
        "    ############################\n",
        "    this_article = corpus[this_index]\n",
        "\n",
        "    embedding2 = get_tfidf_vector(this_article, word_set, split_corpus, word_to_index)\n",
        "\n",
        "    ##################################\n",
        "    # Do basic embedding search here:\n",
        "    ##################################\n",
        "\n",
        "    list_of_comparison_function_tuples = [\n",
        "        (cosine_similarity_distance, \"cosine_similarity_distance\"),\n",
        "        (correlation_distance_dissimilarity_measure, \"correlation_distance_dissimilarity_measure\"),\n",
        "        (pearson_correlation, \"pearson_correlation\"),\n",
        "        (canberra_distance, \"canberra_distance\"),\n",
        "        (euclidean_distance, \"euclidean_distance\"),\n",
        "        (manhattan_distance, \"manhattan_distance\"),\n",
        "        (minkowski_distance, \"minkowski_distance\"),\n",
        "        (squared_euclidean_distance_dissimilarity_measure, \"squared_euclidean_distance_dissimilarity_measure\"),\n",
        "        (chebyshev_distance, \"chebyshev_distance\"),\n",
        "        (kendalls_rank_correlation, \"kendalls_rank_correlation\"),\n",
        "        (bray_curtis_distance_dissimilarity, \"bray_curtis_distance_dissimilarity\"),\n",
        "        (normalized_dot_product, \"normalized_dot_product\"),\n",
        "        (spearmans_rank_correlation, \"spearmans_rank_correlation\"),\n",
        "        (total_variation_distance_dissimilarity_measure, \"total_variation_distance_dissimilarity_measure\"),\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Arguments to pass to the functions\n",
        "    arguments = (embedding1, embedding2, True)\n",
        "\n",
        "    # print(f\"For {comparison_phrase} vs. {extracted_article_string}\")\n",
        "\n",
        "    list_of_boolean_scores = []\n",
        "\n",
        "    \"\"\"\n",
        "    compare to results of keyword search\n",
        "\n",
        "    do self-search\n",
        "\n",
        "    do paraphrase search\n",
        "\n",
        "    Score_Profile\n",
        "    1. get a boolean\n",
        "    2. get threshold\n",
        "    3. get distance past threshold\n",
        "    4. get weak, medium, strong distance score\n",
        "\n",
        "\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    \"\"\"\n",
        "    passed_metrics = []\n",
        "    failed_metrics = []\n",
        "    pass_fail_list = []\n",
        "    list_of_profiles = []\n",
        "    counter = 0\n",
        "    article_id_counter = 1\n",
        "\n",
        "    # Iterate through the functions and call each one with the arguments\n",
        "    for this_function_tuple in list_of_comparison_function_tuples:\n",
        "        function_pointer = this_function_tuple[0]\n",
        "\n",
        "        result_profile = function_pointer(*arguments)\n",
        "\n",
        "        print(f\"result_profile {result_profile}\")\n",
        "\n",
        "        boolean_score = result_profile['boolean']\n",
        "\n",
        "        \"\"\"\n",
        "        Look at which scores are pass or fail\n",
        "        \"\"\"\n",
        "        # preset/reset\n",
        "        passed_metrics = []\n",
        "        failed_metrics = []\n",
        "\n",
        "        if boolean_score:\n",
        "            passed_metrics.append(counter)\n",
        "\n",
        "        else:\n",
        "            failed_metrics.append(counter)\n",
        "\n",
        "        # print(raw_score)\n",
        "        list_of_boolean_scores.append(boolean_score)\n",
        "\n",
        "        list_of_profiles.append(result_profile)\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "    pass_fail_list.append( (counter, passed_metrics,failed_metrics)  )\n",
        "\n",
        "    ratio_score = list_of_boolean_scores.count(True)\n",
        "\n",
        "    print(f\"{ratio_score} / {len(list_of_boolean_scores)}\")\n",
        "\n",
        "    # input(\"PointBreak\")\n",
        "\n",
        "    decimal_percent_true = ratio_score / len(list_of_boolean_scores)\n",
        "\n",
        "    # target_score_decimal_percent = 0.5\n",
        "    target_score_decimal_percent = 5 / len(list_of_boolean_scores)\n",
        "\n",
        "    if decimal_percent_true >= target_score_decimal_percent:\n",
        "\n",
        "        # Append the data to the list\n",
        "        article_data.append({\n",
        "            'article_id': article_id_counter,\n",
        "            'scores': f\"{ratio_score} / {len(list_of_boolean_scores)}\",\n",
        "            'pass_fail_list': pass_fail_list,\n",
        "            'list_of_profiles': list_of_profiles,\n",
        "            'title': title,\n",
        "            'abstract': abstract,\n",
        "            'paper_link': paper_link,\n",
        "            'pdf_link': pdf_link,\n",
        "            'subjects': subjects,\n",
        "            'arxiv_id': arxiv_id,\n",
        "\n",
        "\n",
        "        })\n",
        "\n",
        "    article_id_counter += 1\n"
      ],
      "metadata": {
        "id": "ph5cMZREUTbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e-mzQ12eaOoC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "this_article -> Real-Time Automated donning and doffing detection of PPE based on Yolov4-tiny Maintaining patient safety and the safety of healthcare workers (HCWs) in hospitals and clinics highly depends on following the proper protocol for donning and taking off personal protective equipment (PPE). HCWs can benefit from a feedback system during the putting on and removal process because the process is cognitively demanding and errors are common. Centers for Disease Control and Prevention (CDC) provided guidelines for correct PPE use which should be followed. A real time object detection along with a unique sequencing algorithms are used to identify and determine the donning and doffing process in real time. The purpose of this technical research is two-fold: The user gets real time alert to the step they missed in the sequence if they don't follow the proper procedure during donning or doffing. Secondly, the use of tiny machine learning (yolov4-tiny) in embedded system architecture makes it feasible and cost-effective to deploy in different healthcare settings. Computer Vision and Pattern Recognition (cs.CV)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "query = \"\"\"\n",
        "egg\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EluOAW0GcJE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################\n",
        "# Embedding 1, is the vector of the query\n",
        "##########################################\n",
        "embedding1 = get_tfidf_vector(query, word_set, split_corpus, word_to_index)\n",
        "\n",
        "\n",
        "for this_index, article in enumerate(articles):\n",
        "\n",
        "    ############################\n",
        "    # Do search here:\n",
        "    ############################\n",
        "    this_article = corpus[this_index]\n",
        "\n",
        "    embedding2 = get_tfidf_vector(this_article, word_set, split_corpus, word_to_index)\n",
        "\n",
        "    print(f\"this_article -> {this_article}\")\n",
        "\n",
        "    ##################################\n",
        "    # Do basic embedding search here:\n",
        "    ##################################\n",
        "\n",
        "    list_of_comparison_function_tuples = [\n",
        "        (cosine_similarity_distance, \"cosine_similarity_distance\"),\n",
        "        (correlation_distance_dissimilarity_measure, \"correlation_distance_dissimilarity_measure\"),\n",
        "        (pearson_correlation, \"pearson_correlation\"),\n",
        "        (canberra_distance, \"canberra_distance\"),\n",
        "        (euclidean_distance, \"euclidean_distance\"),\n",
        "        (manhattan_distance, \"manhattan_distance\"),\n",
        "        (minkowski_distance, \"minkowski_distance\"),\n",
        "        (squared_euclidean_distance_dissimilarity_measure, \"squared_euclidean_distance_dissimilarity_measure\"),\n",
        "        (chebyshev_distance, \"chebyshev_distance\"),\n",
        "        (kendalls_rank_correlation, \"kendalls_rank_correlation\"),\n",
        "        (bray_curtis_distance_dissimilarity, \"bray_curtis_distance_dissimilarity\"),\n",
        "        (normalized_dot_product, \"normalized_dot_product\"),\n",
        "        (spearmans_rank_correlation, \"spearmans_rank_correlation\"),\n",
        "        (total_variation_distance_dissimilarity_measure, \"total_variation_distance_dissimilarity_measure\"),\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Arguments to pass to the functions\n",
        "    arguments = (embedding1, embedding2, True)\n",
        "\n",
        "    # print(f\"For {comparison_phrase} vs. {extracted_article_string}\")\n",
        "\n",
        "    list_of_boolean_scores = []\n",
        "\n",
        "    \"\"\"\n",
        "    compare to results of keyword search\n",
        "\n",
        "    do self-search\n",
        "\n",
        "    do paraphrase search\n",
        "\n",
        "    Score_Profile\n",
        "    1. get a boolean\n",
        "    2. get threshold\n",
        "    3. get distance past threshold\n",
        "    4. get weak, medium, strong distance score\n",
        "\n",
        "\n",
        "\n",
        "    profile = {\n",
        "        'boolean': boolean_result,\n",
        "        'threshold': threshold,\n",
        "        'threshold_difference': threshold_difference,\n",
        "        'similarity_measure': similarity,\n",
        "    }\n",
        "\n",
        "    \"\"\"\n",
        "    passed_metrics = []\n",
        "    failed_metrics = []\n",
        "    pass_fail_list = []\n",
        "    list_of_profiles = []\n",
        "    counter = 0\n",
        "    article_id_counter = 1\n",
        "\n",
        "    # Iterate through the functions and call each one with the arguments\n",
        "    for this_function_tuple in list_of_comparison_function_tuples:\n",
        "        function_pointer = this_function_tuple[0]\n",
        "\n",
        "        result_profile = function_pointer(*arguments)\n",
        "\n",
        "        print(f\"result_profile {result_profile}\")\n",
        "\n",
        "        boolean_score = result_profile['boolean']\n",
        "\n",
        "        \"\"\"\n",
        "        Look at which scores are pass or fail\n",
        "        \"\"\"\n",
        "        # preset/reset\n",
        "        passed_metrics = []\n",
        "        failed_metrics = []\n",
        "\n",
        "        if boolean_score:\n",
        "            passed_metrics.append(counter)\n",
        "\n",
        "        else:\n",
        "            failed_metrics.append(counter)\n",
        "\n",
        "        # print(raw_score)\n",
        "        list_of_boolean_scores.append(boolean_score)\n",
        "\n",
        "        list_of_profiles.append(result_profile)\n",
        "\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "    pass_fail_list.append( (counter, passed_metrics,failed_metrics)  )\n",
        "\n",
        "    ratio_score = list_of_boolean_scores.count(True)\n",
        "\n",
        "    print(f\"{ratio_score} / {len(list_of_boolean_scores)}\")\n",
        "\n",
        "    input(\"PointBreak\")\n",
        "\n",
        "    decimal_percent_true = ratio_score / len(list_of_boolean_scores)\n",
        "\n",
        "    # target_score_decimal_percent = 0.5\n",
        "    target_score_decimal_percent = 5 / len(list_of_boolean_scores)\n",
        "\n",
        "    if decimal_percent_true >= target_score_decimal_percent:\n",
        "\n",
        "        # Append the data to the list\n",
        "        article_data.append({\n",
        "            'article_id': article_id_counter,\n",
        "            'scores': f\"{ratio_score} / {len(list_of_boolean_scores)}\",\n",
        "            'pass_fail_list': pass_fail_list,\n",
        "            'list_of_profiles': list_of_profiles,\n",
        "            'title': title,\n",
        "            'abstract': abstract,\n",
        "            'paper_link': paper_link,\n",
        "            'pdf_link': pdf_link,\n",
        "            'subjects': subjects,\n",
        "            'arxiv_id': arxiv_id,\n",
        "\n",
        "\n",
        "        })\n",
        "\n",
        "    article_id_counter += 1\n"
      ],
      "metadata": {
        "id": "CrtRgjbDaOvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Article Corpus"
      ],
      "metadata": {
        "id": "P3nVA8L69dKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "v2"
      ],
      "metadata": {
        "id": "BtCwyW7gUwdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#############\n",
        "# Write Data\n",
        "#############\n",
        "\n",
        "# Posix UTC Seconds\n",
        "# make readable time\n",
        "from datetime import datetime, UTC\n",
        "date_time = datetime.now(UTC)\n",
        "clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "\n",
        "# Save the data to a JSON file\n",
        "with open(f'articles_{clean_timestamp}.json', 'w') as f:\n",
        "    json.dump(article_data, f)\n",
        "\n",
        "\n",
        "# Create an HTML file\n",
        "html = '<html><body>'\n",
        "for article in article_data:\n",
        "    html += f'<h2><a href=\"{article[\"paper_link\"]}\">{article[\"title\"]}</a></h2>'\n",
        "    html += f'<p>{article[\"abstract\"]}</p>'\n",
        "    html += f'<p>Subjects: \", {str(article[\"subjects\"])}</p>'\n",
        "\n",
        "    html += f'<a href=\"{article[\"paper_link\"]}\">{article[\"paper_link\"]}</a>'\n",
        "    html += f'<p>paper link: \", {str(article[\"paper_link\"])}</p>'\n",
        "\n",
        "    html += f'<a href=\"{article[\"pdf_link\"]}\">{article[\"pdf_link\"]}</a>'\n",
        "    html += f'<p>pdf link: \", {str(article[\"pdf_link\"])}</p>'\n",
        "\n",
        "    html += f'<p>arxiv id: \", {str(article[\"arxiv_id\"])}</p>'\n",
        "\n",
        "html += '</body></html>'\n",
        "\n",
        "\n",
        "# Save the HTML to a file\n",
        "with open(f'articles{clean_timestamp}.html', 'w') as f:\n",
        "    f.write(html)\n",
        "\n",
        "# Duration time print\n",
        "end_time_whole_single_task = datetime.now()\n",
        "duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "print(f\"Duration to run -> {duration_time}\")\n"
      ],
      "metadata": {
        "id": "jHRKjBJorzVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be60f579-19ab-4ec6-ac1c-61d9e3884f57",
        "outputId": "f79ef0ba-17ff-4f65-afb1-fd7fae52f31b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'7_min__9.8_sec'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "duration_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adbdeb05-8327-46a1-b64b-85973fc763ee"
      },
      "outputs": [],
      "source": [
        "# todo: subjects not found,\n",
        "# todo: html strings are wrong (list strings?)"
      ]
    }
  ]
}