{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5fjaWvPkKM8a",
        "WPnLaV3fpCkv",
        "MaYRyhUgm1ol"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba77c8fe-bdc2-4e48-91f2-a942055118eb"
      },
      "source": [
        "# Arxiv Explorer Tools - minimal weighted match\n",
        "- Fast:\n",
        "    - 1-2 sec to run in a local jupyter notebook\n",
        "    - ~5-10 sec to run in google colab\n",
        "    - vs. 5-10 min for embedding or TFIDF search\n",
        "- extracts articles on topics of interest from the too-many-to-look-through daily pages of articles that come out each day.\n",
        "- minimal weighted match uses a list of phrases and an weight for each\n",
        "- saves search results to json (for automation later) and html (for easy reading and linking)\n",
        "- saves all articles for archiving\n",
        "- multi-topic: use as many pre-set seaches as you want\n",
        "- set score_floor and top_n to filter which results you see\n",
        "- arxiv site reading uses 'beautiful soup'\n",
        "\n",
        "### Setup & Install:\n",
        "- have python installed and use an python env\n",
        "- use a jupyter notebook or script, etc.\n",
        "- for specialty topics you can create extensive weighted search profiles.\n",
        "\n",
        "Note: should be able to run as a script or in a server, but notebooks are useful\n",
        "\n",
        "### For more on Arxiv search tools, See:\n",
        "- https://medium.com/@GeoffreyGordonAshbrook/search-with-non-generative-ai-d0a3cc77164b\n",
        "- https://github.com/lineality/arxiv_explorer_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Instructions"
      ],
      "metadata": {
        "id": "5fjaWvPkKM8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put this notebook (the .ipynb file) or a script (.py file) into your project directory."
      ],
      "metadata": {
        "id": "YveoAwOoK4aM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdea8fa-7a5d-4d32-a88b-1b1f8619e1b3"
      },
      "source": [
        "requirements.txt ->\n",
        "```\n",
        "requests\n",
        "scikit-learn\n",
        "scipy\n",
        "numpy\n",
        "beautifulsoup4\n",
        "jupyter\n",
        "```\n",
        "- https://pypi.org/project/beautifulsoup4/\n",
        "\n",
        "#### make python env\n",
        "```bash\n",
        "python -m venv env; source env/bin/activate\n",
        "python -m pip install --upgrade pip; python -m pip install -r requirements.txt\n",
        "python -m pip install git+https://github.com/psf/black pylint pydocstyle flake8\n",
        "```\n",
        "\n",
        "#### run notebook\n",
        "```bash\n",
        "jupyter notebook\n",
        "```\n",
        "\n",
        "#### use notebook\n",
        "- Select Notebook\n",
        "- Run all Cells\n",
        "- view and use printed and file-saved results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup"
      ],
      "metadata": {
        "id": "XjAYgxPXDnnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # standard library\n",
        "import time  # standard library\n",
        "from datetime import datetime  # standard library\n",
        "from bs4 import BeautifulSoup  # pip install beautifulsoup4\n",
        "import requests  # standard library\n",
        "import json  # standard library"
      ],
      "metadata": {
        "id": "djUT2fDyDLJe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "e4c5c9be-949c-4c72-b2cf-b26df5316aa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "265ad5a9-3071-4285-add7-6370c35f6209"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nTally time at end.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "\"\"\"\n",
        "Code-Bundle For Time:\n",
        " Commented-out code is for use in different places in the code.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def duration_min_sec(start_time, end_time):\n",
        "\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    duration_seconds = duration.total_seconds()\n",
        "\n",
        "    minutes = int(duration_seconds // 60)\n",
        "    seconds = duration_seconds % 60\n",
        "    time_message = f\"{minutes}_min__{seconds:.1f}_sec\"\n",
        "\n",
        "    return time_message\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Start (and Stop) your Time Tracking:\n",
        "\"\"\"\n",
        "start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "\n",
        "\"\"\"\n",
        "Tally time at end.\n",
        "\"\"\"\n",
        "# # start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "# duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "# print(f\"Duration to run -> {duration_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minimal weighted matching code"
      ],
      "metadata": {
        "id": "SOMfhOwlr-zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An simplistic basic key word search (with optional weights)\n",
        "\n",
        "def minimal_wieghted_match_score(one_document, keyword_weights):\n",
        "    \"\"\"\n",
        "    Simple weight score, lowercase.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    score = 0\n",
        "\n",
        "    try:\n",
        "        # Make the document lowercase and strip all symbols, spaces, and newline characters\n",
        "        match_this_cleaned_document = re.sub(r'[^\\w\\s]', '', one_document.lower()).replace('\\n', '').replace(' ','')\n",
        "\n",
        "        # print(match_this_cleaned_document)\n",
        "        for keyword, weight in keyword_weights:\n",
        "\n",
        "            # Make the keyword lowercase and strip all symbols, spaces, and newline characters\n",
        "            match_this_cleaned_keyword = re.sub(r'[^\\w\\s]', '', keyword.lower()).replace('\\n', '').replace(' ','')\n",
        "\n",
        "            # print(match_this_cleaned_keyword)\n",
        "            # Check if the keyword-phrase is in the document\n",
        "            if match_this_cleaned_keyword in match_this_cleaned_document:\n",
        "                # If the keyword-phrase is in the document, add its weight to the score\n",
        "                score += weight\n",
        "\n",
        "        return score\n",
        "    except Exception as e:\n",
        "        print(f\"{str(e)}: {one_document} {type(one_document)}\")\n",
        "\n",
        "\n",
        "def rank_documents_on_weighted_matches(documents, keyword_weights):\n",
        "    \"\"\"\n",
        "    Ranks documents based on the presence of weighted keywords-phrases.\n",
        "    comparison looks at text without:\n",
        "    - captialization\n",
        "    - spaces\n",
        "    - newlines\n",
        "    - special symbols\n",
        "\n",
        "    Parameters:\n",
        "    - documents (list of str): The list of documents to be ranked.\n",
        "    - keyword_weights (list of tuple): A list of tuples,\n",
        "       where the first element is the keyword and the\n",
        "       second element is the corresponding weight.\n",
        "\n",
        "    Returns:\n",
        "    list of (str, float): A list of tuples, where the first element is the document and the\n",
        "    second element is the ranking score.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    string cleaning steps:\n",
        "    - lower\n",
        "    - strip extra spaces\n",
        "    - remove symbols\n",
        "    - remove newlines\n",
        "    \"\"\"\n",
        "\n",
        "    ranked_documents = []\n",
        "\n",
        "    for document in documents:\n",
        "        score = 0\n",
        "\n",
        "        # Make the document lowercase and strip all symbols, spaces, and newline characters\n",
        "        match_this_cleaned_document = re.sub(r'[^\\w\\s]', '', document.lower()).replace('\\n', '').replace(' ','')\n",
        "\n",
        "        # print(match_this_cleaned_document)\n",
        "        for keyword, weight in keyword_weights:\n",
        "\n",
        "            # Make the keyword lowercase and strip all symbols, spaces, and newline characters\n",
        "            match_this_cleaned_keyword = re.sub(r'[^\\w\\s]', '', keyword.lower()).replace('\\n', '').replace(' ','')\n",
        "\n",
        "            # print(match_this_cleaned_keyword)\n",
        "            # Check if the keyword-phrase is in the document\n",
        "            if match_this_cleaned_keyword in match_this_cleaned_document:\n",
        "                # If the keyword-phrase is in the document, add its weight to the score\n",
        "                score += weight\n",
        "\n",
        "        ranked_documents.append((document, score))\n",
        "\n",
        "    # Sort the documents by their ranking scores in descending order\n",
        "    ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_documents\n",
        "\n",
        "\n",
        "# ################\n",
        "# # Example usage\n",
        "# ################\n",
        "# corpus = [\n",
        "#     \"This is the first document about machine learning.\",\n",
        "#     \"The second document discusses data analysis and visualization.\",\n",
        "#     \"The third document focuses on natural language processing.\",\n",
        "#     \"The fourth document talks about deep learning and neural networks.\",\n",
        "#     \"\"\"to test line breaks\n",
        "#     Emotion mining\n",
        "#      data\n",
        "#     analysis\n",
        "#     Keywords: emotion mining, sentiment analysis, natural disasters, psychology, technological disasters\"\"\",\n",
        "# ]\n",
        "\n",
        "# one_doc = \"This is the first document about machine learning.\"\n",
        "\n",
        "# keyword_weights = [(\"machine learning\", 3), (\"data analysis\", 2), (\"natural language processing\", 4), (\"deep learning\", 5), (\"neural networks\", 6)]\n",
        "\n",
        "# ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "# for document, score in ranked_documents:\n",
        "#     print(f\"Document: {document}\\nScore: {score}\\n\")\n",
        "\n",
        "# one_score = minimal_wieghted_match_score(one_doc, keyword_weights)\n",
        "\n",
        "# print(one_score)"
      ],
      "metadata": {
        "id": "bqy_ZPvpr-6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abbed872-2e75-4516-9bfa-b2508e475ba4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document: The fourth document talks about deep learning and neural networks.\n",
            "Score: 11\n",
            "\n",
            "Document: The third document focuses on natural language processing.\n",
            "Score: 4\n",
            "\n",
            "Document: This is the first document about machine learning.\n",
            "Score: 3\n",
            "\n",
            "Document: The second document discusses data analysis and visualization.\n",
            "Score: 2\n",
            "\n",
            "Document: to test line breaks\n",
            "    Emotion mining\n",
            "     data\n",
            "    analysis\n",
            "    Keywords: emotion mining, sentiment analysis, natural disasters, psychology, technological disasters\n",
            "Score: 2\n",
            "\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv Explorerer\n"
      ],
      "metadata": {
        "id": "YepU-A4Fr_J3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "19bd0781-5480-4ec0-9709-07330763fd06"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# Arxiv Explorerer\n",
        "###################\n",
        "# step 1: embed the search-phrase\n",
        "# step 2: embed each text\n",
        "# step 3: get scores\n",
        "# step 4: evaluates if score is succss or fail\n",
        "# step 5: if success: do stuff with text\n",
        "\n",
        "\n",
        "# # Imports\n",
        "# from bs4 import BeautifulSoup  # pip install beautifulsoup4\n",
        "# import requests  # standard library\n",
        "# import json  # standard library\n",
        "# from datetime import datetime  # standard library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Article Corpus"
      ],
      "metadata": {
        "id": "ItIQ_onG-IXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_segment_time = datetime.now()\n",
        "\n",
        "#####################\n",
        "# Get Article Corpus\n",
        "#####################\n",
        "\n",
        "# List to hold all article data\n",
        "article_data = []\n",
        "\n",
        "# # Make a request to the website\n",
        "r = requests.get('https://arxiv.org/list/cs/new')\n",
        "\n",
        "url = \"https://arxiv.org/list/cs/new\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# # Find all the articles\n",
        "articles = soup.find_all('dt')\n",
        "\n",
        "# # Find all the titles\n",
        "articles_title = soup.find_all('div', {'class': 'list-title mathjax'})\n",
        "\n",
        "# Find all the subject on the page\n",
        "articles_subject = soup.find_all('dd')\n",
        "\n",
        "\n",
        "###############\n",
        "# make corpus\n",
        "###############\n",
        "\n",
        "corpus = []\n",
        "report_list = []\n",
        "article_dicts = []\n",
        "\n",
        "for this_index, article in enumerate(articles):\n",
        "\n",
        "    ################################################\n",
        "    # Extract each field of data about each article\n",
        "    ################################################\n",
        "\n",
        "    # Extract the title\n",
        "    title = articles_title[this_index].text.split('Title:')[1].strip()\n",
        "\n",
        "    # Extract the subjects\n",
        "    subjects = articles_subject[this_index].find('span', {'class': 'primary-subject'}).text\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "\n",
        "    abstract_p = article.find_next_sibling('dd').find('p', {'class': 'mathjax'})\n",
        "\n",
        "    # Extract the abstract\n",
        "    if abstract_p:\n",
        "        abstract = abstract_p.text.strip()\n",
        "    else:\n",
        "        abstract = \"\"\n",
        "\n",
        "    pdf_link_segment = article.find('a', {'title': 'Download PDF'})['href']\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "    pdf_link = f\"https://arxiv.org{pdf_link_segment}\"\n",
        "    paper_link = f\"https://arxiv.org/abs/{arxiv_id[6:]}\"\n",
        "\n",
        "    # extracted_article_string = title + \" \" + abstract + \" \" + str(subjects)\n",
        "\n",
        "    # assemble corpus\n",
        "    article_characters = f\"{this_index}|||| \"\n",
        "\n",
        "    article_characters += f\"\\n'arxiv_id': {arxiv_id}, \"\n",
        "    article_characters += f\"\\n'paper_link': {paper_link}, \"\n",
        "    article_characters += f\"\\n'pdf_link': {pdf_link}, \"\n",
        "\n",
        "    article_characters += \"\\nTitle: \" + title + \" \"\n",
        "    article_characters += \"\\nSubjects: \" + subjects + \" \"\n",
        "    article_characters += \"\\nAbstract: \" + abstract\n",
        "\n",
        "    ##################################\n",
        "    # Make Bundles (sharing an index)\n",
        "    ##################################\n",
        "\n",
        "    # # add to corpus: just the meaningful text\n",
        "    # corpus.append(extracted_article_string)\n",
        "\n",
        "    # add to simple report_list: includes link and article ID info\n",
        "    report_list.append(article_characters)\n",
        "\n",
        "    # Append the data to the list\n",
        "    article_dicts.append({\n",
        "        'title': title,\n",
        "        'abstract': abstract,\n",
        "        'paper_link': paper_link,\n",
        "        'pdf_link': pdf_link,\n",
        "        'subjects': subjects,\n",
        "        'arxiv_id': arxiv_id,\n",
        "        'article_sequence_index': this_index,\n",
        "    })\n",
        "\n",
        "    # using this because only basic search works\n",
        "    corpus = report_list\n",
        "\n",
        "\n",
        "# # Segment Timer\n",
        "# start_segment_time = datetime.now()\n",
        "end_segment_time = datetime.now()\n",
        "duration_time = duration_min_sec(start_segment_time, end_segment_time)\n",
        "print(f\"Duration to run segment -> {duration_time}\")\n",
        "\n",
        "# ALL Save the data to a JSON file\n",
        "date_time = datetime.now()\n",
        "all_article_dicts_clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "with open(f'all_arxiv_article_dicts_{all_article_dicts_clean_timestamp}.json', 'a') as f:\n",
        "    json.dump(article_dicts, f)"
      ],
      "metadata": {
        "id": "e8FPqO0u-IXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1097e0-3e98-416d-9ab4-36f9d71f62d8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run segment -> 0_min__2.4_sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (size of corpus)\n",
        "len(corpus)"
      ],
      "metadata": {
        "id": "bve1wNfDBC-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1eae5a-c1d4-461d-f54b-5b0ce209a009"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "779"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "9dYdsj824h3w",
        "outputId": "cd9b20e9-6f81-4bb7-d1c4-563b865876c1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"0|||| \\n'arxiv_id': arXiv:2412.14174, \\n'paper_link': https://arxiv.org/abs/2412.14174, \\n'pdf_link': https://arxiv.org/pdf/2412.14174, \\nTitle: Steering Large Text-to-Image Model for Abstract Art Synthesis: Preference-based Prompt Optimization and Visualization \\nSubjects: Human-Computer Interaction (cs.HC) \\nAbstract: With the advancement of neural generative capabilities, the art community has increasingly embraced GenAI (Generative Artificial Intelligence), particularly large text-to-image models, for producing aesthetically compelling results. However, the process often lacks determinism and requires a tedious trial-and-error process as users often struggle to devise effective prompts to achieve their desired outcomes. This paper introduces a prompting-free generative approach that applies a genetic algorithm and real-time iterative human feedback to optimize prompt generation, enabling the creation of user-preferred abstract art through a customized Artist Model. The proposed two-part approach begins with constructing an Artist Model capable of deterministically generating abstract art in specific styles, e.g., Kandinsky's Bauhaus style. The second phase integrates real-time user feedback to optimize the prompt generation and obtains an Optimized Prompting Model, which adapts to user preferences and generates prompts automatically. When combined with the Artist Model, this approach allows users to create abstract art tailored to their personal preferences and artistic style.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# print and save: code"
      ],
      "metadata": {
        "id": "WPnLaV3fpCkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from datetime import datetime  # standard library\n",
        "\n",
        "########################################\n",
        "# Filter, Save, & Print the Raw Results\n",
        "########################################\n",
        "# ALL Save the data to a JSON file\n",
        "date_time = datetime.now()\n",
        "all_arxiv_results_clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "all_articles_list = []\n",
        "all_results_json_list = []\n",
        "\n",
        "\n",
        "def result_counter(ranked_documents):\n",
        "    \"\"\"\n",
        "    count non-zero scored results\n",
        "    \"\"\"\n",
        "\n",
        "    result_count = 0\n",
        "\n",
        "    for this_doc in ranked_documents:\n",
        "        score = this_doc[1]\n",
        "\n",
        "        if score != 0:\n",
        "            result_count += 1\n",
        "\n",
        "    return result_count\n",
        "\n",
        "\n",
        "def score_filtered_result_counter(ranked_documents, score_floor=0):\n",
        "    \"\"\"\n",
        "    count non-zero scored results that are greater than or equal to score_floor\n",
        "    \"\"\"\n",
        "\n",
        "    result_count = 0\n",
        "\n",
        "    for this_doc in ranked_documents:\n",
        "        score = this_doc[1]\n",
        "\n",
        "        if score != 0 and score >= score_floor:\n",
        "            result_count += 1\n",
        "\n",
        "    return result_count\n",
        "\n",
        "\n",
        "def print_and_save(ranked_documents, top_n, name_of_set, score_floor=5):\n",
        "    # Posix UTC Seconds\n",
        "    # make readable time\n",
        "    # from datetime import datetime\n",
        "    date_time = datetime.now()\n",
        "    clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    results_json_list = []\n",
        "\n",
        "    for document, score in ranked_documents:\n",
        "\n",
        "        if score >= score_floor:\n",
        "\n",
        "            blurb = f\"Document: {document}\\nScore: {score}\\n\"\n",
        "\n",
        "            print(blurb)\n",
        "\n",
        "        this_index = int(document.split('||||')[0])\n",
        "\n",
        "        data_dict = article_dicts[this_index]\n",
        "\n",
        "        results_json_list.append(data_dict)\n",
        "        all_results_json_list.append(data_dict)\n",
        "\n",
        "        counter += 1\n",
        "        if counter >= top_n:\n",
        "            break\n",
        "\n",
        "    #############\n",
        "    # Write Data\n",
        "    #############\n",
        "\n",
        "    # Save the data to a JSON file\n",
        "    with open(f'{name_of_set}_articles_{clean_timestamp}.json', 'w') as f:\n",
        "        json.dump(results_json_list, f)\n",
        "\n",
        "    # Create an HTML file\n",
        "    html = '<html><body>'\n",
        "    for article in results_json_list:\n",
        "        html += f'<h2><a href=\"{article[\"paper_link\"]}\">{article[\"title\"]}</a></h2>'\n",
        "        html += f'<p>{article[\"abstract\"]}</p>'\n",
        "        html += f'<p>Subjects: {str(article[\"subjects\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"paper_link\"]}\">{article[\"paper_link\"]}</a>'\n",
        "        html += f'<p>paper link: {str(article[\"paper_link\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"pdf_link\"]}\">{article[\"pdf_link\"]}</a>'\n",
        "        html += f'<p>pdf link: {str(article[\"pdf_link\"])}</p>'\n",
        "\n",
        "        html += f'<p>arxiv id: {str(article[\"arxiv_id\"])}</p>'\n",
        "        html += f'<p>article_sequence_index id: {str(article[\"article_sequence_index\"])}</p>'\n",
        "\n",
        "    html += '</body></html>'\n",
        "\n",
        "\n",
        "    # Save the HTML to a file\n",
        "    with open(f'{name_of_set}_articles{clean_timestamp}.html', 'w') as f:\n",
        "        f.write(html)\n",
        "\n",
        "\n",
        "def match_print_save(list_of_lists_of_weights, top_n, score_floor):\n",
        "    date_time = datetime.now()\n",
        "    clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "    counter = 0\n",
        "    for keyword_weights in list_of_lists_of_weights:\n",
        "\n",
        "        ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "        # user first list item as name of set\n",
        "        name_of_set = list_of_lists_of_weights[counter][0][0]\n",
        "\n",
        "        result_quantity = result_counter(ranked_documents)\n",
        "\n",
        "        score_floor_filtered_quantity = score_filtered_result_counter(ranked_documents, score_floor)\n",
        "\n",
        "        this_max_number = top_n\n",
        "\n",
        "        if top_n > result_quantity:\n",
        "            this_max_number = result_quantity\n",
        "\n",
        "        print(f\"\\n\\nSet Name: {name_of_set}\")\n",
        "        print(f\"Total Matches in Set: {result_quantity}\")\n",
        "        print(f\"Matches Above Score-Floor in Set: {score_floor_filtered_quantity}\")\n",
        "        print(clean_timestamp)\n",
        "\n",
        "        print(f\"\\nShowing {score_floor_filtered_quantity} in top-{this_max_number} out of {result_quantity} total results.     -> {score_floor_filtered_quantity} of {this_max_number}/{result_quantity}\")\n",
        "        print(f\"(Ceiling set at {top_n} (top_n) filtered results.)    -> {top_n}\")\n",
        "        print(f\"(Minimum-included-score, 'Score-Floor' set at {score_floor}) -> {score_floor}\\n\\n\")\n",
        "\n",
        "        print_and_save(ranked_documents, top_n, name_of_set, score_floor)\n",
        "        counter += 1\n",
        "\n",
        "\n",
        "        # ALL Save the data to a JSON file\n",
        "        with open(f'all_arxiv_results_{all_arxiv_results_clean_timestamp}.json', 'a') as f:\n",
        "            json.dump(all_results_json_list, f)"
      ],
      "metadata": {
        "id": "peVzbe-Di2xH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multi-set search(es)\n",
        "(optional)"
      ],
      "metadata": {
        "id": "MaYRyhUgm1ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ########\n",
        "# # Batch\n",
        "# ########\n",
        "\n",
        "# # example multi-list\n",
        "\n",
        "# list_of_lists_of_weights = [\n",
        "#     # keyword_weights =\n",
        "#     [\n",
        "#         (\"computer vision\", 3),\n",
        "#         (\"resolution\", 2),\n",
        "#         # (\"natural language processing\", 4),\n",
        "#         # (\"deep learning\", 5),\n",
        "#         (\"neural networks\", 6),\n",
        "#     ],\n",
        "\n",
        "\n",
        "#     # keyword_weights =\n",
        "#     [\n",
        "#         (\"distance measure\", 10),\n",
        "#         (\"similarity measure\", 10),\n",
        "#         (\"vector distance\", 10),\n",
        "#         (\"distance metric\", 10),\n",
        "#         (\"similarity metric\", 10),\n",
        "#         (\"dimension reduction\", 10),\n",
        "\n",
        "\n",
        "#         (\"similarity\", 1),\n",
        "#         (\"distance\", 1),\n",
        "#         (\"metric\", 1),\n",
        "\n",
        "#     ],\n",
        "\n",
        "\n",
        "#     # # keyword_weights =\n",
        "#     # (\"cognitive science\", 2),  # much too broad...\n",
        "#     [\n",
        "#         (\"mental health\", 5),\n",
        "#         (\"psychological health\", 5),\n",
        "#         (\"psycholog\", 2),  # stem vs. lemma\n",
        "\n",
        "\n",
        "#         (\"mental health care\", 3),\n",
        "#         (\"neuroscience\", 2),\n",
        "#         (\"psychological assessment\", 2),\n",
        "#         (\"personality assessment\", 2),\n",
        "#         (\"personality inference\", 2),\n",
        "#         (\"personality traits\", 2),\n",
        "#         (\"personality dimensions\", 2),\n",
        "#         (\"emotion\", 15),\n",
        "#         (\"sports psychology\", 15),\n",
        "#         # (\"\", 2),\n",
        "#         # (\"\", 2),\n",
        "\n",
        "\n",
        "\n",
        "#         # disease terms\n",
        "#         (\"depression\", 5),\n",
        "#         (\"anxiety\", 5),\n",
        "#         (\"mental disorders\", 2),\n",
        "#         (\"social anxiety disorder\", 4),\n",
        "#         (\"mental illness\", 2),\n",
        "#         (\"Major Depressive Disorder\", 2),\n",
        "#         (\"MDD\", 2),\n",
        "#         (\"psychological stressors\", 2),\n",
        "#         (\"cognitive impairment\", 2),\n",
        "#         (\"mci\", 2),\n",
        "#         # (\"\", 2),\n",
        "#         # (\"\", 2),\n",
        "#         # (\"\", 2),\n",
        "\n",
        "#         ],\n",
        "\n",
        "\n",
        "#     # # keyword_weights =\n",
        "#     [\n",
        "#         (\"benchmark\", 5),\n",
        "#         (\"model evaluation\", 5),\n",
        "#         (\"test\", 2),\n",
        "#         (\"measure\", 2),\n",
        "#     ],\n",
        "\n",
        "\n",
        "#     # # keyword_weights =\n",
        "#     [\n",
        "#         (\"training set\", 5),\n",
        "#         (\"synthetic\", 2),\n",
        "#         (\"generate\", 2),\n",
        "#         (\"measure\", 2),\n",
        "#     ],\n",
        "\n",
        "#     # keyword_weights =\n",
        "#     [\n",
        "#         (\"graph\", 5),\n",
        "#         (\"graph generation\", 8),\n",
        "#         (\"subgraph\", 2),\n",
        "#         (\"hierarchical graph\", 2),\n",
        "#         (\"embedding\", 2),\n",
        "#         (\"knowledge graph\", 2),\n",
        "\n",
        "#         (\"graph neural networks\", 2),\n",
        "#         (\"graph representation\", 2),\n",
        "#         (\"node\", 2),\n",
        "#          ## collisions: cryptograph, geograph,\n",
        "#     ],\n",
        "\n",
        "# ]\n",
        "\n",
        "# top_n = 45\n",
        "# score_floor = 3\n",
        "# match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "id": "Sn35USTbt3MM"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find articles:\n",
        "use: keywords+weights, top_n, score_floor"
      ],
      "metadata": {
        "id": "bt_SeRE_l345"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Manifold Approximation\", 10),\n",
        "        (\"UMAP\", 10),\n",
        "        (\"Uniform Manifold Approximation and Projection\", 10),\n",
        "        (\"Manifold hypothesis\", 10),\n",
        "        (\"dimensionality reduction\", 10),\n",
        "        (\"dimension reduction\", 10),\n",
        "        (\"dimension reduction technique\", 10),\n",
        "\n",
        "        (\"stress\", 1),\n",
        "        (\"Manifold\", 1),\n",
        "        (\"lower-dimensional\", 1),\n",
        "        (\"visualiz\", 1),\n",
        "        (\"projection\", 1),\n",
        "        (\"project\", 1),\n",
        "        (\"dimensionality\", 1),\n",
        "        (\"reduction\", 1),\n",
        "    ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLOy8bu3elSO",
        "outputId": "0edcc1a9-0e8e-4a5e-b0d3-b8c9d110441c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Manifold Approximation\n",
            "Total Matches in Set: 94\n",
            "Matches Above Score-Floor in Set: 20\n",
            "2024-12-20__041109376782\n",
            "\n",
            "Showing 20 in top-45 out of 94 total results.     -> 20 of 45/94\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 264|||| \n",
            "'arxiv_id': arXiv:2412.14717, \n",
            "'paper_link': https://arxiv.org/abs/2412.14717, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14717, \n",
            "Title: Computing Gram Matrix for SMILES Strings using RDKFingerprint and Sinkhorn-Knopp Algorithm \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: In molecular structure data, SMILES (Simplified Molecular Input Line Entry System) strings are used to analyze molecular structure design. Numerical feature representation of SMILES strings is a challenging task. This work proposes a kernel-based approach for encoding and analyzing molecular structures from SMILES strings. The proposed approach involves computing a kernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal component analysis (PCA) for dimensionality reduction. The resulting low-dimensional embeddings are then used for classification and regression analysis. The kernel matrix is computed by converting the SMILES strings into molecular structures using the Morgan Fingerprint, which computes a fingerprint for each molecule. The distance matrix is computed using the pairwise kernels function. The Sinkhorn-Knopp algorithm is used to compute the final kernel matrix that satisfies the constraints of a probability distribution. This is achieved by iteratively adjusting the kernel matrix until the marginal distributions of the rows and columns match the desired marginal distributions. We provided a comprehensive empirical analysis of the proposed kernel method to evaluate its goodness with greater depth. The suggested method is assessed for drug subcategory prediction (classification task) and solubility AlogPS ``Aqueous solubility and Octanol/Water partition coefficient\" (regression task) using the benchmark SMILES string dataset. The outcomes show the proposed method outperforms several baseline methods in terms of supervised analysis and has potential uses in molecular design and drug discovery. Overall, the suggested method is a promising avenue for kernel methods-based molecular structure analysis and design.\n",
            "Score: 12\n",
            "\n",
            "Document: 456|||| \n",
            "'arxiv_id': arXiv:2412.14723, \n",
            "'paper_link': https://arxiv.org/abs/2412.14723, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14723, \n",
            "Title: Dimension reduction for path signatures \n",
            "Subjects: Probability (math.PR) \n",
            "Abstract: This paper focuses on the mathematical framework for reducing the complexity of models using path signatures. The structure of these signatures, which can be interpreted as collections of iterated integrals along paths, is discussed and their applications in areas such as stochastic differential equations (SDEs) and financial modeling are pointed out. In particular, exploiting the rough paths view, solutions of SDEs continuously depend on the lift of the driver. Such continuous mappings can be approximated using (truncated) signatures, which are solutions of high-dimensional linear systems. In order to lower the complexity of these models, this paper presents methods for reducing the order of high-dimensional truncated signature models while retaining essential characteristics. The derivation of reduced models and the universal approximation property of (truncated) signatures are treated in detail. Numerical examples, including applications to the (rough) Bergomi model in financial markets, illustrate the proposed reduction techniques and highlight their effectiveness.\n",
            "Score: 11\n",
            "\n",
            "Document: 561|||| \n",
            "'arxiv_id': arXiv:2406.14742, \n",
            "'paper_link': https://arxiv.org/abs/2406.14742, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.14742, \n",
            "Title: Latent Variable Sequence Identification for Cognitive Models with Neural Network Estimators \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Extracting time-varying latent variables from computational cognitive models is a key step in model-based neural analysis, which aims to understand the neural correlates of cognitive processes. However, existing methods only allow researchers to infer latent variables that explain subjects' behavior in a relatively small class of cognitive models. For example, a broad class of relevant cognitive models with analytically intractable likelihood is currently out of reach from standard techniques, based on Maximum a Posteriori parameter estimation. Here, we present an approach that extends neural Bayes estimation to learn a direct mapping between experimental data and the targeted latent variable space using recurrent neural networks and simulated datasets. We show that our approach achieves competitive performance in inferring latent variable sequences in both tractable and intractable models. Furthermore, the approach is generalizable across different computational models and is adaptable for both continuous and discrete latent spaces. We then demonstrate its applicability in real world datasets. Our work underscores that combining recurrent neural networks and simulation-based inference to identify latent variable sequences can enable researchers to access a wider class of cognitive models for model-based neural analyses, and thus test a broader set of theories.\n",
            "Score: 10\n",
            "\n",
            "Document: 765|||| \n",
            "'arxiv_id': arXiv:2407.13012, \n",
            "'paper_link': https://arxiv.org/abs/2407.13012, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.13012, \n",
            "Title: CUAOA: A Novel CUDA-Accelerated Simulation Framework for the QAOA \n",
            "Subjects: Quantum Physics (quant-ph) \n",
            "Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a prominent quantum algorithm designed to find approximate solutions to combinatorial optimization problems, which are challenging for classical computers. In the current era, where quantum hardware is constrained by noise and limited qubit availability, simulating the QAOA remains essential for research. However, existing state-of-the-art simulation frameworks suffer from long execution times or lack comprehensive functionality, usability, and versatility, often requiring users to implement essential features themselves. Additionally, these frameworks are primarily restricted to Python, limiting their use in safer and faster languages like Rust, which offer, e.g., advanced parallelization capabilities. In this paper, we develop a GPU accelerated QAOA simulation framework utilizing the NVIDIA CUDA toolkit. This framework offers a complete interface for QAOA simulations, enabling the calculation of (exact) expectation values, direct access to the statevector, fast sampling, and high-performance optimization methods using an advanced state-of-the-art gradient calculation technique. The framework is designed for use in Python and Rust, providing flexibility for integration into a wide range of applications, including those requiring fast algorithm implementations leveraging QAOA at its core. The new framework's performance is rigorously benchmarked on the MaxCut problem and compared against the current state-of-the-art general-purpose quantum circuit simulation frameworks Qiskit and Pennylane as well as the specialized QAOA simulation tool QOKit. Our evaluation shows that our approach outperforms the existing state-of-the-art solutions in terms of runtime up to multiple orders of magnitude. Our implementation is publicly available at this https URL and Zenodo.\n",
            "Score: 10\n",
            "\n",
            "Document: 123|||| \n",
            "'arxiv_id': arXiv:2412.14460, \n",
            "'paper_link': https://arxiv.org/abs/2412.14460, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14460, \n",
            "Title: A tensor-train reduced basis solver for parameterized partial differential equations \n",
            "Subjects: Numerical Analysis (math.NA) \n",
            "Abstract: In this manuscript we present the tensor-train reduced basis method, a novel projection-based reduced-order model for the efficient solution of parameterized partial differential equations. Despite their popularity and considerable computational advantages with respect to their full order counterparts, reduced-order models are typically characterized by a considerable offline computational cost. The proposed approach addresses this issue by efficiently representing high dimensional finite element quantities with the tensor train format. This method entails numerous benefits, namely, the smaller number of operations required to compute the reduced subspaces, the cheaper hyper-reduction strategy employed to reduce the complexity of the PDE residual and Jacobian, and the decreased dimensionality of the projection subspaces for a fixed accuracy. We provide a posteriori estimates that demonstrate the accuracy of the proposed method, we test its computational performance for the heat equation and transient linear elasticity on three-dimensional Cartesian geometries.\n",
            "Score: 4\n",
            "\n",
            "Document: 24|||| \n",
            "'arxiv_id': arXiv:2412.14206, \n",
            "'paper_link': https://arxiv.org/abs/2412.14206, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14206, \n",
            "Title: Design of an AI-Enhanced Digital Stethoscope: Advancing Cardiovascular Diagnostics Through Smart Auscultation \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: In the ever-evolving landscape of medical diagnostics, this study details the systematic design process and concept selection methodology for developing an advanced digital stethoscope, demonstrating the evolution from traditional acoustic models to AI-enhanced digital solutions. The device integrates cutting-edge AI technology with traditional auscultation methods to create a more accurate, efficient, and user-friendly diagnostic tool. Through systematic product planning, customer need analysis, and rigorous specification development, we identified key opportunities to enhance conventional stethoscope functionality. The proposed system features real-time sound analysis, automated classification of heart sounds, wireless connectivity for remote consultations, and an intuitive user interface accessible via smartphone integration. The design process employed a methodical approach incorporating customer feedback, competitive benchmarking, and systematic concept generation and selection. Through a structured evaluation framework, we analyzed portability, frequency response sensitivity, transmission quality, maintenance ease, user interface simplicity, output signal quality, power efficiency, and cost-effectiveness. The final design prioritizes biocompatibility, reliability, and cost-effectiveness while addressing the growing demand for telemedicine capabilities in cardiovascular care. The project emphasizes the transition from conventional design to advanced digital solutions while maintaining a focus on practical clinical applications. Each concept was modelled using SOLIDWORKS software, enabling detailed visualization and engineering analysis. This systematic approach to concept screening and selection ensures the final design meets both current healthcare needs and future technological adaptability.\n",
            "Score: 2\n",
            "\n",
            "Document: 92|||| \n",
            "'arxiv_id': arXiv:2412.14384, \n",
            "'paper_link': https://arxiv.org/abs/2412.14384, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14384, \n",
            "Title: I0T: Embedding Standardization Method Towards Zero Modality Gap \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, $\\text{I0T}_{\\text{post}}$ that reduces the modality gap approximately to zero and (2) a trainable method, $\\text{I0T}_{\\text{async}}$, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, $\\text{I0T}_{\\text{post}}$ can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S).\n",
            "Score: 2\n",
            "\n",
            "Document: 117|||| \n",
            "'arxiv_id': arXiv:2412.14449, \n",
            "'paper_link': https://arxiv.org/abs/2412.14449, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14449, \n",
            "Title: Color Enhancement for V-PCC Compressed Point Cloud via 2D Attribute Map Optimization \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Video-based point cloud compression (V-PCC) converts the dynamic point cloud data into video sequences using traditional video codecs for efficient encoding. However, this lossy compression scheme introduces artifacts that degrade the color attributes of the data. This paper introduces a framework designed to enhance the color quality in the V-PCC compressed point clouds. We propose the lightweight de-compression Unet (LDC-Unet), a 2D neural network, to optimize the projection maps generated during V-PCC encoding. The optimized 2D maps will then be back-projected to the 3D space to enhance the corresponding point cloud attributes. Additionally, we introduce a transfer learning strategy and develop a customized natural image dataset for the initial training. The model was then fine-tuned using the projection maps of the compressed point clouds. The whole strategy effectively addresses the scarcity of point cloud training data. Our experiments, conducted on the public 8i voxelized full bodies long sequences (8iVSLF) dataset, demonstrate the effectiveness of our proposed method in improving the color quality.\n",
            "Score: 2\n",
            "\n",
            "Document: 150|||| \n",
            "'arxiv_id': arXiv:2412.14494, \n",
            "'paper_link': https://arxiv.org/abs/2412.14494, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14494, \n",
            "Title: Drive-1-to-3: Enriching Diffusion Priors for Novel View Synthesis of Real Vehicles \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The recent advent of large-scale 3D data, e.g. Objaverse, has led to impressive progress in training pose-conditioned diffusion models for novel view synthesis. However, due to the synthetic nature of such 3D data, their performance drops significantly when applied to real-world images. This paper consolidates a set of good practices to finetune large pretrained models for a real-world task -- harvesting vehicle assets for autonomous driving applications. To this end, we delve into the discrepancies between the synthetic data and real driving data, then develop several strategies to account for them properly. Specifically, we start with a virtual camera rotation of real images to ensure geometric alignment with synthetic data and consistency with the pose manifold defined by pretrained models. We also identify important design choices in object-centric data curation to account for varying object distances in real driving scenes -- learn across varying object scales with fixed camera focal length. Further, we perform occlusion-aware training in latent spaces to account for ubiquitous occlusions in real data, and handle large viewpoint changes by leveraging a symmetric prior. Our insights lead to effective finetuning that results in a $68.8\\%$ reduction in FID for novel view synthesis over prior arts.\n",
            "Score: 2\n",
            "\n",
            "Document: 176|||| \n",
            "'arxiv_id': arXiv:2412.14539, \n",
            "'paper_link': https://arxiv.org/abs/2412.14539, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14539, \n",
            "Title: Downscaling Precipitation with Bias-informed Conditional Diffusion Model \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Climate change is intensifying rainfall extremes, making high-resolution precipitation projections crucial for society to better prepare for impacts such as flooding. However, current Global Climate Models (GCMs) operate at spatial resolutions too coarse for localized analyses. To address this limitation, deep learning-based statistical downscaling methods offer promising solutions, providing high-resolution precipitation projections with a moderate computational cost. In this work, we introduce a bias-informed conditional diffusion model for statistical downscaling of precipitation. Specifically, our model leverages a conditional diffusion approach to learn distribution priors from large-scale, high-resolution precipitation datasets. The long-tail distribution of precipitation poses a unique challenge for training diffusion models; to address this, we apply gamma correction during preprocessing. Additionally, to correct biases in the downscaled results, we employ a guided-sampling strategy to enhance bias correction. Our experiments demonstrate that the proposed model achieves highly accurate results in an 8 times downscaling setting, outperforming previous deterministic methods. The code and dataset are available at this https URL\n",
            "Score: 2\n",
            "\n",
            "Document: 187|||| \n",
            "'arxiv_id': arXiv:2412.14568, \n",
            "'paper_link': https://arxiv.org/abs/2412.14568, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14568, \n",
            "Title: Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF Separation \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Recent learning-based Multi-View Stereo models have demonstrated state-of-the-art performance in sparse-view 3D reconstruction. However, directly applying 3D Gaussian Splatting (3DGS) as a refinement step following these models presents challenges. We hypothesize that the excessive positional degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting color patterns at the cost of structural fidelity. To address this, we propose reprojection-based DoF separation, a method distinguishing positional DoFs in terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To independently manage each DoF, we introduce a reprojection process along with tailored constraints for each DoF. Through experiments across various datasets, we confirm that separating the positional DoFs of Gaussians and applying targeted constraints effectively suppresses geometric artifacts, producing reconstruction results that are both visually and geometrically plausible.\n",
            "Score: 2\n",
            "\n",
            "Document: 322|||| \n",
            "'arxiv_id': arXiv:2412.14873, \n",
            "'paper_link': https://arxiv.org/abs/2412.14873, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14873, \n",
            "Title: Zero-Shot Artifact2Artifact: Self-incentive artifact removal for photoacoustic imaging without any data \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Photoacoustic imaging (PAI) uniquely combines optical contrast with the penetration depth of ultrasound, making it critical for clinical applications. However, the quality of 3D PAI is often degraded due to reconstruction artifacts caused by the sparse and angle-limited configuration of detector arrays. Existing iterative or deep learning-based methods are either time-consuming or require large training datasets, significantly limiting their practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a zero-shot self-supervised artifact removal method based on a super-lightweight network, which leverages the fact that reconstruction artifacts are sensitive to irregularities caused by data loss. By introducing random perturbations to the acquired PA data, it spontaneously generates subset data, which in turn stimulates the network to learn the artifact patterns in the reconstruction results, thus enabling zero-shot artifact removal. This approach requires neither training data nor prior knowledge of the artifacts, and is capable of artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or slice images in 3D PAI acquired with arbitrarily sparse or angle-limited detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in both simulation study and $ in\\ vivo $ animal experiments. Results demonstrate that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing zero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from 17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in the following GitHub repository: this https URL.\n",
            "Score: 2\n",
            "\n",
            "Document: 344|||| \n",
            "'arxiv_id': arXiv:2412.14967, \n",
            "'paper_link': https://arxiv.org/abs/2412.14967, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14967, \n",
            "Title: ECLIPSE: Contrastive Dimension Importance Estimation with Pseudo-Irrelevance Feedback for Dense Retrieval \n",
            "Subjects: Information Retrieval (cs.IR) \n",
            "Abstract: Recent advances in Information Retrieval have leveraged high-dimensional embedding spaces to improve the retrieval of relevant documents. Moreover, the Manifold Clustering Hypothesis suggests that despite these high-dimensional representations, documents relevant to a query reside on a lower-dimensional, query-dependent manifold. While this hypothesis has inspired new retrieval methods, existing approaches still face challenges in effectively separating non-relevant information from relevant signals. We propose a novel methodology that addresses these limitations by leveraging information from both relevant and non-relevant documents. Our method, ECLIPSE, computes a centroid based on irrelevant documents as a reference to estimate noisy dimensions present in relevant ones, enhancing retrieval performance. Extensive experiments on three in-domain and one out-of-domain benchmarks demonstrate an average improvement of up to 19.50% (resp. 22.35%) in mAP(AP) and 11.42% (resp. 13.10%) in nDCG@10 w.r.t. the DIME-based baseline (resp. the baseline using all dimensions). Our results pave the way for more robust, pseudo-irrelevance-based retrieval systems in future IR research.\n",
            "Score: 2\n",
            "\n",
            "Document: 361|||| \n",
            "'arxiv_id': arXiv:2412.15009, \n",
            "'paper_link': https://arxiv.org/abs/2412.15009, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15009, \n",
            "Title: Projection-based preprocessing for electrical impedance tomography to reduce the effect of electrode contacts \n",
            "Subjects: Numerical Analysis (math.NA) \n",
            "Abstract: This work introduces a method for preprocessing measurements of electrical impedance tomography to considerably reduce the effect uncertainties in the electrode contacts have on the reconstruction quality, without a need to explicitly estimate the contacts. The idea is to compute the Jacobian matrix of the forward map with respect to the contact strengths and project the electrode measurements and the forward map onto the orthogonal complement of the range of this Jacobian. Using the smoothened complete electrode model as the forward model, it is demonstrated that inverting the resulting projected equation with respect to only the internal conductivity of the examined body results in good quality reconstructions both when resorting to a single step linearization with a smoothness prior and when combining lagged diffusivity iteration with total variation regularization. The quality of the reconstructions is further improved if the range of the employed projection is also orthogonal to that of the Jacobian with respect to the electrode positions. These results hold even if the projections are formed at internal and contact conductivities that significantly differ from the true ones; it is numerically demonstrated that the orthogonal complement of the range of the contact Jacobian is almost independent of the conductivity parameters at which it is evaluated. In particular, our observations introduce a numerical technique for inferring whether a change in the electrode measurements is caused by a change in the internal conductivity or alterations in the electrode contacts, which has potential applications, e.g., in bedside monitoring of stroke patients. The ideas are tested both on simulated data and on real-world water tank measurements with adjustable contact resistances.\n",
            "Score: 2\n",
            "\n",
            "Document: 422|||| \n",
            "'arxiv_id': arXiv:2412.15188, \n",
            "'paper_link': https://arxiv.org/abs/2412.15188, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15188, \n",
            "Title: LlamaFusion: Adapting Pretrained Language Models for Multimodal Generation \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: We present LlamaFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LlamaFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LlamaFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LlamaFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development.\n",
            "Score: 2\n",
            "\n",
            "Document: 448|||| \n",
            "'arxiv_id': arXiv:2412.14403, \n",
            "'paper_link': https://arxiv.org/abs/2412.14403, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14403, \n",
            "Title: Short-term wind forecasting via surface pressure measurements: stochastic modeling and sensor placement \n",
            "Subjects: Fluid Dynamics (physics.flu-dyn) \n",
            "Abstract: We propose a short-term wind forecasting framework for predicting real-time variations in atmospheric turbulence based on nacelle-mounted anemometer and ground-level air-pressure measurements. Our approach combines linear stochastic estimation and Kalman filtering algorithms to assimilate and process real-time field measurements with the predictions of a stochastic reduced-order model that is confined to a two-dimensional plane at the hub height of turbines. We bridge the vertical gap between the computational plane of the model at hub height and the measurement plane on the ground using a projection technique that allows us to infer the pressure in one plane from the other. Depending on the quality of this inference, we show that customized variants of the extended and ensemble Kalman filters can be tuned to balance estimation quality and computational speed 1-1.5 diameters ahead and behind leading turbines. In particular, we show how synchronizing the sign of estimates with that of velocity fluctuations recorded at the nacelle can significantly improve the ability to follow temporal variations upwind of the leading turbine. We also propose a convex optimization-based framework for selecting a subset of pressure sensors that achieve a desired level of accuracy relative to the optimal Kalman filter that uses all sensing capabilities.\n",
            "Score: 2\n",
            "\n",
            "Document: 570|||| \n",
            "'arxiv_id': arXiv:2407.00640, \n",
            "'paper_link': https://arxiv.org/abs/2407.00640, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.00640, \n",
            "Title: Physics-augmented neural networks for constitutive modeling of hyperelastic geometrically exact beams \n",
            "Subjects: Computational Engineering, Finance, and Science (cs.CE) \n",
            "Abstract: We present neural network-based constitutive models for hyperelastic geometrically exact beams. The proposed models are physics-augmented, i.e., formulated to fulfill important mechanical conditions by construction, which improves accuracy and generalization. Strains and curvatures of the beam are used as input for feed-forward neural networks that represent the effective hyperelastic beam potential. Forces and moments are received as the gradients of the beam potential, ensuring thermodynamic consistency. Normalization conditions are considered via additional projection terms. Symmetry conditions are implemented by an invariant-based approach for transverse isotropy and a more flexible point symmetry constraint, which is included in transverse isotropy but poses fewer restrictions on the constitutive response. Furthermore, a data augmentation approach is proposed to improve the scaling behavior of the models for varying cross-section radii. Additionally, we introduce a parameterization with a scalar parameter to represent ring-shaped cross-sections with different ratios between the inner and outer radii. Formulating the beam potential as a neural network provides a highly flexible model. This enables efficient constitutive surrogate modeling for geometrically exact beams with nonlinear material behavior and cross-sectional deformation, which otherwise would require computationally much more expensive methods. The models are calibrated and tested with data generated for beams with circular and ring-shaped hyperelastic deformable cross-sections at varying inner and outer radii, showing excellent accuracy and generalization. The applicability of the proposed point symmetric model is further demonstrated by applying it in beam simulations. In all studied cases, the proposed model shows excellent performance.\n",
            "Score: 2\n",
            "\n",
            "Document: 599|||| \n",
            "'arxiv_id': arXiv:2408.11479, \n",
            "'paper_link': https://arxiv.org/abs/2408.11479, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.11479, \n",
            "Title: Learning Deep Dissipative Dynamics \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: This study challenges strictly guaranteeing ``dissipativity'' of a dynamical system represented by neural networks learned from given time-series data. Dissipativity is a crucial indicator for dynamical systems that generalizes stability and input-output stability, known to be valid across various systems including robotics, biological systems, and molecular dynamics. By analytically proving the general solution to the nonlinear Kalman-Yakubovich-Popov (KYP) lemma, which is the necessary and sufficient condition for dissipativity, we propose a differentiable projection that transforms any dynamics represented by neural networks into dissipative ones and a learning method for the transformed dynamics. Utilizing the generality of dissipativity, our method strictly guarantee stability, input-output stability, and energy conservation of trained dynamical systems. Finally, we demonstrate the robustness of our method against out-of-domain input through applications to robotic arms and fluid dynamics. Code is this https URL\n",
            "Score: 2\n",
            "\n",
            "Document: 644|||| \n",
            "'arxiv_id': arXiv:2410.22870, \n",
            "'paper_link': https://arxiv.org/abs/2410.22870, \n",
            "'pdf_link': https://arxiv.org/pdf/2410.22870, \n",
            "Title: Conditioned quantum-assisted deep generative surrogate for particle-calorimeter interactions \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Particle collisions at accelerators such as the Large Hadron Collider, recorded and analyzed by experiments such as ATLAS and CMS, enable exquisite measurements of the Standard Model and searches for new phenomena. Simulations of collision events at these detectors have played a pivotal role in shaping the design of future experiments and analyzing ongoing ones. However, the quest for accuracy in Large Hadron Collider (LHC) collisions comes at an imposing computational cost, with projections estimating the need for millions of CPU-years annually during the High Luminosity LHC (HL-LHC) run \\cite{collaboration2022atlas}. Simulating a single LHC event with \\textsc{Geant4} currently devours around 1000 CPU seconds, with simulations of the calorimeter subdetectors in particular imposing substantial computational demands \\cite{rousseau2023experimental}. To address this challenge, we propose a conditioned quantum-assisted deep generative model. Our model integrates a conditioned variational autoencoder (VAE) on the exterior with a conditioned Restricted Boltzmann Machine (RBM) in the latent space, providing enhanced expressiveness compared to conventional VAEs. The RBM nodes and connections are meticulously engineered to enable the use of qubits and couplers on D-Wave's Pegasus-structured \\textit{Advantage} quantum annealer (QA) for sampling. We introduce a novel method for conditioning the quantum-assisted RBM using \\textit{flux biases}. We further propose a novel adaptive mapping to estimate the effective inverse temperature in quantum annealers. The effectiveness of our framework is illustrated using Dataset 2 of the CaloChallenge \\cite{calochallenge}.\n",
            "Score: 2\n",
            "\n",
            "Document: 766|||| \n",
            "'arxiv_id': arXiv:2408.06401, \n",
            "'paper_link': https://arxiv.org/abs/2408.06401, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.06401, \n",
            "Title: Langevin dynamics for high-dimensional optimization: the case of multi-spiked tensor PCA \n",
            "Subjects: Machine Learning (stat.ML) \n",
            "Abstract: We study nonconvex optimization in high dimensions through Langevin dynamics, focusing on the multi-spiked tensor PCA problem. This tensor estimation problem involves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian tensor observations using maximum likelihood estimation. We study the number of samples required for Langevin dynamics to efficiently recover the spikes and determine the necessary separation condition on the signal-to-noise ratios (SNRs) for exact recovery, distinguishing the cases $p \\ge 3$ and $p=2$, where $p$ denotes the order of the tensor. In particular, we show that the sample complexity required for recovering the spike associated with the largest SNR matches the well-known algorithmic threshold for the single-spike case, while this threshold degrades when recovering all $r$ spikes. As a key step, we provide a detailed characterization of the trajectory and interactions of low-dimensional projections that capture the high-dimensional dynamics.\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 3\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"distance measure\", 10),\n",
        "        (\"similarity measure\", 10),\n",
        "        (\"vector distance\", 10),\n",
        "        (\"distance metric\", 10),\n",
        "        (\"similarity metric\", 10),\n",
        "        (\"dimension reduction\", 10),\n",
        "\n",
        "        (\"similarity\", 1),\n",
        "        (\"distance\", 1),\n",
        "        (\"metric\", 1),\n",
        "    ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HOGJ_6VhZtO",
        "outputId": "7cf58afa-23e1-4441-8b4c-5bbc9a158b74"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: distance measure\n",
            "Total Matches in Set: 140\n",
            "Matches Above Score-Floor in Set: 4\n",
            "2024-12-20__041109494279\n",
            "\n",
            "Showing 4 in top-45 out of 140 total results.     -> 4 of 45/140\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 3) -> 3\n",
            "\n",
            "\n",
            "Document: 194|||| \n",
            "'arxiv_id': arXiv:2412.14580, \n",
            "'paper_link': https://arxiv.org/abs/2412.14580, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14580, \n",
            "Title: DiffSim: Taming Diffusion Models for Evaluating Visual Similarity \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Diffusion models have fundamentally transformed the field of generative models, making the assessment of similarity between customized model outputs and reference inputs critically important. However, traditional perceptual similarity metrics operate primarily at the pixel and patch levels, comparing low-level colors and textures but failing to capture mid-level similarities and differences in image layout, object pose, and semantic content. Contrastive learning-based CLIP and self-supervised learning-based DINO are often used to measure semantic similarity, but they highly compress image features, inadequately assessing appearance details. This paper is the first to discover that pretrained diffusion models can be utilized for measuring visual similarity and introduces the DiffSim method, addressing the limitations of traditional metrics in capturing perceptual consistency in custom generation tasks. By aligning features in the attention layers of the denoising U-Net, DiffSim evaluates both appearance and style similarity, showing superior alignment with human visual preferences. Additionally, we introduce the Sref and IP benchmarks to evaluate visual similarity at the level of style and instance, respectively. Comprehensive evaluations across multiple benchmarks demonstrate that DiffSim achieves state-of-the-art performance, providing a robust tool for measuring visual coherence in generative models.\n",
            "Score: 12\n",
            "\n",
            "Document: 242|||| \n",
            "'arxiv_id': arXiv:2412.14671, \n",
            "'paper_link': https://arxiv.org/abs/2412.14671, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14671, \n",
            "Title: MUSTER: Longitudinal Deformable Registration by Composition of Consecutive Deformations \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Longitudinal imaging allows for the study of structural changes over time. One approach to detecting such changes is by non-linear image registration. This study introduces Multi-Session Temporal Registration (MUSTER), a novel method that facilitates longitudinal analysis of changes in extended series of medical images. MUSTER improves upon conventional pairwise registration by incorporating more than two imaging sessions to recover longitudinal deformations. Longitudinal analysis at a voxel-level is challenging due to effects of a changing image contrast as well as instrumental and environmental sources of bias between sessions. We show that local normalized cross-correlation as an image similarity metric leads to biased results and propose a robust alternative. We test the performance of MUSTER on a synthetic multi-site, multi-session neuroimaging dataset and show that, in various scenarios, using MUSTER significantly enhances the estimated deformations relative to pairwise registration. Additionally, we apply MUSTER on a sample of older adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. The results show that MUSTER can effectively identify patterns of neuro-degeneration from T1-weighted images and that these changes correlate with changes in cognition, matching the performance of state of the art segmentation methods. By leveraging GPU acceleration, MUSTER efficiently handles large datasets, making it feasible also in situations with limited computational resources.\n",
            "Score: 12\n",
            "\n",
            "Document: 623|||| \n",
            "'arxiv_id': arXiv:2409.18472, \n",
            "'paper_link': https://arxiv.org/abs/2409.18472, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.18472, \n",
            "Title: URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological and Multilingual Knowledge Base \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: URIEL is a knowledge base offering geographical, phylogenetic, and typological vector representations for 7970 languages. It includes distance measures between these vectors for 4005 languages, which are accessible via the lang2vec tool. Despite being frequently cited, URIEL is limited in terms of linguistic inclusion and overall usability. To tackle these challenges, we introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses these limitations. In addition to expanding typological feature coverage for 2898 languages, URIEL+ improves the user experience with robust, customizable distance calculations to better suit the needs of users. These upgrades also offer competitive performance on downstream tasks and provide distances that better align with linguistic distance studies.\n",
            "Score: 11\n",
            "\n",
            "Document: 456|||| \n",
            "'arxiv_id': arXiv:2412.14723, \n",
            "'paper_link': https://arxiv.org/abs/2412.14723, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14723, \n",
            "Title: Dimension reduction for path signatures \n",
            "Subjects: Probability (math.PR) \n",
            "Abstract: This paper focuses on the mathematical framework for reducing the complexity of models using path signatures. The structure of these signatures, which can be interpreted as collections of iterated integrals along paths, is discussed and their applications in areas such as stochastic differential equations (SDEs) and financial modeling are pointed out. In particular, exploiting the rough paths view, solutions of SDEs continuously depend on the lift of the driver. Such continuous mappings can be approximated using (truncated) signatures, which are solutions of high-dimensional linear systems. In order to lower the complexity of these models, this paper presents methods for reducing the order of high-dimensional truncated signature models while retaining essential characteristics. The derivation of reduced models and the universal approximation property of (truncated) signatures are treated in detail. Numerical examples, including applications to the (rough) Bergomi model in financial markets, illustrate the proposed reduction techniques and highlight their effectiveness.\n",
            "Score: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 3\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"parametric\", 10),\n",
        "    ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAncfvGn6URQ",
        "outputId": "675bf1c3-ce08-4bde-99d5-357c720917d9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: parametric\n",
            "Total Matches in Set: 8\n",
            "Matches Above Score-Floor in Set: 8\n",
            "2024-12-20__041109580067\n",
            "\n",
            "Showing 8 in top-8 out of 8 total results.     -> 8 of 8/8\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 3) -> 3\n",
            "\n",
            "\n",
            "Document: 276|||| \n",
            "'arxiv_id': arXiv:2412.14744, \n",
            "'paper_link': https://arxiv.org/abs/2412.14744, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14744, \n",
            "Title: A parametric algorithm is optimal for non-parametric regression of smooth functions \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: We address the regression problem for a general function $f:[-1,1]^d\\to \\mathbb R$ when the learner selects the training points $\\{x_i\\}_{i=1}^n$ to achieve a uniform error bound across the entire domain. In this setting, known historically as nonparametric regression, we aim to establish a sample complexity bound that depends solely on the function's degree of smoothness. Assuming periodicity at the domain boundaries, we introduce PADUA, an algorithm that, with high probability, provides performance guarantees optimal up to constant or logarithmic factors across all problem parameters. Notably, PADUA is the first parametric algorithm with optimal sample complexity for this setting. Due to this feature, we prove that, differently from the non-parametric state of the art, PADUA enjoys optimal space complexity in the prediction phase. To validate these results, we perform numerical experiments over functions coming from real audio data, where PADUA shows comparable performance to state-of-the-art methods, while requiring only a fraction of the computational time.\n",
            "Score: 10\n",
            "\n",
            "Document: 419|||| \n",
            "'arxiv_id': arXiv:2412.15182, \n",
            "'paper_link': https://arxiv.org/abs/2412.15182, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15182, \n",
            "Title: STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Robot learning is witnessing a significant increase in the size, diversity, and complexity of pre-collected datasets, mirroring trends in domains such as natural language processing and computer vision. Many robot learning methods treat such datasets as multi-task expert data and learn a multi-task, generalist policy by training broadly across them. Notably, while these generalist policies can improve the average performance across many tasks, the performance of generalist policies on any one task is often suboptimal due to negative transfer between partitions of the data, compared to task-specific specialist policies. In this work, we argue for the paradigm of training policies during deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant data at test time. Furthermore, we show that many robotics tasks share considerable amounts of low-level behaviors and that retrieval at the \"sub\"-trajectory granularity enables significantly improved data utilization, generalization, and robustness in adapting policies to novel problems. In contrast, existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared cross-task content. This work proposes STRAP, a technique for leveraging pre-trained vision foundation models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in a robust fashion. STRAP outperforms both prior retrieval algorithms and multi-task learning methods in simulated and real experiments, showing the ability to scale to much larger offline datasets in the real world as well as the ability to learn robust control policies with just a handful of real-world demonstrations.\n",
            "Score: 10\n",
            "\n",
            "Document: 429|||| \n",
            "'arxiv_id': arXiv:2412.15200, \n",
            "'paper_link': https://arxiv.org/abs/2412.15200, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15200, \n",
            "Title: DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models.\n",
            "Score: 10\n",
            "\n",
            "Document: 545|||| \n",
            "'arxiv_id': arXiv:2405.14205, \n",
            "'paper_link': https://arxiv.org/abs/2405.14205, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.14205, \n",
            "Title: Agent Planning with World Knowledge Model \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Recent endeavors towards directly using large language models (LLMs) as agent models to execute interactive planning tasks have shown commendable results. Despite their achievements, however, they still struggle with brainless trial-and-error in global planning and generating hallucinatory actions in local planning due to their poor understanding of the ``real'' physical world. Imitating humans' mental world knowledge model which provides global prior knowledge before the task and maintains local dynamic knowledge during the task, in this paper, we introduce parametric World Knowledge Model (WKM) to facilitate agent planning. Concretely, we steer the agent model to self-synthesize knowledge from both expert and sampled trajectories. Then we develop WKM, providing prior task knowledge to guide the global planning and dynamic state knowledge to assist the local planning. Experimental results on three complex real-world simulated datasets with three state-of-the-art open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our method can achieve superior performance compared to various strong baselines. Besides, we analyze to illustrate that our WKM can effectively alleviate the blind trial-and-error and hallucinatory action issues, providing strong support for the agent's understanding of the world. Other interesting findings include: 1) our instance-level task knowledge can generalize better to unseen tasks, 2) weak WKM can guide strong agent model planning, and 3) unified WKM training has promising potential for further development. The code is available at this https URL.\n",
            "Score: 10\n",
            "\n",
            "Document: 548|||| \n",
            "'arxiv_id': arXiv:2405.14768, \n",
            "'paper_link': https://arxiv.org/abs/2405.14768, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.14768, \n",
            "Title: WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Large language models (LLMs) need knowledge updates to meet the ever-growing world facts and correct the hallucinated responses, facilitating the methods of lifelong model editing. Where the updated knowledge resides in memories is a fundamental question for model editing. In this paper, we find that editing either long-term memory (direct model parameters) or working memory (non-parametric knowledge of neural network activations/representations by retrieval) will result in an impossible triangle -- reliability, generalization, and locality can not be realized together in the lifelong editing settings. For long-term memory, directly editing the parameters will cause conflicts with irrelevant pretrained knowledge or previous edits (poor reliability and locality). For working memory, retrieval-based activations can hardly make the model understand the edits and generalize (poor generalization). Therefore, we propose WISE to bridge the gap between memories. In WISE, we design a dual parametric memory scheme, which consists of the main memory for the pretrained knowledge and a side memory for the edited knowledge. We only edit the knowledge in the side memory and train a router to decide which memory to go through when given a query. For continual editing, we devise a knowledge-sharding mechanism where different sets of edits reside in distinct subspaces of parameters, and are subsequently merged into a shared memory without conflicts. Extensive experiments show that WISE can outperform previous model editing methods and overcome the impossible triangle under lifelong model editing of question answering, hallucination, and out-of-distribution settings across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is available at this https URL.\n",
            "Score: 10\n",
            "\n",
            "Document: 549|||| \n",
            "'arxiv_id': arXiv:2405.15557, \n",
            "'paper_link': https://arxiv.org/abs/2405.15557, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.15557, \n",
            "Title: Learning from Linear Algebra: A Graph Neural Network Approach to Preconditioner Design for Conjugate Gradient Solvers \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Large linear systems are ubiquitous in modern computational science and engineering. The main recipe for solving them is the use of Krylov subspace iterative methods with well-designed preconditioners. Deep learning models can be used as nonlinear preconditioners during the iteration of linear solvers such as the conjugate gradient (CG) method. Neural network models require an enormous number of parameters to approximate well in this setup. Another approach is to take advantage of small graph neural networks (GNNs) to construct preconditioners with predefined sparsity patterns. Recently, GNNs have been shown to be a promising tool for designing preconditioners to reduce the overall computational cost of iterative methods by constructing them more efficiently than with classical linear algebra techniques. However, preconditioners designed with these approaches cannot outperform those designed with classical methods in terms of the number of iterations in CG. In our work, we recall well-established preconditioners from linear algebra and use them as a starting point for training the GNN to obtain preconditioners that reduce the condition number of the system more significantly. Numerical experiments show that our approach outperforms both classical and neural network-based methods for an important class of parametric partial differential equations. We also provide a heuristic justification for the loss function used and show that preconditioners obtained by learning with this loss function reduce the condition number in a more desirable way for CG.\n",
            "Score: 10\n",
            "\n",
            "Document: 738|||| \n",
            "'arxiv_id': arXiv:2412.14168, \n",
            "'paper_link': https://arxiv.org/abs/2412.14168, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14168, \n",
            "Title: FashionComposer: Compositional Fashion Image Generation \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: We present FashionComposer for compositional fashion image generation. Unlike previous methods, FashionComposer is highly flexible. It takes multi-modal input (i.e., text prompt, parametric human model, garment image, and face image) and supports personalizing the appearance, pose, and figure of the human and assigning multiple garments in one pass. To achieve this, we first develop a universal framework capable of handling diverse input modalities. We construct scaled training data to enhance the model's robust compositional capabilities. To accommodate multiple reference images (garments and faces) seamlessly, we organize these references in a single image as an \"asset library\" and employ a reference UNet to extract appearance features. To inject the appearance features into the correct pixels in the generated result, we propose subject-binding attention. It binds the appearance features from different \"assets\" with the corresponding text features. In this way, the model could understand each asset according to their semantics, supporting arbitrary numbers and types of reference images. As a comprehensive solution, FashionComposer also supports many other applications like human album generation, diverse virtual try-on tasks, etc.\n",
            "Score: 10\n",
            "\n",
            "Document: 744|||| \n",
            "'arxiv_id': arXiv:2202.06374, \n",
            "'paper_link': https://arxiv.org/abs/2202.06374, \n",
            "'pdf_link': https://arxiv.org/pdf/2202.06374, \n",
            "Title: Holdouts set for safe predictive model updating \n",
            "Subjects: Machine Learning (stat.ML) \n",
            "Abstract: Predictive risk scores for adverse outcomes are increasingly crucial in guiding health interventions. Such scores may need to be periodically updated due to change in the distributions they model. However, directly updating risk scores used to guide intervention can lead to biased risk estimates. To address this, we propose updating using a `holdout set' - a subset of the population that does not receive interventions guided by the risk score. Balancing the holdout set size is essential to ensure good performance of the updated risk score whilst minimising the number of held out samples. We prove that this approach reduces adverse outcome frequency to an asymptotically optimal level and argue that often there is no competitive alternative. We describe conditions under which an optimal holdout size (OHS) can be readily identified, and introduce parametric and semi-parametric algorithms for OHS estimation. We apply our methods to the ASPRE risk score for pre-eclampsia to recommend a plan for updating it in the presence of change in the underlying data distribution. We show that, in order to minimise the number of pre-eclampsia cases over time, this is best achieved using a holdout set of around 10,000 individuals.\n",
            "Score: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"survey\", 1),\n",
        "        (\"election\", 1),\n",
        "        (\"voting\", 1),\n",
        "        (\"poll\", 1),\n",
        "        (\"vote\", 1),\n",
        "        (\"candidate\", 1),\n",
        "\n",
        "        (\"selection\", .5),\n",
        "        (\"coordination\", .5),\n",
        "        (\"consensus\", .5),\n",
        "        (\"campaign\", .5),\n",
        "\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Po_ipgoynfr",
        "outputId": "088d16aa-445c-46a8-c0b9-3de5147e54a2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: survey\n",
            "Total Matches in Set: 72\n",
            "Matches Above Score-Floor in Set: 4\n",
            "2024-12-20__041109650292\n",
            "\n",
            "Showing 4 in top-45 out of 72 total results.     -> 4 of 45/72\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 405|||| \n",
            "'arxiv_id': arXiv:2412.15146, \n",
            "'paper_link': https://arxiv.org/abs/2412.15146, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15146, \n",
            "Title: Cruise Control: Dynamic Model Selection for ML-Based Network Traffic Analysis \n",
            "Subjects: Networking and Internet Architecture (cs.NI) \n",
            "Abstract: Modern networks increasingly rely on machine learning models for real-time insights, including traffic classification, application quality of experience inference, and intrusion detection. However, existing approaches prioritize prediction accuracy without considering deployment constraints or the dynamism of network traffic, leading to potentially suboptimal performance. Because of this, deploying ML models in real-world networks with tight performance constraints remains an open challenge. In contrast with existing work that aims to select an optimal candidate model for each task based on offline information, we propose an online, system-driven approach to dynamically select the best ML model for network traffic analysis. To this end, we present Cruise Control, a system that pre-trains several models for a given task with different accuracy-cost tradeoffs and selects the most appropriate model based on lightweight signals representing the system's current traffic processing ability. Experimental results using two real-world traffic analysis tasks demonstrate Cruise Control's effectiveness in adapting to changing network conditions. Our evaluation shows that Cruise Control improves median accuracy by 2.78% while reducing packet loss by a factor of four compared to offline-selected models.\n",
            "Score: 2.5\n",
            "\n",
            "Document: 581|||| \n",
            "'arxiv_id': arXiv:2407.16302, \n",
            "'paper_link': https://arxiv.org/abs/2407.16302, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.16302, \n",
            "Title: DeepClean: Integrated Distortion Identification and Algorithm Selection for Rectifying Image Corruptions \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Distortion identification and rectification in images and videos is vital for achieving good performance in downstream vision applications. Instead of relying on fixed trial-and-error based image processing pipelines, we propose a two-level sequential planning approach for automated image distortion classification and rectification. At the higher level it detects the class of corruptions present in the input image, if any. The lower level selects a specific algorithm to be applied, from a set of externally provided candidate algorithms. The entire two-level setup runs in the form of a single forward pass during inference and it is to be queried iteratively until the retrieval of the original image. We demonstrate improvements compared to three baselines on the object detection task on COCO image dataset with rich set of distortions. The advantage of our approach is its dynamic reconfiguration, conditioned on the input image and generalisability to unseen candidate algorithms at inference time, since it relies only on the comparison of their output of the image embeddings.\n",
            "Score: 2.5\n",
            "\n",
            "Document: 755|||| \n",
            "'arxiv_id': arXiv:2403.07728, \n",
            "'paper_link': https://arxiv.org/abs/2403.07728, \n",
            "'pdf_link': https://arxiv.org/pdf/2403.07728, \n",
            "Title: CAP: A General Algorithm for Online Selective Conformal Prediction with FCR Control \n",
            "Subjects: Machine Learning (stat.ML) \n",
            "Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) which measures the overall miscoverage level. We develop a general framework named CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on historical data to construct a calibration set if the current individual is selected and then outputs a conformal prediction interval for the unobserved label. We provide tractable procedures for constructing the calibration set for popular online selection rules. We proved that CAP can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. To account for the distribution shift in online data, we also embed CAP into some recent dynamic conformal prediction algorithms and show that the proposed method can deliver long-run FCR control. Numerical results on both synthetic and real data corroborate that CAP can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.\n",
            "Score: 2.5\n",
            "\n",
            "Document: 661|||| \n",
            "'arxiv_id': arXiv:2411.16456, \n",
            "'paper_link': https://arxiv.org/abs/2411.16456, \n",
            "'pdf_link': https://arxiv.org/pdf/2411.16456, \n",
            "Title: Proxima. A DAG based cooperative distributed ledger \n",
            "Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) \n",
            "Abstract: This paper introduces a novel architecture for a distributed ledger, commonly referred to as a \"blockchain\", which is organized in the form of directed acyclic graph (DAG) with UTXO transactions as vertices, rather than as a chain of blocks. Consensus on the state of ledger assets is achieved through the cooperative consensus: a profit-driven behavior of token holders themselves, which is viable only when they cooperate by following the biggest ledger coverage rule. The cooperative behavior is facilitated by enforcing purposefully designed UTXO transaction validity constraints. Token holders are the sole category of participants authorized to make amendments to the ledger, making participation completely permissionless - without miners, validators, committees or staking - and without any need of knowledge about the composition of the set of all participants in the consensus. The setup allows to achieve high throughput and scalability alongside with low transaction costs, while preserving key aspects of high decentralization, open participation, and asynchronicity found in Bitcoin and other proof-of-work blockchains, but without unreasonable energy consumption. Sybil protection is achieved similarly to proof-of-stake blockchains, using tokens native to the ledger, yet the architecture operates in a leaderless manner without block proposers and committee selection.\n",
            "Score: 2.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 1\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"disinformation\", 1),\n",
        "        (\"manipulate public opinion\", 1),\n",
        "        (\"conspiracy\", 1),\n",
        "        (\"radicalization\", 1),\n",
        "        (\"conspiracy theories\", 1),\n",
        "        (\"violent extremism\", 2),\n",
        "\n",
        "        (\"extremism\", 1),\n",
        "        (\"extremist\", 1),\n",
        "        (\"extreme views\", 1),\n",
        "        (\"extreme beliefs\", 1),\n",
        "        (\"extreme action\", 1),\n",
        "        (\"ideology\", .5),        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdmCM3canYCW",
        "outputId": "80317c1f-536c-4437-c247-e3a14193ab1c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: disinformation\n",
            "Total Matches in Set: 5\n",
            "Matches Above Score-Floor in Set: 4\n",
            "2024-12-20__041109749584\n",
            "\n",
            "Showing 4 in top-5 out of 5 total results.     -> 4 of 5/5\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 1) -> 1\n",
            "\n",
            "\n",
            "Document: 386|||| \n",
            "'arxiv_id': arXiv:2412.15098, \n",
            "'paper_link': https://arxiv.org/abs/2412.15098, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15098, \n",
            "Title: A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts.\n",
            "Score: 2\n",
            "\n",
            "Document: 710|||| \n",
            "'arxiv_id': arXiv:2412.11745, \n",
            "'paper_link': https://arxiv.org/abs/2412.11745, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.11745, \n",
            "Title: Beyond Dataset Creation: Critical View of Annotation Variation and Bias Probing of a Dataset for Online Radical Content Detection \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: The proliferation of radical content on online platforms poses significant risks, including inciting violence and spreading extremist ideologies. Despite ongoing research, existing datasets and models often fail to address the complexities of multilingual and diverse data. To bridge this gap, we introduce a publicly available multilingual dataset annotated with radicalization levels, calls for action, and named entities in English, French, and Arabic. This dataset is pseudonymized to protect individual privacy while preserving contextual information. Beyond presenting our freely available dataset, we analyze the annotation process, highlighting biases and disagreements among annotators and their implications for model performance. Additionally, we use synthetic data to investigate the influence of socio-demographic traits on annotation patterns and model predictions. Our work offers a comprehensive examination of the challenges and opportunities in building robust datasets for radical content detection, emphasizing the importance of fairness and transparency in model development.\n",
            "Score: 2\n",
            "\n",
            "Document: 155|||| \n",
            "'arxiv_id': arXiv:2412.14500, \n",
            "'paper_link': https://arxiv.org/abs/2412.14500, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14500, \n",
            "Title: The Digital Ecosystem of Beliefs: does evolution favour AI over humans? \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: As AI systems are integrated into social networks, there are AI safety concerns that AI-generated content may dominate the web, e.g. in popularity or impact on this http URL understand such questions, this paper proposes the Digital Ecosystem of Beliefs (Digico), the first evolutionary framework for controlled experimentation with multi-population interactions in simulated social networks. The framework models a population of agents which change their messaging strategies due to evolutionary updates following a Universal Darwinism approach, interact via messages, influence each other's beliefs through dynamics based on a contagion model, and maintain their beliefs through cognitive Lamarckian inheritance. Initial experiments with an abstract implementation of Digico show that: a) when AIs have faster messaging, evolution, and more influence in the recommendation algorithm, they get 80% to 95% of the views, depending on the size of the influence benefit; b) AIs designed for propaganda can typically convince 50% of humans to adopt extreme beliefs, and up to 85% when agents believe only a limited number of channels; c) a penalty for content that violates agents' beliefs reduces propaganda effectiveness by up to 8%. We further discuss implications for control (e.g. legislation) and Digico as a means of studying evolutionary principles.\n",
            "Score: 1\n",
            "\n",
            "Document: 237|||| \n",
            "'arxiv_id': arXiv:2412.14663, \n",
            "'paper_link': https://arxiv.org/abs/2412.14663, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14663, \n",
            "Title: IOHunter: Graph Foundation Model to Uncover Online Information Operations \n",
            "Subjects: Social and Information Networks (cs.SI) \n",
            "Abstract: Social media platforms have become vital spaces for public discourse, serving as modern agors where a wide range of voices influence societal narratives. However, their open nature also makes them vulnerable to exploitation by malicious actors, including state-sponsored entities, who can conduct information operations (IOs) to manipulate public opinion. The spread of misinformation, false news, and misleading claims threatens democratic processes and societal cohesion, making it crucial to develop methods for the timely detection of inauthentic activity to protect the integrity of online discourse. In this work, we introduce a methodology designed to identify users orchestrating information operations, a.k.a. \\textit{IO drivers}, across various influence campaigns. Our framework, named \\texttt{IOHunter}, leverages the combined strengths of Language Models and Graph Neural Networks to improve generalization in \\emph{supervised}, \\emph{scarcely-supervised}, and \\emph{cross-IO} contexts. Our approach achieves state-of-the-art performance across multiple sets of IOs originating from six countries, significantly surpassing existing approaches. This research marks a step toward developing Graph Foundation Models specifically tailored for the task of IO detection on social media platforms.\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 1\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Speech-LLM\", 1),\n",
        "\n",
        "        (\"spoken language understanding\", 1),\n",
        "\n",
        "        (\"speech to text\", 1),\n",
        "        (\"text to speech\", 1),\n",
        "\n",
        "        (\"audio modality\", .5),\n",
        "        (\"speech encoder\", .5),\n",
        "        (\"SLU\", .5),\n",
        "        (\"stt\", .5),\n",
        "        (\"tts\", .5),\n",
        "\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHUhj_4zWLME",
        "outputId": "7f5e3041-0d8c-4a10-85fe-8fbe72e90f8b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Speech-LLM\n",
            "Total Matches in Set: 112\n",
            "Matches Above Score-Floor in Set: 3\n",
            "2024-12-20__041109849517\n",
            "\n",
            "Showing 3 in top-45 out of 112 total results.     -> 3 of 45/112\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 1) -> 1\n",
            "\n",
            "\n",
            "Document: 711|||| \n",
            "'arxiv_id': arXiv:2412.11795, \n",
            "'paper_link': https://arxiv.org/abs/2412.11795, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.11795, \n",
            "Title: ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible Speech Synthesis \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Prosody contains rich information beyond the literal meaning of words, which is crucial for the intelligibility of speech. Current models still fall short in phrasing and intonation; they not only miss or misplace breaks when synthesizing long sentences with complex structures but also produce unnatural intonation. We propose ProsodyFM, a prosody-aware text-to-speech synthesis (TTS) model with a flow-matching (FM) backbone that aims to enhance the phrasing and intonation aspects of prosody. ProsodyFM introduces two key components: a Phrase Break Encoder to capture initial phrase break locations, followed by a Duration Predictor for the flexible adjustment of break durations; and a Terminal Intonation Encoder which learns a bank of intonation shape tokens combined with a novel Pitch Processor for more robust modeling of human-perceived intonation change. ProsodyFM is trained with no explicit prosodic labels and yet can uncover a broad spectrum of break durations and intonation patterns. Experimental results demonstrate that ProsodyFM can effectively improve the phrasing and intonation aspects of prosody, thereby enhancing the overall intelligibility compared to four state-of-the-art (SOTA) models. Out-of-distribution experiments show that this prosody improvement can further bring ProsodyFM superior generalizability for unseen complex sentences and speakers. Our case study intuitively illustrates the powerful and fine-grained controllability of ProsodyFM over phrasing and intonation.\n",
            "Score: 2.0\n",
            "\n",
            "Document: 520|||| \n",
            "'arxiv_id': arXiv:2402.15083, \n",
            "'paper_link': https://arxiv.org/abs/2402.15083, \n",
            "'pdf_link': https://arxiv.org/pdf/2402.15083, \n",
            "Title: Hands-Free VR \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their first language, and to word phonetic similarity, correctly transcribing the voice command 96.71% of the time; (2) Hands-Free VR is robust to natural language diversity, correctly mapping the transcribed command to an executable command in 97.83% of the time; (3) Hands-Free VR had a significant efficiency advantage over the conventional VR interface in terms of task completion time, total viewpoint translation, total view direction rotation, and total left and right hand translations; (4) Hands-Free VR received high user preference ratings in terms of ease of use, intuitiveness, ergonomics, reliability, and desirability.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 465|||| \n",
            "'arxiv_id': arXiv:2412.14890, \n",
            "'paper_link': https://arxiv.org/abs/2412.14890, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14890, \n",
            "Title: Scale This, Not That: Investigating Key Dataset Attributes for Efficient Speech Enhancement Scaling \n",
            "Subjects: Audio and Speech Processing (eess.AS) \n",
            "Abstract: Recent speech enhancement models have shown impressive performance gains by scaling up model complexity and training data. However, the impact of dataset variability (e.g. text, language, speaker, and noise) has been underexplored. Analyzing each attribute individually is often challenging, as multiple attributes are usually entangled in commonly used datasets, posing a significant obstacle in understanding the distinct contributions of each attribute to the model's performance. To address this challenge, we propose a generation-training-evaluation framework that leverages zero-shot text-to-speech systems to investigate the impact of controlled attribute variations on speech enhancement performance. It enables us to synthesize training datasets in a scalable manner while carefully altering each attribute. Based on the proposed framework, we analyze the scaling effects of various dataset attributes on the performance of both discriminative and generative SE models. Extensive experiments on multi-domain corpora imply that acoustic attributes (e.g., speaker and noise) are much more important to current speech enhancement models than semantic attributes (e.g., language and text), offering new insights for future research.\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = .5\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"multiple agents\", 1),\n",
        "        (\"Multiagent Systems\", 1),\n",
        "        (\"Multiagent\", 1),\n",
        "        (\"(cs.MA)\", 1),\n",
        "        (\"multi-agent and multi-rack path finding\", 1),  #  (MARPF)\n",
        "\n",
        "        (\"agent interactions\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdHdGAdC9U1a",
        "outputId": "1db88d76-86d7-4043-bbd9-057719219cd5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: multiple agents\n",
            "Total Matches in Set: 24\n",
            "Matches Above Score-Floor in Set: 24\n",
            "2024-12-20__041109950529\n",
            "\n",
            "Showing 24 in top-24 out of 24 total results.     -> 24 of 24/24\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 0.5) -> 0.5\n",
            "\n",
            "\n",
            "Document: 286|||| \n",
            "'arxiv_id': arXiv:2412.14779, \n",
            "'paper_link': https://arxiv.org/abs/2412.14779, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14779, \n",
            "Title: Agent-Temporal Credit Assignment for Optimal Policy Preservation in Sparse Multi-Agent Reinforcement Learning \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: In multi-agent environments, agents often struggle to learn optimal policies due to sparse or delayed global rewards, particularly in long-horizon tasks where it is challenging to evaluate actions at intermediate time steps. We introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach designed to address the agent-temporal credit assignment problem by redistributing sparse rewards both temporally and across agents. TAR$^2$ decomposes sparse global rewards into time-step-specific rewards and calculates agent-specific contributions to these rewards. We theoretically prove that TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$ stabilizes and accelerates the learning process. Additionally, we show that when TAR$^2$ is integrated with single-agent reinforcement learning algorithms, it performs as well as or better than traditional multi-agent reinforcement learning methods.\n",
            "Score: 3\n",
            "\n",
            "Document: 413|||| \n",
            "'arxiv_id': arXiv:2412.15163, \n",
            "'paper_link': https://arxiv.org/abs/2412.15163, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15163, \n",
            "Title: Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: Social norms are standards of behaviour common in a society. However, when agents make decisions without considering how others are impacted, norms can emerge that lead to the subjugation of certain agents. We present RAWL-E, a method to create ethical norm-learning agents. RAWL-E agents operationalise maximin, a fairness principle from Rawlsian ethics, in their decision-making processes to promote ethical norms by balancing societal well-being with individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios. We find that norms emerging in RAWL-E agent societies enhance social welfare, fairness, and robustness, and yield higher minimum experience compared to those that emerge in agent societies that do not implement Rawlsian ethics.\n",
            "Score: 3\n",
            "\n",
            "Document: 471|||| \n",
            "'arxiv_id': arXiv:2102.12461, \n",
            "'paper_link': https://arxiv.org/abs/2102.12461, \n",
            "'pdf_link': https://arxiv.org/pdf/2102.12461, \n",
            "Title: MAPFAST: A Deep Algorithm Selector for Multi Agent Path Finding using Shortest Path Embeddings \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: Solving the Multi-Agent Path Finding (MAPF) problem optimally is known to be NP-Hard for both make-span and total arrival time minimization. While many algorithms have been developed to solve MAPF problems, there is no dominating optimal MAPF algorithm that works well in all types of problems and no standard guidelines for when to use which algorithm. In this work, we develop the deep convolutional network MAPFAST (Multi-Agent Path Finding Algorithm SelecTor), which takes a MAPF problem instance and attempts to select the fastest algorithm to use from a portfolio of algorithms. We improve the performance of our model by including single-agent shortest paths in the instance embedding given to our model and by utilizing supplemental loss functions in addition to a classification loss. We evaluate our model on a large and diverse dataset of MAPF instances, showing that it outperforms all individual algorithms in its portfolio as well as the state-of-the-art optimal MAPF algorithm selector. We also provide an analysis of algorithm behavior in our dataset to gain a deeper understanding of optimal MAPF algorithms' strengths and weaknesses to help other researchers leverage different heuristics in algorithm designs.\n",
            "Score: 3\n",
            "\n",
            "Document: 542|||| \n",
            "'arxiv_id': arXiv:2405.04702, \n",
            "'paper_link': https://arxiv.org/abs/2405.04702, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.04702, \n",
            "Title: Mitigating Side Effects in Multi-Agent Systems Using Blame Assignment \n",
            "Subjects: Multiagent Systems (cs.MA) \n",
            "Abstract: When independently trained or designed robots are deployed in a shared environment, their combined actions can lead to unintended negative side effects (NSEs). To ensure safe and efficient operation, robots must optimize task performance while minimizing the penalties associated with NSEs, balancing individual objectives with collective impact. We model the problem of mitigating NSEs in a cooperative multi-agent system as a bi-objective lexicographic decentralized Markov decision process. We assume independence of transitions and rewards with respect to the robots' tasks, but the joint NSE penalty creates a form of dependence in this setting. To improve scalability, the joint NSE penalty is decomposed into individual penalties for each robot using credit assignment, which facilitates decentralized policy computation. We empirically demonstrate, using mobile robots and in simulation, the effectiveness and scalability of our approach in mitigating NSEs.\n",
            "Score: 3\n",
            "\n",
            "Document: 401|||| \n",
            "'arxiv_id': arXiv:2412.15135, \n",
            "'paper_link': https://arxiv.org/abs/2412.15135, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15135, \n",
            "Title: Probabilistic Strategy Logic with Degrees of Observability \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: There has been considerable work on reasoning about the strategic ability of agents under imperfect information. However, existing logics such as Probabilistic Strategy Logic are unable to express properties relating to information transparency. Information transparency concerns the extent to which agents' actions and behaviours are observable by other agents. Reasoning about information transparency is useful in many domains including security, privacy, and decision-making. In this paper, we present a formal framework for reasoning about information transparency properties in stochastic multi-agent systems. We extend Probabilistic Strategy Logic with new observability operators that capture the degree of observability of temporal properties by agents. We show that the model checking problem for the resulting logic is decidable.\n",
            "Score: 2\n",
            "\n",
            "Document: 614|||| \n",
            "'arxiv_id': arXiv:2409.08406, \n",
            "'paper_link': https://arxiv.org/abs/2409.08406, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.08406, \n",
            "Title: Knowledge Tagging with Large Language Model based Multi-Agent System \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Knowledge tagging for questions is vital in modern intelligent educational applications, including learning progress diagnosis, practice question recommendations, and course content organization. Traditionally, these annotations have been performed by pedagogical experts, as the task demands not only a deep semantic understanding of question stems and knowledge definitions but also a strong ability to link problem-solving logic with relevant knowledge concepts. With the advent of advanced natural language processing (NLP) algorithms, such as pre-trained language models and large language models (LLMs), pioneering studies have explored automating the knowledge tagging process using various machine learning models. In this paper, we investigate the use of a multi-agent system to address the limitations of previous algorithms, particularly in handling complex cases involving intricate knowledge definitions and strict numerical constraints. By demonstrating its superior performance on the publicly available math question knowledge tagging dataset, MathKnowCT, we highlight the significant potential of an LLM-based multi-agent system in overcoming the challenges that previous methods have encountered. Finally, through an in-depth discussion of the implications of automating knowledge tagging, we underscore the promising results of deploying LLM-based algorithms in educational contexts.\n",
            "Score: 2\n",
            "\n",
            "Document: 34|||| \n",
            "'arxiv_id': arXiv:2412.14218, \n",
            "'paper_link': https://arxiv.org/abs/2412.14218, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14218, \n",
            "Title: Heterogeneous Multi-Agent Reinforcement Learning for Distributed Channel Access in WLANs \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: This paper investigates the use of multi-agent reinforcement learning (MARL) to address distributed channel access in wireless local area networks. In particular, we consider the challenging yet more practical case where the agents heterogeneously adopt value-based or policy-based reinforcement learning algorithms to train the model. We propose a heterogeneous MARL training framework, named QPMIX, which adopts a centralized training with distributed execution paradigm to enable heterogeneous agents to collaborate. Moreover, we theoretically prove the convergence of the proposed heterogeneous MARL method when using the linear value function approximation. Our method maximizes the network throughput and ensures fairness among stations, therefore, enhancing the overall network performance. Simulation results demonstrate that the proposed QPMIX algorithm improves throughput, mean delay, delay jitter, and collision rates compared with conventional carrier-sense multiple access with collision avoidance in the saturated traffic scenario. Furthermore, the QPMIX is shown to be robust in unsaturated and delay-sensitive traffic scenarios, and promotes cooperation among heterogeneous agents.\n",
            "Score: 1\n",
            "\n",
            "Document: 37|||| \n",
            "'arxiv_id': arXiv:2412.14222, \n",
            "'paper_link': https://arxiv.org/abs/2412.14222, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14222, \n",
            "Title: A Survey on Large Language Model-based Agents for Statistics and Data Science \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.\n",
            "Score: 1\n",
            "\n",
            "Document: 182|||| \n",
            "'arxiv_id': arXiv:2412.14555, \n",
            "'paper_link': https://arxiv.org/abs/2412.14555, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14555, \n",
            "Title: Single-Loop Federated Actor-Critic across Heterogeneous Environments \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Federated reinforcement learning (FRL) has emerged as a promising paradigm, enabling multiple agents to collaborate and learn a shared policy adaptable across heterogeneous environments. Among the various reinforcement learning (RL) algorithms, the actor-critic (AC) algorithm stands out for its low variance and high sample efficiency. However, little to nothing is known theoretically about AC in a federated manner, especially each agent interacts with a potentially different environment. The lack of such results is attributed to various technical challenges: a two-level structure illustrating the coupling effect between the actor and the critic, heterogeneous environments, Markovian sampling and multiple local updates. In response, we study \\textit{Single-loop Federated Actor Critic} (SFAC) where agents perform actor-critic learning in a two-level federated manner while interacting with heterogeneous environments. We then provide bounds on the convergence error of SFAC. The results show that the convergence error asymptotically converges to a near-stationary point, with the extent proportional to environment heterogeneity. Moreover, the sample complexity exhibits a linear speed-up through the federation of agents. We evaluate the performance of SFAC through numerical experiments using common RL benchmarks, which demonstrate its effectiveness.\n",
            "Score: 1\n",
            "\n",
            "Document: 212|||| \n",
            "'arxiv_id': arXiv:2412.14613, \n",
            "'paper_link': https://arxiv.org/abs/2412.14613, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14613, \n",
            "Title: HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic Evaluation Using a Vision Language Model \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Vision-language models (VLMs) have shown impressive abilities in text and image understanding. However, existing metrics for evaluating the text generated by VLMs focus exclusively on overall quality, leading to two limitations: 1) it is challenging to identify which aspects of the text need improvement from the overall score; 2) metrics may overlook specific evaluation criteria when predicting an overall score. To address these limitations, we propose HarmonicEval, a reference-free evaluation metric that aggregates criterion-wise scores to produce the overall score in a bottom-up manner. Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE) dataset, which comprises 18,000 expert human judgments across four vision-language tasks. Our experiments demonstrate that HarmonicEval achieves higher correlations with human judgments than conventional metrics while providing numerical scores for each criterion.\n",
            "Score: 1\n",
            "\n",
            "Document: 250|||| \n",
            "'arxiv_id': arXiv:2412.14684, \n",
            "'paper_link': https://arxiv.org/abs/2412.14684, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14684, \n",
            "Title: Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user-defined requirements. Bel Esprit employs a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate the effectiveness of this framework in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline construction. Bel Esprit is available for a free trial at this https URL.\n",
            "Score: 1\n",
            "\n",
            "Document: 283|||| \n",
            "'arxiv_id': arXiv:2412.14769, \n",
            "'paper_link': https://arxiv.org/abs/2412.14769, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14769, \n",
            "Title: PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \\method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment.\n",
            "Score: 1\n",
            "\n",
            "Document: 443|||| \n",
            "'arxiv_id': arXiv:2412.14287, \n",
            "'paper_link': https://arxiv.org/abs/2412.14287, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14287, \n",
            "Title: Subset Selection Problems in Planar Point Sets \n",
            "Subjects: Combinatorics (math.CO) \n",
            "Abstract: Given a finite set satisfying condition $\\mathcal{A}$, the subset selection problem asks, how large of a subset satisfying condition $\\mathcal{B}$ can we find? We make progress on three instances of subset selection problems in planar point sets. Let $n,s\\in\\mathbb{N}$ with $n\\geq s$, and let $P\\subseteq\\mathbb{R}^2$ be a set of $n$ points, where at most $s$ points lie on the same line.\n",
            "Firstly, we select a general position subset of $P$, i.e., a subset containing no $3$ points on the same line. This problem was proposed by Erds under the regime when $s$ is a constant. For $s$ being non-constant, we give new lower and upper bounds on the maximum size of such a subset. In particular, we show that in the worst case such a set can have size at most $O(n/s)$ when $n^{1/3}\\leq s\\leq n$ and $O(n^{5/6+o(1)}/\\sqrt{s})$ when $3\\leq s\\leq n^{1/3}$.\n",
            "Secondly, we select a monotone general position subset of $P$, that is, a subset in general position where the points are ordered from left to right and their $y$-coordinates are either non-decreasing or non-increasing. We present bounds on the maximum size of such a subset. In particular, when $s=\\Theta(\\sqrt{n})$, our upper and lower bounds differ only by a logarithmic factor.\n",
            "Lastly, we select a subset of $P$ with pairwise distinct slopes. This problem was initially studied by Erds, Graham, Ruzsa, and Taylor on the grid. We show that for $s=O(\\sqrt{n})$ such a subset of size $\\Omega((n/\\log{s})^{1/3})$ can always be found in $P$. When $s=\\Theta(\\sqrt{n})$, this matches a lower bound given by Zhang on the grid. As for the upper bound, we show that in the worst case such a subset has size at most $O(\\sqrt{n})$ for $2\\leq s\\leq n^{3/8}$ and $O((n/s)^{4/5})$ for $n^{3/8}\\leq s=O(\\sqrt{n})$.\n",
            "The proofs use a wide range of tools such as incidence geometry, probabilistic methods, the hypergraph container method, and additive combinatorics.\n",
            "Score: 1\n",
            "\n",
            "Document: 453|||| \n",
            "'arxiv_id': arXiv:2412.14641, \n",
            "'paper_link': https://arxiv.org/abs/2412.14641, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14641, \n",
            "Title: Some permutation pentanomials over finite fields of even characteristic \n",
            "Subjects: Combinatorics (math.CO) \n",
            "Abstract: In a recent paper Zhang et al. constructed 17 families of permutation pentanomials of the form $x^t+x^{r_1(q-1)+t}+x^{r_2(q-1)+t}+x^{r_3(q-1)+t}+x^{r_4(q-1)+t}$ over $\\mathbb{F}_{q^2}$ where $q=2^m$. In this paper for 14 of these 17 families we provide a simple explanation as to why they are permutations. We also extend these 14 families into three general classes of permutation pentanomials over $\\mathbb{F}_{q^2}$.\n",
            "Score: 1\n",
            "\n",
            "Document: 461|||| \n",
            "'arxiv_id': arXiv:2412.14784, \n",
            "'paper_link': https://arxiv.org/abs/2412.14784, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14784, \n",
            "Title: Large Induced Subgraphs of Bounded Degree in Planar Graphs and Beyond \n",
            "Subjects: Combinatorics (math.CO) \n",
            "Abstract: In this paper, we introduce and study the following question. Let $\\mathcal G$ be a family of graphs and let $k\\geq 3$ be an integer. What is the largest value $f_k(n)$ such that every $n$-vertex graph in $\\mathcal G$ has an induced subgraph with degree at most $k$ and with $f_k(n)$ vertices? Similar questions, in which one seeks a large induced forest, or a large induced linear forest, or a large induced $d$-degenerate graph, rather than a large induced graph of bounded degree, have been studied for decades and have given rise to some of the most fascinating and elusive conjectures in Graph Theory. We tackle our problem when $\\mathcal G$ is the class of the outerplanar graphs, or the class of the planar graphs, or the class of the graphs whose degree is bounded by a value $d>k$. In all cases, we provide upper and lower bounds on the value of $f_k(n)$. For example, we prove that every $n$-vertex planar graph has an induced subgraph with degree at most $3$ and with $\\frac{5n}{13}>0.384n$ vertices, and that there exist $n$-vertex planar graphs whose largest induced subgraph with degree at most $3$ has $\\frac{4n}{7}+O(1)<0.572n+O(1)$ vertices.\n",
            "Score: 1\n",
            "\n",
            "Document: 462|||| \n",
            "'arxiv_id': arXiv:2412.14845, \n",
            "'paper_link': https://arxiv.org/abs/2412.14845, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14845, \n",
            "Title: Asymptotically Enumerating Independent Sets in Regular $k$-Partite $k$-Uniform Hypergraphs \n",
            "Subjects: Combinatorics (math.CO) \n",
            "Abstract: The number of independent sets in regular bipartite expander graphs can be efficiently approximated by expressing it as the partition function of a suitable polymer model and truncating its cluster expansion. While this approach has been extensively used for graphs, surprisingly little is known about analogous questions in the context of hypergraphs. In this work, we apply this method to asymptotically determine the number of independent sets in regular $k$-partite $k$-uniform hypergraphs which satisfy natural expansion properties. The resulting formula depends only on the local structure of the hypergraph, making it computationally efficient. In particular, we provide a simple closed-form expression for linear hypergraphs.\n",
            "Score: 1\n",
            "\n",
            "Document: 521|||| \n",
            "'arxiv_id': arXiv:2403.05974, \n",
            "'paper_link': https://arxiv.org/abs/2403.05974, \n",
            "'pdf_link': https://arxiv.org/pdf/2403.05974, \n",
            "Title: Deep Reinforcement Learning Enhanced Rate-Splitting Multiple Access for Interference Mitigation \n",
            "Subjects: Information Theory (cs.IT) \n",
            "Abstract: This study explores the application of the rate-splitting multiple access (RSMA) technique, vital for interference mitigation in modern communication systems. It investigates the use of precoding methods in RSMA, especially in complex multiple-antenna interference channels, employing deep reinforcement learning. The aim is to optimize precoders and power allocation for common and private data streams involving multiple decision-makers. A multi-agent deep deterministic policy gradient (MADDPG) framework is employed to address this complexity, where decentralized agents collectively learn to optimize actions in a continuous policy space. We also explore the challenges posed by imperfect channel side information at the transmitter. Additionally, decoding order estimation is addressed to determine the optimal decoding sequence for common and private data sequences. Simulation results demonstrate the effectiveness of the proposed RSMA method based on MADDPG, achieving the upper bound in single-antenna scenarios and closely approaching theoretical limits in multi-antenna scenarios. Comparative analysis shows superiority over other techniques such as MADDPG without rate-splitting, maximal ratio transmission (MRT), zero-forcing (ZF), and leakage-based precoding methods. These findings highlight the potential of deep reinforcement learning-driven RSMA in reducing interference and enhancing system performance in communication systems.\n",
            "Score: 1\n",
            "\n",
            "Document: 547|||| \n",
            "'arxiv_id': arXiv:2405.14573, \n",
            "'paper_link': https://arxiv.org/abs/2405.14573, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.14573, \n",
            "Title: AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: Autonomous agents that execute human tasks by controlling computers can enhance human productivity and application accessibility. However, progress in this field will be driven by realistic and reproducible benchmarks. We present AndroidWorld, a fully functional Android environment that provides reward signals for 116 programmatic tasks across 20 real-world Android apps. Unlike existing interactive environments, which provide a static test set, AndroidWorld dynamically constructs tasks that are parameterized and expressed in natural language in unlimited ways, thus enabling testing on a much larger and more realistic suite of tasks. To ensure reproducibility, each task includes dedicated initialization, success-checking, and tear-down logic, which modifies and inspects the device's system state. We experiment with baseline agents to test AndroidWorld and provide initial results on the benchmark. Our best agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for future work. Furthermore, we adapt a popular desktop web agent to work on Android, which we find to be less effective on mobile, suggesting future research is needed to achieve universal, cross-platform agents. Finally, we also conduct a robustness analysis, showing that task variations can significantly affect agent performance, demonstrating that without such testing, agent performance metrics may not fully reflect practical challenges. AndroidWorld and the experiments in this paper are available at this http URL.\n",
            "Score: 1\n",
            "\n",
            "Document: 577|||| \n",
            "'arxiv_id': arXiv:2407.06968, \n",
            "'paper_link': https://arxiv.org/abs/2407.06968, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.06968, \n",
            "Title: An automata-based approach for synchronizable mailbox communication \n",
            "Subjects: Logic in Computer Science (cs.LO) \n",
            "Abstract: We revisit finite-state communicating systems with round-based communication under mailbox semantics. Mailboxes correspond to one FIFO buffer per process (instead of one buffer per pair of processes in peer-to-peer systems). Round-based communication corresponds to sequences of rounds in which processes can first send messages, then only receive (and receives must be in the same round as their sends). A system is called synchronizable if every execution can be re-scheduled into an equivalent execution that is a sequence of rounds. Previous work mostly considered the setting where rounds have fixed size. Our main contribution shows that the problem whether a mailbox communication system complies with the round-based policy, with no size limitation on rounds, is Pspace-complete. For this we use a novel automata-based approach, that also allows to determine the precise complexity (Pspace) of several questions considered in previous literature.\n",
            "Score: 1\n",
            "\n",
            "Document: 583|||| \n",
            "'arxiv_id': arXiv:2407.18551, \n",
            "'paper_link': https://arxiv.org/abs/2407.18551, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.18551, \n",
            "Title: Multi-Agent Trajectory Prediction with Difficulty-Guided Feature Enhancement Network \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Trajectory prediction is crucial for autonomous driving as it aims to forecast the future movements of traffic participants. Traditional methods usually perform holistic inference on the trajectories of agents, neglecting the differences in prediction difficulty among agents. This paper proposes a novel Difficulty-Guided Feature Enhancement Network (DGFNet), which leverages the prediction difficulty differences among agents for multi-agent trajectory prediction. Firstly, we employ spatio-temporal feature encoding and interaction to capture rich spatio-temporal features. Secondly, a difficulty-guided decoder controls the flow of future trajectories into subsequent modules, obtaining reliable future trajectories. Then, feature interaction and fusion are performed through the future feature interaction module. Finally, the fused agent features are fed into the final predictor to generate the predicted trajectory distributions for multiple participants. Experimental results demonstrate that our DGFNet achieves state-of-the-art performance on the Argoverse 1\\&2 motion forecasting benchmarks. Ablation studies further validate the effectiveness of each module. Moreover, compared with SOTA methods, our method balances trajectory prediction accuracy and real-time inference speed.\n",
            "Score: 1\n",
            "\n",
            "Document: 585|||| \n",
            "'arxiv_id': arXiv:2407.20041, \n",
            "'paper_link': https://arxiv.org/abs/2407.20041, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.20041, \n",
            "Title: Counterfactual rewards promote collective transport using individually controlled swarm microrobots \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Swarm robots offer fascinating opportunities to perform complex tasks beyond the capabilities of individual machines. Just as a swarm of ants collectively moves a large object, similar functions can emerge within a group of robots through individual strategies based on local sensing. However, realizing collective functions with individually controlled microrobots is particularly challenging due to their micrometer size, large number of degrees of freedom, strong thermal noise relative to the propulsion speed, complex physical coupling between neighboring microrobots, and surface collisions. Here, we implement Multi-Agent Reinforcement Learning (MARL) to generate a control strategy for up to 200 microrobots whose motions are individually controlled by laser spots. During the learning process, we employ so-called counterfactual rewards that automatically assign credit to the individual microrobots, which allows for fast and unbiased training. With the help of this efficient reward scheme, swarm microrobots learn to collectively transport a large cargo object to an arbitrary position and orientation, similar to ant swarms. We demonstrate that this flexible and versatile swarm robotic system is robust to variations in group size, the presence of malfunctioning units, and environmental noise. Such control strategies can potentially enable complex and automated assembly of mobile micromachines, programmable drug delivery capsules, and other advanced lab-on-a-chip applications.\n",
            "Score: 1\n",
            "\n",
            "Document: 700|||| \n",
            "'arxiv_id': arXiv:2412.10743, \n",
            "'paper_link': https://arxiv.org/abs/2412.10743, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.10743, \n",
            "Title: NeuralPLexer3: Accurate Biomolecular Complex Structure Prediction with Flow Models \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Structure determination is essential to a mechanistic understanding of diseases and the development of novel therapeutics. Machine-learning-based structure prediction methods have made significant advancements by computationally predicting protein and bioassembly structures from sequences and molecular topology alone. Despite substantial progress in the field, challenges remain to deliver structure prediction models to real-world drug discovery. Here, we present NeuralPLexer3 -- a physics-inspired flow-based generative model that achieves state-of-the-art prediction accuracy on key biomolecular interaction types and improves training and sampling efficiency compared to its predecessors and alternative methodologies. Examined through newly developed benchmarking strategies, NeuralPLexer3 excels in vital areas that are crucial to structure-based drug design, such as physical validity and ligand-induced conformational changes.\n",
            "Score: 1\n",
            "\n",
            "Document: 727|||| \n",
            "'arxiv_id': arXiv:2412.13693, \n",
            "'paper_link': https://arxiv.org/abs/2412.13693, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.13693, \n",
            "Title: A2H: A UI Converter from Android to HarmonyOS Platform \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: With the growing importance of smartphones, developers face the challenge of creating separate applications for multiple platforms (e.g., Android, iOS, and HarmonyOS), leading to increased development costs and longer iteration cycles. One potential solution is to develop an app on one platform and then automatically convert it to other platforms, reducing the need for separate development efforts. However, migrating user interfaces (UIs) between platforms is particularly challenging due to significant differences in layout structures and development paradigms, such as the disparity between XML layout files in Android and ArkUI framework in HarmonyOS. Manual conversion of UIs is time-consuming, error-prone, and inefficient, necessitating an automated solution to streamline the process and enable seamless migration from Android to HarmonyOS. To address this challenge, we propose the A2H Converter, an automated tool for migrating Android UIs to HarmonyOS. The tool employs an large language model (LLM)-driven multi-agent framework to convert Android XML layouts into HarmonyOS ArkUI layouts. Using the RAG combing with decision rules, the system maps Android UI components to ArkUI equivalents, while a reflective mechanism continuously improves conversion accuracy. A2H Converter handles project-level layouts, ensuring consistency across multiple files and addressing complex UI logic. Experiments on six Android applications collected from GitHub demonstrate that our A2H Converter achieves a migration success rate of over 90.1%, 89.3%, and 89.2% at the component, page, and project levels, respectively. The demo video is available at. The tool is available at this http URL.\n",
            "Score: 1\n",
            "\n",
            "Document: 743|||| \n",
            "'arxiv_id': arXiv:2201.09399, \n",
            "'paper_link': https://arxiv.org/abs/2201.09399, \n",
            "'pdf_link': https://arxiv.org/pdf/2201.09399, \n",
            "Title: Fault-tolerant Locating-Dominating Sets on the Infinite King Grid \n",
            "Subjects: Combinatorics (math.CO) \n",
            "Abstract: Let $G$ be a graph of a network system with vertices, $V(G)$, representing physical locations and edges, $E(G)$, representing informational connectivity. A \\emph{locating-dominating (LD)} set $S \\subseteq V(G)$ is a subset of vertices representing detectors capable of sensing an \"intruder\" at precisely their location or somewhere in their open-neighborhood -- an LD set must be capable of locating an intruder anywhere in the graph. We explore three types of fault-tolerant LD sets: \\emph{redundant LD} sets, which allow a detector to be removed, \\emph{error-detecting LD} sets, which allow at most one false negative, and \\emph{error-correcting LD} sets, which allow at most one error (false positive or negative). In particular, we determine lower and upper bounds for the minimum density of these three fault-tolerant locating-dominating sets in the \\emph{infinite king grid}.\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = .5\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Agents for Software Engineering\", .5),\n",
        "        (\"ai writing code\", .5),\n",
        "        (\"coding done by ai\", .5),\n",
        "        (\"AI-Generated Code\", .5),\n",
        "        (\"Generated Code\", .5),\n",
        "        (\"code generation\", .5),\n",
        "        (\"ai code writing\", .5),\n",
        "        (\"solutions to produce computer code\", .5),\n",
        "        (\"Generated Code\", .5),\n",
        "\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w24GXHdtcVgr",
        "outputId": "01b72714-7bf4-441d-866d-2888517fdec0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Agents for Software Engineering\n",
            "Total Matches in Set: 13\n",
            "Matches Above Score-Floor in Set: 13\n",
            "2024-12-20__041110059505\n",
            "\n",
            "Showing 13 in top-13 out of 13 total results.     -> 13 of 13/13\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 0.5) -> 0.5\n",
            "\n",
            "\n",
            "Document: 210|||| \n",
            "'arxiv_id': arXiv:2412.14611, \n",
            "'paper_link': https://arxiv.org/abs/2412.14611, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14611, \n",
            "Title: Is This You, LLM? Recognizing AI-written Programs with Multilingual Code Stylometry \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: With the increasing popularity of LLM-based code completers, like GitHub Copilot, the interest in automatically detecting AI-generated code is also increasing-in particular in contexts where the use of LLMs to program is forbidden by policy due to security, intellectual property, or ethical this http URL introduce a novel technique for AI code stylometry, i.e., the ability to distinguish code generated by LLMs from code written by humans, based on a transformer-based encoder classifier. Differently from previous work, our classifier is capable of detecting AI-written code across 10 different programming languages with a single machine learning model, maintaining high average accuracy across all languages (84.1% $\\pm$ 3.8%).Together with the classifier we also release H-AIRosettaMP, a novel open dataset for AI code stylometry tasks, consisting of 121 247 code snippets in 10 popular programming languages, labeled as either human-written or AI-generated. The experimental pipeline (dataset, training code, resulting models) is the first fully reproducible one for the AI code stylometry task. Most notably our experiments rely only on open LLMs, rather than on proprietary/closed ones like ChatGPT.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 310|||| \n",
            "'arxiv_id': arXiv:2412.14841, \n",
            "'paper_link': https://arxiv.org/abs/2412.14841, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14841, \n",
            "Title: Helping LLMs Improve Code Generation Using Feedback from Testing and Static Analysis \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Large Language Models (LLMs) are one of the most promising developments in the field of artificial intelligence, and the software engineering community has readily noticed their potential role in the software development life-cycle. Developers routinely ask LLMs to generate code snippets, increasing productivity but also potentially introducing ownership, privacy, correctness, and security issues. Previous work highlighted how code generated by mainstream commercial LLMs is often not safe, containing vulnerabilities, bugs, and code smells. In this paper, we present a framework that leverages testing and static analysis to assess the quality, and guide the self-improvement, of code generated by general-purpose, open-source LLMs.\n",
            "First, we ask LLMs to generate C code to solve a number of programming tasks. Then we employ ground-truth tests to assess the (in)correctness of the generated code, and a static analysis tool to detect potential safety vulnerabilities. Next, we assess the models ability to evaluate the generated code, by asking them to detect errors and vulnerabilities. Finally, we test the models ability to fix the generated code, providing the reports produced during the static analysis and incorrectness evaluation phases as feedback.\n",
            "Our results show that models often produce incorrect code, and that the generated code can include safety issues. Moreover, they perform very poorly at detecting either issue. On the positive side, we observe a substantial ability to fix flawed code when provided with information about failed tests or potential vulnerabilities, indicating a promising avenue for improving the safety of LLM-based code generation tools.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 495|||| \n",
            "'arxiv_id': arXiv:2309.16120, \n",
            "'paper_link': https://arxiv.org/abs/2309.16120, \n",
            "'pdf_link': https://arxiv.org/pdf/2309.16120, \n",
            "Title: Fixing Large Language Models' Specification Misunderstanding for Better Code Generation \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Code generation is to automatically generate source code conforming to a given programming specification, which has received extensive attention especially with the development of large language models (LLMs). Due to the inherent difficulty of code generation, the code generated by LLMs may not be aligned with the specification. Although thought-eliciting prompting techniques have been proposed to enhance the code generation performance of LLMs, producing correct understanding for complicated programming problems remains challenging, resulting in unsatisfactory performance. Also, some feedback-based prompting techniques have been proposed to fix incorrect code using error messages produced by test execution. However, when the generated code deviates significantly from the ground truth, they encounter difficulties in improving performance based on such coarse-grained information. In this work, we propose a novel prompting technique, called {\\mu}FiX, to improve the code generation performance of LLMs by devising both sophisticated thought-eliciting prompting and feedback-based prompting and making the first exploration on their synergy. It first exploits test case analysis to obtain specification understanding and enables a self-improvement process to identify and refine the misunderstanding in the thought-eliciting prompting phase. {\\mu}FiX further fixes the specification understanding towards the direction reducing the gap between the provided understanding (from the first phase) and the actual understanding implicitly utilized by LLMs for code generation in the feedback-based prompting phase. By improving the understanding with {\\mu}FiX, the code generation performance of LLMs can be largely improved. Our evaluation on two advanced LLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing with 15 baselines, demonstrates the effectiveness of {\\mu}FiX ...\n",
            "Score: 1.5\n",
            "\n",
            "Document: 553|||| \n",
            "'arxiv_id': arXiv:2406.00215, \n",
            "'paper_link': https://arxiv.org/abs/2406.00215, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.00215, \n",
            "Title: Benchmarking the Communication Competence of Code Generation for LLMs and LLM Agent \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Large language models (LLMs) have significantly improved their ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that top-level software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, we argue that the same should be applied to LLMs for code generation tasks.\n",
            "In this work, we conducted an empirical study on the benchmark and analysis of the communication skills of LLMs for code generation. We define communication skills of LLMs as ``being able to ask clarifying questions when the description of the code generation problem has issues''. We created a new benchmark, HumanEvalComm, by modifying problem descriptions according to three issues: inconsistency, ambiguity, incompleteness. We defined new evaluation metrics such as Communication Rate and Good Question Rate, and then experimented on HumanEvalComm with different Code LLMs, and a new LLM agent approach, Okanagan, to identify and ask questions in ambiguous parts from code and descriptions for further refining the generated code. Finally, we discussed evaluation results by comparing Code LLMs and Okanagan with our findings.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 636|||| \n",
            "'arxiv_id': arXiv:2410.14748, \n",
            "'paper_link': https://arxiv.org/abs/2410.14748, \n",
            "'pdf_link': https://arxiv.org/pdf/2410.14748, \n",
            "Title: ETF: An Entity Tracing Framework for Hallucination Detection in Code Summaries \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their ability to understand both natural language and code, driving their use in tasks like natural language-to-code (NL2Code) and code summarization. However, LLMs are prone to hallucination-outputs that stray from intended meanings. Detecting hallucinations in code summarization is especially difficult due to the complex interplay between programming and natural languages. We introduce a first-of-its-kind dataset with $\\sim$10K samples, curated specifically for hallucination detection in code summarization. We further propose a novel Entity Tracing Framework (ETF) that a) utilizes static program analysis to identify code entities from the program and b) uses LLMs to map and verify these entities and their intents within generated code summaries. Our experimental analysis demonstrates the effectiveness of the framework, leading to a 0.73 F1 score. This approach provides an interpretable method for detecting hallucinations by grounding entities, allowing us to evaluate summary accuracy.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 30|||| \n",
            "'arxiv_id': arXiv:2412.14212, \n",
            "'paper_link': https://arxiv.org/abs/2412.14212, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14212, \n",
            "Title: Tree-of-Code: A Hybrid Approach for Robust Complex Task Planning and Execution \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: The exceptional capabilities of large language models (LLMs) have substantially accelerated the rapid rise and widespread adoption of agents. Recent studies have demonstrated that generating Python code to consolidate LLM-based agents' actions into a unified action space (CodeAct) is a promising approach for developing real-world LLM agents. However, this step-by-step code generation approach often lacks consistency and robustness, leading to instability in agent applications, particularly for complex reasoning and out-of-domain tasks. In this paper, we propose a novel approach called Tree-of-Code (ToC) to tackle the challenges of complex problem planning and execution with an end-to-end mechanism. By integrating key ideas from both Tree-of-Thought and CodeAct, ToC combines their strengths to enhance solution exploration. In our framework, each final code execution result is treated as a node in the decision tree, with a breadth-first search strategy employed to explore potential solutions. The final outcome is determined through a voting mechanism based on the outputs of the nodes.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 45|||| \n",
            "'arxiv_id': arXiv:2412.14234, \n",
            "'paper_link': https://arxiv.org/abs/2412.14234, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14234, \n",
            "Title: Syzygy: Dual Code-Test C to (safe) Rust Translation using LLMs and Dynamic Analysis \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Despite extensive usage in high-performance, low-level systems programming applications, C is susceptible to vulnerabilities due to manual memory management and unsafe pointer operations. Rust, a modern systems programming language, offers a compelling alternative. Its unique ownership model and type system ensure memory safety without sacrificing performance.\n",
            "In this paper, we present Syzygy, an automated approach to translate C to safe Rust. Our technique uses a synergistic combination of LLM-driven code and test translation guided by dynamic-analysis-generated execution information. This paired translation runs incrementally in a loop over the program in dependency order of the code elements while maintaining per-step correctness. Our approach exposes novel insights on combining the strengths of LLMs and dynamic analysis in the context of scaling and combining code generation with testing. We apply our approach to successfully translate Zopfli, a high-performance compression library with ~3000 lines of code and 98 functions. We validate the translation by testing equivalence with the source C program on a set of inputs. To our knowledge, this is the largest automated and test-validated C to safe Rust code translation achieved so far.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 132|||| \n",
            "'arxiv_id': arXiv:2412.14471, \n",
            "'paper_link': https://arxiv.org/abs/2412.14471, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14471, \n",
            "Title: Why We Build Local Large Language Models: An Observational Analysis from 35 Japanese and Multilingual LLMs \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Why do we build local large language models (LLMs)? What should a local LLM learn from the target language? Which abilities can be transferred from other languages? Do language-specific scaling laws exist? To explore these research questions, we evaluated 35 Japanese, English, and multilingual LLMs on 19 evaluation benchmarks for Japanese and English, taking Japanese as a local language. Adopting an observational approach, we analyzed correlations of benchmark scores, and conducted principal component analysis (PCA) on the scores to derive \\textit{ability factors} of local LLMs. We found that training on English text can improve the scores of academic subjects in Japanese (JMMLU). In addition, it is unnecessary to specifically train on Japanese text to enhance abilities for solving Japanese code generation, arithmetic reasoning, commonsense, and reading comprehension tasks. In contrast, training on Japanese text could improve question-answering tasks about Japanese knowledge and English-Japanese translation, which indicates that abilities for solving these two tasks can be regarded as \\textit{Japanese abilities} for LLMs. Furthermore, we confirmed that the Japanese abilities scale with the computational budget for Japanese text.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 181|||| \n",
            "'arxiv_id': arXiv:2412.14554, \n",
            "'paper_link': https://arxiv.org/abs/2412.14554, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14554, \n",
            "Title: The Current Challenges of Software Engineering in the Era of Large Language Models \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: With the advent of large language models (LLMs) in the artificial intelligence (AI) area, the field of software engineering (SE) has also witnessed a paradigm shift. These models, by leveraging the power of deep learning and massive amounts of data, have demonstrated an unprecedented capacity to understand, generate, and operate programming languages. They can assist developers in completing a broad spectrum of software development activities, encompassing software design, automated programming, and maintenance, which potentially reduces huge human efforts. Integrating LLMs within the SE landscape (LLM4SE) has become a burgeoning trend, necessitating exploring this emergent landscape's challenges and opportunities.\n",
            "The paper aims at revisiting the software development life cycle (SDLC) under LLMs, and highlighting challenges and opportunities of the new paradigm. The paper first summarizes the overall process of LLM4SE, and then elaborates on the current challenges based on a through discussion. The discussion was held among more than 20 participants from academia and industry, specializing in fields such as software engineering and artificial intelligence. Specifically, we achieve 26 key challenges from seven aspects, including software requirement & design, coding assistance, testing code generation, code review, code maintenance, software vulnerability management, and data, training, and evaluation. We hope the achieved challenges would benefit future research in the LLM4SE field.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 393|||| \n",
            "'arxiv_id': arXiv:2412.15118, \n",
            "'paper_link': https://arxiv.org/abs/2412.15118, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15118, \n",
            "Title: Outcome-Refining Process Supervision for Code Generation \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Large Language Models have demonstrated remarkable capabilities in code generation, yet they often struggle with complex programming tasks that require deep algorithmic reasoning. While process supervision through learned reward models shows promise in guiding reasoning steps, it requires expensive training data and suffers from unreliable evaluation. We propose Outcome-Refining Process Supervision, a novel paradigm that treats outcome refinement itself as the process to be supervised. Our framework leverages concrete execution signals to ground the supervision of reasoning steps, while using tree-structured exploration to maintain multiple solution trajectories simultaneously. Experiments demonstrate that our approach enables even smaller models to achieve high success accuracy and performance metrics on competitive programming tasks, creates more reliable verification than traditional reward models without requiring training PRMs. Our approach achieves significant improvements across 5 models and 3 datasets: an average of 26.9% increase in correctness and 42.2% in efficiency. The results suggest that providing structured reasoning space with concrete verification signals is crucial for solving complex programming tasks. We open-source all our code and data at: this https URL\n",
            "Score: 0.5\n",
            "\n",
            "Document: 418|||| \n",
            "'arxiv_id': arXiv:2412.15178, \n",
            "'paper_link': https://arxiv.org/abs/2412.15178, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15178, \n",
            "Title: HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages \n",
            "Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) \n",
            "Abstract: Large Language Model (LLM) based coding tools have been tremendously successful as software development assistants, yet they are often designed for general purpose programming tasks and perform poorly for more specialized domains such as high performance computing. Creating specialized models and tools for these domains is crucial towards gaining the benefits of LLMs in areas such as HPC. While previous work has explored HPC-specific models, LLMs still struggle to generate parallel code and it is not at all clear what hurdles are still holding back these LLMs and what must be done to overcome them. In this work, we conduct an in-depth study along the many axes of fine-tuning a specialized HPC LLM in order to better understand the challenges. Based on our findings we fine-tune and evaluate a specialized HPC LLM that is shown to be the best performing open-source code LLM for parallel code generation to date.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 652|||| \n",
            "'arxiv_id': arXiv:2411.05199, \n",
            "'paper_link': https://arxiv.org/abs/2411.05199, \n",
            "'pdf_link': https://arxiv.org/pdf/2411.05199, \n",
            "Title: CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Large Language Models (LLMs) have revolutionized code generation but require significant resources and often over-generalize, limiting their task-specific efficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective alternative. However, standard supervised approaches rely only on correct examples, missing valuable insights from failures. We introduce CodeLutra, a framework that leverages both correct and incorrect code attempts. Instead of using only correct solutions, CodeLutra applies iterative preference-based refinement, comparing successful and failed outputs to better approximate desired results. This approach narrows the performance gap with state-of-the-art larger models without requiring massive datasets or auxiliary models. For instance, on a challenging data science coding task, using only 500 samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's level. By learning from both successes and mistakes, CodeLutra provides a scalable and efficient path to high-quality code generation, making smaller open-source models more competitive with leading closed-source alternatives.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 657|||| \n",
            "'arxiv_id': arXiv:2411.11532, \n",
            "'paper_link': https://arxiv.org/abs/2411.11532, \n",
            "'pdf_link': https://arxiv.org/pdf/2411.11532, \n",
            "Title: CKGFuzzer: LLM-Based Fuzz Driver Generation Enhanced By Code Knowledge Graph \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: In recent years, the programming capabilities of large language models (LLMs) have garnered significant attention. Fuzz testing, a highly effective technique, plays a key role in enhancing software reliability and detecting vulnerabilities. However, traditional fuzz testing tools rely on manually crafted fuzz drivers, which can limit both testing efficiency and effectiveness. To address this challenge, we propose an automated fuzz testing method driven by a code knowledge graph and powered by an LLM-based intelligent agent system, referred to as CKGFuzzer. We approach fuzz driver creation as a code generation task, leveraging the knowledge graph of the code repository to automate the generation process within the fuzzing loop, while continuously refining both the fuzz driver and input seeds. The code knowledge graph is constructed through interprocedural program analysis, where each node in the graph represents a code entity, such as a function or a file. The knowledge graph-enhanced CKGFuzzer not only effectively resolves compilation errors in fuzz drivers and generates input seeds tailored to specific API usage scenarios, but also analyzes fuzz driver crash reports, assisting developers in improving code quality. By querying the knowledge graph of the code repository and learning from API usage scenarios, we can better identify testing targets and understand the specific purpose of each fuzz driver. We evaluated our approach using eight open-source software projects. The experimental results indicate that CKGFuzzer achieved an average improvement of 8.73% in code coverage compared to state-of-the-art techniques. Additionally, CKGFuzzer reduced the manual review workload in crash case analysis by 84.4% and successfully detected 11 real bugs (including nine previously unreported bugs) across the tested libraries.\n",
            "Score: 0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = .5\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"e-Learners\", 1),\n",
        "        (\"educational content\", 1),\n",
        "        (\"learning styles\", 1),\n",
        "        (\"educational process\", 1),\n",
        "        (\"human learning\", 1),\n",
        "\n",
        "        (\"education\", .5),\n",
        "        (\"learner\", .5),\n",
        "        (\"individual needs\", .5),\n",
        "\n",
        "        (\"learning sciences\", .5),\n",
        "        (\"educational technology\", .5),\n",
        "        (\"human-computer interaction\", .5),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ1iMqUJt8l9",
        "outputId": "a8bf4a07-535a-4e49-c8e4-7d534c4fb2ed"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: e-Learners\n",
            "Total Matches in Set: 52\n",
            "Matches Above Score-Floor in Set: 52\n",
            "2024-12-20__041110164466\n",
            "\n",
            "Showing 52 in top-45 out of 52 total results.     -> 52 of 45/52\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 0.5) -> 0.5\n",
            "\n",
            "\n",
            "Document: 17|||| \n",
            "'arxiv_id': arXiv:2412.14195, \n",
            "'paper_link': https://arxiv.org/abs/2412.14195, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14195, \n",
            "Title: IMPROVE: Impact of Mobile Phones on Remote Online Virtual Education \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: This work presents the IMPROVE dataset, designed to evaluate the effects of mobile phone usage on learners during online education. The dataset not only assesses academic performance and subjective learner feedback but also captures biometric, behavioral, and physiological signals, providing a comprehensive analysis of the impact of mobile phone use on learning. Multimodal data were collected from 120 learners in three groups with different phone interaction levels. A setup involving 16 sensors was implemented to collect data that have proven to be effective indicators for understanding learner behavior and cognition, including electroencephalography waves, videos, eye tracker, etc. The dataset includes metadata from the processed videos like face bounding boxes, facial landmarks, and Euler angles for head pose estimation. In addition, learner performance data and self-reported forms are included. Phone usage events were labeled, covering both supervisor-triggered and uncontrolled events. A semi-manual re-labeling system, using head pose and eye tracker data, is proposed to improve labeling accuracy. Technical validation confirmed signal quality, with statistical analyses revealing biometric changes during phone use.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 276|||| \n",
            "'arxiv_id': arXiv:2412.14744, \n",
            "'paper_link': https://arxiv.org/abs/2412.14744, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14744, \n",
            "Title: A parametric algorithm is optimal for non-parametric regression of smooth functions \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: We address the regression problem for a general function $f:[-1,1]^d\\to \\mathbb R$ when the learner selects the training points $\\{x_i\\}_{i=1}^n$ to achieve a uniform error bound across the entire domain. In this setting, known historically as nonparametric regression, we aim to establish a sample complexity bound that depends solely on the function's degree of smoothness. Assuming periodicity at the domain boundaries, we introduce PADUA, an algorithm that, with high probability, provides performance guarantees optimal up to constant or logarithmic factors across all problem parameters. Notably, PADUA is the first parametric algorithm with optimal sample complexity for this setting. Due to this feature, we prove that, differently from the non-parametric state of the art, PADUA enjoys optimal space complexity in the prediction phase. To validate these results, we perform numerical experiments over functions coming from real audio data, where PADUA shows comparable performance to state-of-the-art methods, while requiring only a fraction of the computational time.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 389|||| \n",
            "'arxiv_id': arXiv:2412.15109, \n",
            "'paper_link': https://arxiv.org/abs/2412.15109, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15109, \n",
            "Title: Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Current efforts to learn scalable policies in robotic manipulation primarily fall into two categories: one focuses on \"action,\" which involves behavior cloning from extensive collections of robotic data, while the other emphasizes \"vision,\" enhancing model generalization by pre-training representations or generative models, also referred to as world models, using large-scale visual datasets. This paper presents an end-to-end paradigm that predicts actions using inverse dynamics models conditioned on the robot's forecasted visual states, named Predictive Inverse Dynamics Models (PIDM). By closing the loop between vision and action, the end-to-end PIDM can be a better scalable action learner. In practice, we use Transformers to process both visual states and actions, naming the model Seer. It is initially pre-trained on large-scale robotic datasets, such as DROID, and can be adapted to realworld scenarios with a little fine-tuning data. Thanks to large-scale, end-to-end training and the synergy between vision and action, Seer significantly outperforms previous methods across both simulation and real-world experiments. It achieves improvements of 13% on the LIBERO-LONG benchmark, 21% on CALVIN ABC-D, and 43% in real-world tasks. Notably, Seer sets a new state-of-the-art on CALVIN ABC-D benchmark, achieving an average length of 4.28, and exhibits superior generalization for novel objects, lighting conditions, and environments under high-intensity disturbances on real-world scenarios. Code and models are publicly available at this https URL.\n",
            "Score: 1.5\n",
            "\n",
            "Document: 6|||| \n",
            "'arxiv_id': arXiv:2412.14180, \n",
            "'paper_link': https://arxiv.org/abs/2412.14180, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14180, \n",
            "Title: The Influence and Relationship between Computational Thinking, Learning Motivation, Attitude, and Achievement of Code.org in K-12 Programming Education \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: This study examined the impact of this http URL's block-based coding curriculum on primary school students' computational thinking, motivation, attitudes, and academic performance. Twenty students participated, and a range of tools was used: the Programming Computational Thinking Scale (PCTS) to evaluate computational thinking, the Instructional Materials Motivation Survey (IMMS) for motivation, the Attitude Scale of Computer Programming Learning (ASCOPL) for attitudes, and the Programming Achievement Test (PAT) for programming performance. The results revealed significant improvements in computational thinking, motivation, attitudes, and programming performance, with strong positive correlations among these factors. ANOVA analysis highlighted significant differences in computational concepts, perspectives, and motivational factors like attention and confidence, emphasizing their interdependence in programming success. This study highlights the interconnectedness of these factors and their importance in supporting programming achievement in primary school students, addressing gaps in the literature on block-based programming education.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 11|||| \n",
            "'arxiv_id': arXiv:2412.14188, \n",
            "'paper_link': https://arxiv.org/abs/2412.14188, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14188, \n",
            "Title: CogSimulator: A Model for Simulating User Cognition & Behavior with Minimal Data for Tailored Cognitive Enhancement \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: The interplay between cognition and gaming, notably through educational games enhancing cognitive skills, has garnered significant attention in recent years. This research introduces the CogSimulator, a novel algorithm for simulating user cognition in small-group settings with minimal data, as the educational game Wordle exemplifies. The CogSimulator employs Wasserstein-1 distance and coordinates search optimization for hyperparameter tuning, enabling precise few-shot predictions in new game scenarios. Comparative experiments with the Wordle dataset illustrate that our model surpasses most conventional machine learning models in mean Wasserstein-1 distance, mean squared error, and mean accuracy, showcasing its efficacy in cognitive enhancement through tailored game design.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 20|||| \n",
            "'arxiv_id': arXiv:2412.14200, \n",
            "'paper_link': https://arxiv.org/abs/2412.14200, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14200, \n",
            "Title: ActiveAI: Enabling K-12 AI Literacy Education & Analytics at Scale \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Interest in K-12 AI Literacy education has surged in the past year, yet large-scale learning data remains scarce despite considerable efforts in developing learning materials and running summer programs. To make larger scale dataset available and enable more replicable findings, we developed an intelligent online learning platform featuring AI Literacy modules and assessments, engaging 1,000 users from 12 secondary schools. Preliminary analysis of the data reveals patterns in prior knowledge levels of AI Literacy, gender differences in assessment scores, and the effectiveness of instructional activities. With open access to this de-identified dataset, researchers can perform secondary analyses, advancing the understanding in this emerging field of AI Literacy education.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 21|||| \n",
            "'arxiv_id': arXiv:2412.14201, \n",
            "'paper_link': https://arxiv.org/abs/2412.14201, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14201, \n",
            "Title: The \"Huh?\" Button: Improving Understanding in Educational Videos with Large Language Models \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: We propose a simple way to use large language models (LLMs) in education. Specifically, our method aims to improve individual comprehension by adding a novel feature to online videos. We combine the low threshold for interactivity in digital experiences with the benefits of rephrased and elaborated explanations typical of face-to-face interactions, thereby supporting to close knowledge gaps at scale. To demonstrate the technical feasibility of our approach, we conducted a proof-of-concept experiment and implemented a prototype which is available for testing online. Through the use case, we also show how caching can be applied in LLM-powered applications to reduce their carbon footprint.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 365|||| \n",
            "'arxiv_id': arXiv:2412.15030, \n",
            "'paper_link': https://arxiv.org/abs/2412.15030, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15030, \n",
            "Title: When Copilot Becomes Autopilot: Generative AI's Critical Risk to Knowledge Work and a Critical Solution \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Generative AI, with its tendency to \"hallucinate\" incorrect results, may pose a risk to knowledge work by introducing errors. On the other hand, it may also provide unprecedented opportunities for users, particularly non-experts, to learn and apply advanced software features and greatly increase the scope and complexity of tasks they can successfully achieve.\n",
            "As an example of a complex knowledge workflow that is subject to risks and opportunities from generative AI, we consider the spreadsheet. AI hallucinations are an important challenge, but they are not the greatest risk posed by generative AI to spreadsheet workflows. Rather, as more work can be safely delegated to AI, the risk is that human critical thinking -- the ability to holistically and rigorously evaluate a problem and its solutions -- is degraded in the process. The solution is to design the interfaces of generative AI systems deliberately to foster and encourage critical thinking in knowledge work, building primarily on a long history of research on critical thinking tools for education.\n",
            "We discuss a prototype system for the activity of critical shortlisting in spreadsheets. The system uses generative AI to suggest shortlisting criteria and applies these criteria to sort rows in a spreadsheet. It also generates \"provocations\": short text snippets that critique the AI-generated criteria, highlighting risks, shortcomings, and alternatives. Our prototype opens up a rich and completely unexplored design space of critical thinking tools for modern AI-assisted knowledge work. We outline a research agenda for AI as a critic or provocateur, including questions about where and when provocations should appear, their form and content, and potential design trade-offs.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 616|||| \n",
            "'arxiv_id': arXiv:2409.08498, \n",
            "'paper_link': https://arxiv.org/abs/2409.08498, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.08498, \n",
            "Title: Incorporating Procedural Fairness in Flag Submissions on Social Media Platforms \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Flagging mechanisms on social media platforms allow users to report inappropriate posts/accounts for review by content moderators. These reports are pivotal to platforms' efforts toward regulating norm violations. This paper examines how platforms' design choices in implementing flagging mechanisms influence flaggers' perceptions of content moderation. We conducted a survey experiment asking US respondents (N=2,936) to flag inappropriate posts using one of 54 randomly assigned flagging implementations. After flagging, participants rated their fairness perceptions of the flag submission process along the dimensions of consistency, transparency, and voice (agency). We found that participants perceived greater transparency when flagging interfaces included community guidelines and greater voice when they incorporated a text box for open-ended feedback. Our qualitative analysis highlights user needs for improved accessibility, educational support for reporting, and protections against false flags. We offer design recommendations for building fairer flagging systems without exacerbating the cognitive burden of submitting flags.\n",
            "Score: 1.0\n",
            "\n",
            "Document: 0|||| \n",
            "'arxiv_id': arXiv:2412.14174, \n",
            "'paper_link': https://arxiv.org/abs/2412.14174, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14174, \n",
            "Title: Steering Large Text-to-Image Model for Abstract Art Synthesis: Preference-based Prompt Optimization and Visualization \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: With the advancement of neural generative capabilities, the art community has increasingly embraced GenAI (Generative Artificial Intelligence), particularly large text-to-image models, for producing aesthetically compelling results. However, the process often lacks determinism and requires a tedious trial-and-error process as users often struggle to devise effective prompts to achieve their desired outcomes. This paper introduces a prompting-free generative approach that applies a genetic algorithm and real-time iterative human feedback to optimize prompt generation, enabling the creation of user-preferred abstract art through a customized Artist Model. The proposed two-part approach begins with constructing an Artist Model capable of deterministically generating abstract art in specific styles, e.g., Kandinsky's Bauhaus style. The second phase integrates real-time user feedback to optimize the prompt generation and obtains an Optimized Prompting Model, which adapts to user preferences and generates prompts automatically. When combined with the Artist Model, this approach allows users to create abstract art tailored to their personal preferences and artistic style.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 2|||| \n",
            "'arxiv_id': arXiv:2412.14176, \n",
            "'paper_link': https://arxiv.org/abs/2412.14176, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14176, \n",
            "Title: A Panopticon on My Wrist: The Biopower of Big Data Visualization for Wearables \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Big data visualization - the visual-spatial display of quantitative information culled from huge data sets - is now firmly embedded within the everyday experiences of people across the globe, yet scholarship on it remains surprisingly small. Within this literature, critical theorizations of big data visualizations are rare, as digital positivist perspectives dominate. This paper offers a critical, design-informed perspective on big data visualization in wearable health tracking ecosystems like FitBit. I argue that such visualizations are tools of individualized, neoliberal governance that operate largely through experiences of seduction and addiction to facilitate participation in the corporate capture and monetization of personal information. Exploration of my personal experience of the FitBit ecosystem illuminates this argument and emphasizes the capacity for harm to individuals using these ecosystems, leading to an exploration of the complex professional challenges for user experience designers working on visualizations within the ecosystems of wearables.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 7|||| \n",
            "'arxiv_id': arXiv:2412.14183, \n",
            "'paper_link': https://arxiv.org/abs/2412.14183, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14183, \n",
            "Title: Automating Compliance in Government Organisations using eFLINT \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Ensuring compliance of norms and policies when working on administrative law cases can be difficult to manage for government organisations. Automating this process could save a lot of time, effort and ensure compliance. Prior research resulted in a method to formalize sources of norms. These can be turned into executable specifications using the domain-specific language eFLINT, which can be used for automating compliance. However, the current interface of eFLINT prevents adaption by legal experts. The aim of this research was to bridge this gap by developing a prototype based on eFLINT, for automating compliance within government organisations. To get a better understanding of the needs and requirements of potential users, qualitative research was conducted. This consisted of semi-structured interviews to gather requirements, which were analyzed using a thematic analysis method. Based on the analyzed data, a design for the interface of the prototype was made. The final prototype was evaluated in a user end study which included a cognitive walkthrough and user testing. The prototype proved to be a good first step in the right direction with a lot of room for further development.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 8|||| \n",
            "'arxiv_id': arXiv:2412.14185, \n",
            "'paper_link': https://arxiv.org/abs/2412.14185, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14185, \n",
            "Title: Fabric Sensing of Intrinsic Hand Muscle Activity \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Wearable robotics have the capacity to assist stroke survivors in assisting and rehabilitating hand function. Many devices that use surface electromyographic (sEMG) for control rely on extrinsic muscle signals, since sEMG sensors are relatively easy to place on the forearm without interfering with hand activity. In this work, we target the intrinsic muscles of the thumb, which are superficial to the skin and thus potentially more accessible via sEMG sensing. However, traditional, rigid electrodes can not be placed on the hand without adding bulk and affecting hand functionality. We thus present a novel sensing sleeve that uses textile electrodes to measure sEMG activity of intrinsic thumb muscles. We evaluate the sleeve's performance on detecting thumb movements and muscle activity during both isolated and isometric muscle contractions of the thumb and fingers. This work highlights the potential of textile-based sensors as a low-cost, lightweight, and non-obtrusive alternative to conventional sEMG sensors for wearable robotics.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 10|||| \n",
            "'arxiv_id': arXiv:2412.14187, \n",
            "'paper_link': https://arxiv.org/abs/2412.14187, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14187, \n",
            "Title: Detecting Dark Patterns in User Interfaces Using Logistic Regression and Bag-of-Words Representation \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Dark patterns in user interfaces represent deceptive design practices intended to manipulate users' behavior, often leading to unintended consequences such as coerced purchases, involuntary data disclosures, or user frustration. Detecting and mitigating these dark patterns is crucial for promoting transparency, trust, and ethical design practices in digital environments. This paper proposes a novel approach for detecting dark patterns in user interfaces using logistic regression and bag-of-words representation. Our methodology involves collecting a diverse dataset of user interface text samples, preprocessing the data, extracting text features using the bag-of-words representation, training a logistic regression model, and evaluating its performance using various metrics such as accuracy, precision, recall, F1-score, and the area under the ROC curve (AUC). Experimental results demonstrate the effectiveness of the proposed approach in accurately identifying instances of dark patterns, with high predictive performance and robustness to variations in dataset composition and model parameters. The insights gained from this study contribute to the growing body of knowledge on dark patterns detection and classification, offering practical implications for designers, developers, and policymakers in promoting ethical design practices and protecting user rights in digital environments.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 12|||| \n",
            "'arxiv_id': arXiv:2412.14189, \n",
            "'paper_link': https://arxiv.org/abs/2412.14189, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14189, \n",
            "Title: Toward Ethical Spatial Analysis: Addressing Endogenous Bias Through Visual Analytics \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Spatial analysis can generate both exogenous and endogenous biases, which will lead to ethics issues. Exogenous biases arise from external factors or environments and are unrelated to internal operating mechanisms, while endogenous biases stem from internal processes or technologies. Although much attention has been given to exogenous biases, endogenous biases in spatial analysis have been largely overlooked, and a comprehensive methodology for addressing them is yet to be developed. To tackle this challenge, we propose that visual analytics can play a key role in understanding geographic data and improving the interpretation of analytical results. In this study, we conducted a preliminary investigation using various visualization techniques to explore endogenous biases. Our findings demonstrate the potentials of visual analytics to uncover hidden biases and identify associated issues. Additionally, we synthesized these visualization strategies into a framework that approximates a method for detecting endogenous biases. Through this work, we advocate for the integration of visualization at three critical stages of spatial analysis in order to minimize errors, address ethical concerns, and reduce misinterpretations associated with endogenous biases.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 13|||| \n",
            "'arxiv_id': arXiv:2412.14190, \n",
            "'paper_link': https://arxiv.org/abs/2412.14190, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14190, \n",
            "Title: Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion's identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the \"new\" AI relative to the \"original\". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 14|||| \n",
            "'arxiv_id': arXiv:2412.14191, \n",
            "'paper_link': https://arxiv.org/abs/2412.14191, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14191, \n",
            "Title: Ontology-Aware RAG for Improved Question-Answering in Cybersecurity Education \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Integrating AI into education has the potential to transform the teaching of science and technology courses, particularly in the field of cybersecurity. AI-driven question-answering (QA) systems can actively manage uncertainty in cybersecurity problem-solving, offering interactive, inquiry-based learning experiences. Large language models (LLMs) have gained prominence in AI-driven QA systems, offering advanced language understanding and user engagement. However, they face challenges like hallucinations and limited domain-specific knowledge, which reduce their reliability in educational settings. To address these challenges, we propose CyberRAG, an ontology-aware retrieval-augmented generation (RAG) approach for developing a reliable and safe QA system in cybersecurity education. CyberRAG employs a two-step approach: first, it augments the domain-specific knowledge by retrieving validated cybersecurity documents from a knowledge base to enhance the relevance and accuracy of the response. Second, it mitigates hallucinations and misuse by integrating a knowledge graph ontology to validate the final answer. Experiments on publicly available cybersecurity datasets show that CyberRAG delivers accurate, reliable responses aligned with domain knowledge, demonstrating the potential of AI tools to enhance education.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 15|||| \n",
            "'arxiv_id': arXiv:2412.14193, \n",
            "'paper_link': https://arxiv.org/abs/2412.14193, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14193, \n",
            "Title: Whom do Explanations Serve? A Systematic Literature Survey of User Characteristics in Explainable Recommender Systems Evaluation \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Adding explanations to recommender systems is said to have multiple benefits, such as increasing user trust or system transparency. Previous work from other application areas suggests that specific user characteristics impact the users' perception of the explanation. However, we rarely find this type of evaluation for recommender systems explanations. This paper addresses this gap by surveying 124 papers in which recommender systems explanations were evaluated in user studies. We analyzed their participant descriptions and study results where the impact of user characteristics on the explanation effects was measured. Our findings suggest that the results from the surveyed studies predominantly cover specific users who do not necessarily represent the users of recommender systems in the evaluation domain. This may seriously hamper the generalizability of any insights we may gain from current studies on explanations in recommender systems. We further find inconsistencies in the data reporting, which impacts the reproducibility of the reported results. Hence, we recommend actions to move toward a more inclusive and reproducible evaluation.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 16|||| \n",
            "'arxiv_id': arXiv:2412.14194, \n",
            "'paper_link': https://arxiv.org/abs/2412.14194, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14194, \n",
            "Title: Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: INTRODUCTION: The aging society urgently requires scalable methods to monitor cognitive decline and identify social and psychological factors indicative of dementia risk in older adults. METHODS: Our machine learning models captured facial, acoustic, linguistic, and cardiovascular features from 39 individuals with normal cognition or Mild Cognitive Impairment derived from remote video conversations and classified cognitive status, social isolation, neuroticism, and psychological well-being. RESULTS: Our model could distinguish Clinical Dementia Rating Scale of 0.5 (vs. 0) with 0.78 area under the receiver operating characteristic curve (AUC), social isolation with 0.75 AUC, neuroticism with 0.71 AUC, and negative affect scales with 0.79 AUC. DISCUSSION: Our findings demonstrate the feasibility of remotely monitoring cognitive status, social isolation, neuroticism, and psychological well-being. Speech and language patterns were more useful for quantifying cognitive impairment, whereas facial expression and cardiovascular patterns using remote photoplethysmography were more useful for quantifying personality and psychological well-being.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 19|||| \n",
            "'arxiv_id': arXiv:2412.14199, \n",
            "'paper_link': https://arxiv.org/abs/2412.14199, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14199, \n",
            "Title: Designing Human and Generative AI Collaboration \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: We examined the effectiveness of human-AI collaboration designs in creative work. Through a human subjects experiment in the context of creative writing, we found that while AI assistance improved productivity across all models, collaboration design significantly influenced output quality, user satisfaction, and content characteristics. Models incorporating human creative input delivered higher content interestingness and overall quality as well as greater task performer satisfaction compared to conditions where humans were limited to confirming AI outputs. Increased AI involvement encouraged creators to explore beyond personal experience but also led to greater story and genre similarities among participants. However, this effect was mitigated through human creative input. These findings underscore the importance of preserving the human creative role to ensure quality, satisfaction, and creative diversity in human-AI collaboration.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 22|||| \n",
            "'arxiv_id': arXiv:2412.14203, \n",
            "'paper_link': https://arxiv.org/abs/2412.14203, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14203, \n",
            "Title: BlenderLLM: Training Large Language Models for Computer-Aided Design with Self-improvement \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: The application of Large Language Models (LLMs) in Computer-Aided Design (CAD) remains an underexplored area, despite their remarkable advancements in other domains. In this paper, we present BlenderLLM, a novel framework for training LLMs specifically for CAD tasks leveraging a self-improvement methodology. To support this, we developed a bespoke training dataset, BlendNet, and introduced a comprehensive evaluation suite, CADBench. Our results reveal that existing models demonstrate significant limitations in generating accurate CAD scripts. However, through minimal instruction-based fine-tuning and iterative self-improvement, BlenderLLM significantly surpasses these models in both functionality and accuracy of CAD script generation. This research establishes a strong foundation for the application of LLMs in CAD while demonstrating the transformative potential of self-improving models in advancing CAD automation. We encourage further exploration and adoption of these methodologies to drive innovation in the field. The dataset, model, benchmark, and source code are publicly available at this https URL\n",
            "Score: 0.5\n",
            "\n",
            "Document: 23|||| \n",
            "'arxiv_id': arXiv:2412.14205, \n",
            "'paper_link': https://arxiv.org/abs/2412.14205, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14205, \n",
            "Title: Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Conversational Swarm Intelligence (CSI) is an AI-facilitated method for enabling real-time conversational deliberations and prioritizations among networked human groups of potentially unlimited size. Based on the biological principle of Swarm Intelligence and modelled on the decision-making dynamics of fish schools, CSI has been shown in prior studies to amplify group intelligence, increase group participation, and facilitate productive collaboration among hundreds of participants at once. It works by dividing a large population into a set of small subgroups that are woven together by real-time AI agents called Conversational Surrogates. The present study focuses on the use of a CSI platform called Thinkscape to enable real-time brainstorming and prioritization among groups of 75 networked users. The study employed a variant of a common brainstorming intervention called an Alternative Use Task (AUT) and was designed to compare through subjective feedback, the experience of participants brainstorming using a CSI structure vs brainstorming in a single large chat room. This comparison revealed that participants significantly preferred brainstorming with the CSI structure and reported that it felt (i) more collaborative, (ii) more productive, and (iii) was better at surfacing quality answers. In addition, participants using the CSI structure reported (iv) feeling more ownership and more buy-in in the final answers the group converged on and (v) reported feeling more heard as compared to brainstorming in a traditional text chat environment. Overall, the results suggest that CSI is a very promising AI-facilitated method for brainstorming and prioritization among large-scale, networked human groups.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 24|||| \n",
            "'arxiv_id': arXiv:2412.14206, \n",
            "'paper_link': https://arxiv.org/abs/2412.14206, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14206, \n",
            "Title: Design of an AI-Enhanced Digital Stethoscope: Advancing Cardiovascular Diagnostics Through Smart Auscultation \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: In the ever-evolving landscape of medical diagnostics, this study details the systematic design process and concept selection methodology for developing an advanced digital stethoscope, demonstrating the evolution from traditional acoustic models to AI-enhanced digital solutions. The device integrates cutting-edge AI technology with traditional auscultation methods to create a more accurate, efficient, and user-friendly diagnostic tool. Through systematic product planning, customer need analysis, and rigorous specification development, we identified key opportunities to enhance conventional stethoscope functionality. The proposed system features real-time sound analysis, automated classification of heart sounds, wireless connectivity for remote consultations, and an intuitive user interface accessible via smartphone integration. The design process employed a methodical approach incorporating customer feedback, competitive benchmarking, and systematic concept generation and selection. Through a structured evaluation framework, we analyzed portability, frequency response sensitivity, transmission quality, maintenance ease, user interface simplicity, output signal quality, power efficiency, and cost-effectiveness. The final design prioritizes biocompatibility, reliability, and cost-effectiveness while addressing the growing demand for telemedicine capabilities in cardiovascular care. The project emphasizes the transition from conventional design to advanced digital solutions while maintaining a focus on practical clinical applications. Each concept was modelled using SOLIDWORKS software, enabling detailed visualization and engineering analysis. This systematic approach to concept screening and selection ensures the final design meets both current healthcare needs and future technological adaptability.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 27|||| \n",
            "'arxiv_id': arXiv:2412.14209, \n",
            "'paper_link': https://arxiv.org/abs/2412.14209, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14209, \n",
            "Title: Integrating Evidence into the Design of XAI and AI-based Decision Support Systems: A Means-End Framework for End-users in Construction \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: A narrative review is used to develop a theoretical evidence-based means-end framework to build an epistemic foundation to uphold explainable artificial intelligence instruments so that the reliability of outcomes generated from decision support systems can be assured and better explained to end-users. The implications of adopting an evidence-based approach to designing decision support systems in construction are discussed with emphasis placed on evaluating the strength, value, and utility of evidence needed to develop meaningful human explanations for end-users. While the developed means-end framework is focused on end-users, stakeholders can also utilize it to create meaningful human explanations. However, they will vary due to their different epistemic goals. Including evidence in the design and development of explainable artificial intelligence and decision support systems will improve decision-making effectiveness, enabling end-users' epistemic goals to be achieved. The proposed means-end framework is developed from a broad spectrum of literature. Thus, it is suggested that it can be used in construction and other engineering domains where there is a need to integrate evidence into the design of explainable artificial intelligence and decision support systems.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 28|||| \n",
            "'arxiv_id': arXiv:2412.14210, \n",
            "'paper_link': https://arxiv.org/abs/2412.14210, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14210, \n",
            "Title: Mobilizing Waldo: Evaluating Multimodal AI for Public Mobilization \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Advancements in multimodal Large Language Models (LLMs), such as OpenAI's GPT-4o, offer significant potential for mediating human interactions across various contexts. However, their use in areas such as persuasion, influence, and recruitment raises ethical and security concerns. To evaluate these models ethically in public influence and persuasion scenarios, we developed a prompting strategy using \"Where's Waldo?\" images as proxies for complex, crowded gatherings. This approach provides a controlled, replicable environment to assess the model's ability to process intricate visual information, interpret social dynamics, and propose engagement strategies while avoiding privacy concerns. By positioning Waldo as a hypothetical agent tasked with face-to-face mobilization, we analyzed the model's performance in identifying key individuals and formulating mobilization tactics. Our results show that while the model generates vivid descriptions and creative strategies, it cannot accurately identify individuals or reliably assess social dynamics in these scenarios. Nevertheless, this methodology provides a valuable framework for testing and benchmarking the evolving capabilities of multimodal LLMs in social contexts.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 37|||| \n",
            "'arxiv_id': arXiv:2412.14222, \n",
            "'paper_link': https://arxiv.org/abs/2412.14222, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14222, \n",
            "Title: A Survey on Large Language Model-based Agents for Statistics and Data Science \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 39|||| \n",
            "'arxiv_id': arXiv:2412.14225, \n",
            "'paper_link': https://arxiv.org/abs/2412.14225, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14225, \n",
            "Title: The Effect of Age Introduced to Virtual Reality on Susceptibility to Motion Sickness \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Every human with a functioning vestibular system is capable of feeling motion sickness, but some are more vulnerable than others. Based on the leading theories explaining this condition, vulnerability should be predicted by a person's years of real-life experience before using a VR device and years of VR experience after. A questionnaire was filled out on susceptibility to motion sickness in VR by people on VR-related forums. Results from the survey show that the condition has a significant relationship with age or experience outside the environment.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 41|||| \n",
            "'arxiv_id': arXiv:2412.14229, \n",
            "'paper_link': https://arxiv.org/abs/2412.14229, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14229, \n",
            "Title: Transversal PACS Browser API: Addressing Interoperability Challenges in Medical Imaging Systems \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Advances in imaging technologies have revolutionised the medical imaging and healthcare sectors, leading to the widespread adoption of PACS for the storage, retrieval, and communication of medical images. Although these systems have improved operational efficiency, significant challenges remain in effectively retrieving DICOM images, which are essential for diagnosis and overall patient care. Moreover, issues such as fragmented systems, interoperability barriers, and complex user interfaces can often prevent healthcare professionals from efficiently accessing medical images. Addressing these challenges, the Transversal PACS Browser API is a robust and user-friendly solution designed to enhance the process of querying and retrieving DICOM images. It offers advanced filtering capabilities through a variety of filter options as well as a custom field search, that allows users to easily navigate through large medical image collections with ease. Additionally, the application provides a unified interface for querying and retrieving from multiple PACS stations, addressing the challenges of fragmentation and complexity associated with accessing medical images. Other key features include the ability to pre-view images directly within the application. All of this contributes to the transversal nature of the API, serving not only healthcare providers, but anyone who relies on efficient access to these resources. To validate the performance and usability of the application, comprehensive testing was carried out with stakeholders of the field, the results of which showed general satisfaction, highlighting the API's clean design, ease of use, and effective search capabilities of the API, as well as the usefulness of previewing images within the application.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 43|||| \n",
            "'arxiv_id': arXiv:2412.14232, \n",
            "'paper_link': https://arxiv.org/abs/2412.14232, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14232, \n",
            "Title: Human-in-the-loop or AI-in-the-loop? Automate or Collaborate? \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Human-in-the-loop (HIL) systems have emerged as a promising approach for combining the strengths of data-driven machine learning models with the contextual understanding of human experts. However, a deeper look into several of these systems reveals that calling them HIL would be a misnomer, as they are quite the opposite, namely AI-in-the-loop ($AI^2L$) systems, where the human is in control of the system, while the AI is there to support the human. We argue that existing evaluation methods often overemphasize the machine (learning) component's performance, neglecting the human expert's critical role. Consequently, we propose an $AI^2L$ perspective, which recognizes that the human expert is an active participant in the system, significantly influencing its overall performance. By adopting an $AI^2L$ approach, we can develop more comprehensive systems that faithfully model the intricate interplay between the human and machine components, leading to more effective and robust AI systems.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 130|||| \n",
            "'arxiv_id': arXiv:2412.14469, \n",
            "'paper_link': https://arxiv.org/abs/2412.14469, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14469, \n",
            "Title: Who is Helping Whom? Student Concerns about AI- Teacher Collaboration in Higher Education Classrooms \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: AI's integration into education promises to equip teachers with data-driven insights and intervene in student learning. Despite the intended advancements, there is a lack of understanding of interactions and emerging dynamics in classrooms where various stakeholders including teachers, students, and AI, collaborate. This paper aims to understand how students perceive the implications of AI in Education in terms of classroom collaborative dynamics, especially AI used to observe students and notify teachers to provide targeted help. Using the story completion method, we analyzed narratives from 65 participants, highlighting three challenges: AI decontextualizing of the educational context; AI-teacher cooperation with bias concerns and power disparities; and AI's impact on student behavior that further challenges AI's effectiveness. We argue that for effective and ethical AI-facilitated cooperative education, future AIEd design must factor in the situated nature of implementation. Designers must consider the broader nuances of the education context, impacts on multiple stakeholders, dynamics involving these stakeholders, and the interplay among potential consequences for AI systems and stakeholders. It is crucial to understand the values in the situated context, the capacity and limitations of both AI and humans for effective cooperation, and any implications to the relevant ecosystem.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 140|||| \n",
            "'arxiv_id': arXiv:2412.14481, \n",
            "'paper_link': https://arxiv.org/abs/2412.14481, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14481, \n",
            "Title: The Shape of Agency: Designing for Personal Agency in Qualitative Data Analysis \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Computational thematic analysis is rapidly emerging as a method of using large text corpora to understand the lived experience of people across the continuum of health care: patients, practitioners, and everyone in between. However, many qualitative researchers do not have the necessary programming skills to write machine learning code on their own, but also seek to maintain ownership, intimacy, and control over their analysis. In this work we explore the use of data visualizations to foster researcher agency and make computational thematic analysis more accessible to domain experts. We used a design science research approach to develop a datavis prototype over four phases: (1) problem comprehension, (2) specifying needs and requirements, (3) prototype development, and (4) feedback on the prototype. We show that qualitative researchers have a wide range of cognitive needs when conducting data analysis and place high importance upon choices and freedom, wanting to feel autonomy over their own research and not be replaced or hindered by AI.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 144|||| \n",
            "'arxiv_id': arXiv:2412.14486, \n",
            "'paper_link': https://arxiv.org/abs/2412.14486, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14486, \n",
            "Title: Moving Beyond LDA: A Comparison of Unsupervised Topic Modelling Techniques for Qualitative Data Analysis of Online Communities \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Social media constitutes a rich and influential source of information for qualitative researchers. Although computational techniques like topic modelling assist with managing the volume and diversity of social media content, qualitative researcher's lack of programming expertise creates a significant barrier to their adoption. In this paper we explore how BERTopic, an advanced Large Language Model (LLM)-based topic modelling technique, can support qualitative data analysis of social media. We conducted interviews and hands-on evaluations in which qualitative researchers compared topics from three modelling techniques: LDA, NMF, and BERTopic. BERTopic was favoured by 8 of 12 participants for its ability to provide detailed, coherent clusters for deeper understanding and actionable insights. Participants also prioritised topic relevance, logical organisation, and the capacity to reveal unexpected relationships within the data. Our findings underscore the potential of LLM-based techniques for supporting qualitative analysis.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 166|||| \n",
            "'arxiv_id': arXiv:2412.14521, \n",
            "'paper_link': https://arxiv.org/abs/2412.14521, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14521, \n",
            "Title: Dynamic User Interface Generation for Enhanced Human-Computer Interaction Using Variational Autoencoders \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: This study presents a novel approach for intelligent user interaction interface generation and optimization, grounded in the variational autoencoder (VAE) model. With the rapid advancement of intelligent technologies, traditional interface design methods struggle to meet the evolving demands for diversity and personalization, often lacking flexibility in real-time adjustments to enhance the user experience. Human-Computer Interaction (HCI) plays a critical role in addressing these challenges by focusing on creating interfaces that are functional, intuitive, and responsive to user needs. This research leverages the RICO dataset to train the VAE model, enabling the simulation and creation of user interfaces that align with user aesthetics and interaction habits. By integrating real-time user behavior data, the system dynamically refines and optimizes the interface, improving usability and underscoring the importance of HCI in achieving a seamless user experience. Experimental findings indicate that the VAE-based approach significantly enhances the quality and precision of interface generation compared to other methods, including autoencoders (AE), generative adversarial networks (GAN), conditional GANs (cGAN), deep belief networks (DBN), and VAE-GAN. This work contributes valuable insights into HCI, providing robust technical solutions for automated interface generation and enhanced user experience optimization.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 168|||| \n",
            "'arxiv_id': arXiv:2412.14526, \n",
            "'paper_link': https://arxiv.org/abs/2412.14526, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14526, \n",
            "Title: Knowledge Distillation in RNN-Attention Models for Early Prediction of Student Performance \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Educational data mining (EDM) is a part of applied computing that focuses on automatically analyzing data from learning contexts. Early prediction for identifying at-risk students is a crucial and widely researched topic in EDM research. It enables instructors to support at-risk students to stay on track, preventing student dropout or failure. Previous studies have predicted students' learning performance to identify at-risk students by using machine learning on data collected from e-learning platforms. However, most studies aimed to identify at-risk students utilizing the entire course data after the course finished. This does not correspond to the real-world scenario that at-risk students may drop out before the course ends. To address this problem, we introduce an RNN-Attention-KD (knowledge distillation) framework to predict at-risk students early throughout a course. It leverages the strengths of Recurrent Neural Networks (RNNs) in handling time-sequence data to predict students' performance at each time step and employs an attention mechanism to focus on relevant time steps for improved predictive accuracy. At the same time, KD is applied to compress the time steps to facilitate early prediction. In an empirical evaluation, RNN-Attention-KD outperforms traditional neural network models in terms of recall and F1-measure. For example, it obtained recall and F1-measure of 0.49 and 0.51 for Weeks 1--3 and 0.51 and 0.61 for Weeks 1--6 across all datasets from four years of a university course. Then, an ablation study investigated the contributions of different knowledge transfer methods (distillation objectives). We found that hint loss from the hidden layer of RNN and context vector loss from the attention module on RNN could enhance the model's prediction performance for identifying at-risk students. These results are relevant for EDM researchers employing deep learning models.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 270|||| \n",
            "'arxiv_id': arXiv:2412.14732, \n",
            "'paper_link': https://arxiv.org/abs/2412.14732, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14732, \n",
            "Title: Beyond the Hype: A Comprehensive Review of Current Trends in Generative AI Research, Teaching Practices, and Tools \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Generative AI (GenAI) is advancing rapidly, and the literature in computing education is expanding almost as quickly. Initial responses to GenAI tools were mixed between panic and utopian optimism. Many were fast to point out the opportunities and challenges of GenAI. Researchers reported that these new tools are capable of solving most introductory programming tasks and are causing disruptions throughout the curriculum. These tools can write and explain code, enhance error messages, create resources for instructors, and even provide feedback and help for students like a traditional teaching assistant. In 2024, new research started to emerge on the effects of GenAI usage in the computing classroom. These new data involve the use of GenAI to support classroom instruction at scale and to teach students how to code with GenAI. In support of the former, a new class of tools is emerging that can provide personalized feedback to students on their programming assignments or teach both programming and prompting skills at the same time. With the literature expanding so rapidly, this report aims to summarize and explain what is happening on the ground in computing classrooms. We provide a systematic literature review; a survey of educators and industry professionals; and interviews with educators using GenAI in their courses, educators studying GenAI, and researchers who create GenAI tools to support computing education. The triangulation of these methods and data sources expands the understanding of GenAI usage and perceptions at this critical moment for our community.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 275|||| \n",
            "'arxiv_id': arXiv:2412.14741, \n",
            "'paper_link': https://arxiv.org/abs/2412.14741, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14741, \n",
            "Title: Active Inference and Human--Computer Interaction \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Active Inference is a closed-loop computational theoretical basis for understanding behaviour, based on agents with internal probabilistic generative models that encode their beliefs about how hidden states in their environment cause their sensations. We review Active Inference and how it could be applied to model the human-computer interaction loop. Active Inference provides a coherent framework for managing generative models of humans, their environments, sensors and interface components. It informs off-line design and supports real-time, online adaptation. It provides model-based explanations for behaviours observed in HCI, and new tools to measure important concepts such as agency and engagement. We discuss how Active Inference offers a new basis for a theory of interaction in HCI, tools for design of modern, complex sensor-based systems, and integration of artificial intelligence technologies, enabling it to cope with diversity in human users and contexts. We discuss the practical challenges in implementing such Active Inference-based systems.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 285|||| \n",
            "'arxiv_id': arXiv:2412.14776, \n",
            "'paper_link': https://arxiv.org/abs/2412.14776, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14776, \n",
            "Title: Collaborative Problem Solving in Mixed Reality: A Study on Visual Graph Analysis \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Problem solving is a composite cognitive process, invoking a number of systems and subsystems, such as perception and memory. Individuals may form collectives to solve a given problem together, in collaboration, especially when complexity is thought to be high. To determine if and when collaborative problem solving is desired, we must quantify collaboration first. For this, we investigate the practical virtue of collaborative problem solving. Using visual graph analysis, we perform a study with 72 participants in two countries and three languages. We compare ad hoc pairs to individuals and nominal pairs, solving two different tasks on graphs in visuospatial mixed reality. The average collaborating pair does not outdo its nominal counterpart, but it does have a significant trade-off against the individual: an ad hoc pair uses 1.46 more time to achieve 4.6 higher accuracy. We also use the concept of task instance complexity to quantify differences in complexity. As task instance complexity increases, these differences largely scale, though with two notable exceptions. With this study we show the importance of using nominal groups as benchmark in collaborative virtual environments research. We conclude that a mixed reality environment does not automatically imply superior collaboration.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 320|||| \n",
            "'arxiv_id': arXiv:2412.14870, \n",
            "'paper_link': https://arxiv.org/abs/2412.14870, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14870, \n",
            "Title: Large-scale School Mapping using Weakly Supervised Deep Learning for Universal School Connectivity \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Improving global school connectivity is critical for ensuring inclusive and equitable quality education. To reliably estimate the cost of connecting schools, governments and connectivity providers require complete and accurate school location data - a resource that is often scarce in many low- and middle-income countries. To address this challenge, we propose a cost-effective, scalable approach to locating schools in high-resolution satellite images using weakly supervised deep learning techniques. Our best models, which combine vision transformers and convolutional neural networks, achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging explainable AI techniques, our approach can approximate the precise geographical coordinates of the school locations using only low-cost, classification-level annotations. To demonstrate the scalability of our method, we generate nationwide maps of school location predictions in African countries and present a detailed analysis of our results, using Senegal as our case study. Finally, we demonstrate the immediate usability of our work by introducing an interactive web mapping tool to streamline human-in-the-loop model validation efforts by government partners. This work successfully showcases the real-world utility of deep learning and satellite images for planning regional infrastructure and accelerating universal school connectivity.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 369|||| \n",
            "'arxiv_id': arXiv:2412.15047, \n",
            "'paper_link': https://arxiv.org/abs/2412.15047, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15047, \n",
            "Title: Measuring, Modeling, and Helping People Account for Privacy Risks in Online Self-Disclosures with AI \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: In pseudonymous online fora like Reddit, the benefits of self-disclosure are often apparent to users (e.g., I can vent about my in-laws to understanding strangers), but the privacy risks are more abstract (e.g., will my partner be able to tell that this is me?). Prior work has sought to develop natural language processing (NLP) tools that help users identify potentially risky self-disclosures in their text, but none have been designed for or evaluated with the users they hope to protect. Absent this assessment, these tools will be limited by the social-technical gap: users need assistive tools that help them make informed decisions, not paternalistic tools that tell them to avoid self-disclosure altogether. To bridge this gap, we conducted a study with N = 21 Reddit users; we had them use a state-of-the-art NLP disclosure detection model on two of their authored posts and asked them questions to understand if and how the model helped, where it fell short, and how it could be improved to help them make more informed decisions. Despite its imperfections, users responded positively to the model and highlighted its use as a tool that can help them catch mistakes, inform them of risks they were unaware of, and encourage self-reflection. However, our work also shows how, to be useful and usable, AI for supporting privacy decision-making must account for posting context, disclosure norms, and users' lived threat models, and provide explanations that help contextualize detected risks.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 520|||| \n",
            "'arxiv_id': arXiv:2402.15083, \n",
            "'paper_link': https://arxiv.org/abs/2402.15083, \n",
            "'pdf_link': https://arxiv.org/pdf/2402.15083, \n",
            "Title: Hands-Free VR \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: The paper introduces Hands-Free VR, a voice-based natural-language interface for VR. The user gives a command using their voice, the speech audio data is converted to text using a speech-to-text deep learning model that is fine-tuned for robustness to word phonetic similarity and to spoken English accents, and the text is mapped to an executable VR command using a large language model that is robust to natural language diversity. Hands-Free VR was evaluated in a controlled within-subjects study (N = 22) that asked participants to find specific objects and to place them in various configurations. In the control condition participants used a conventional VR user interface to grab, carry, and position the objects using the handheld controllers. In the experimental condition participants used Hands-Free VR. The results confirm that: (1) Hands-Free VR is robust to spoken English accents, as for 20 of our participants English was not their first language, and to word phonetic similarity, correctly transcribing the voice command 96.71% of the time; (2) Hands-Free VR is robust to natural language diversity, correctly mapping the transcribed command to an executable command in 97.83% of the time; (3) Hands-Free VR had a significant efficiency advantage over the conventional VR interface in terms of task completion time, total viewpoint translation, total view direction rotation, and total left and right hand translations; (4) Hands-Free VR received high user preference ratings in terms of ease of use, intuitiveness, ergonomics, reliability, and desirability.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 544|||| \n",
            "'arxiv_id': arXiv:2405.11965, \n",
            "'paper_link': https://arxiv.org/abs/2405.11965, \n",
            "'pdf_link': https://arxiv.org/pdf/2405.11965, \n",
            "Title: No Free Lunch: Research Software Testing in Teaching \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Software is at the core of most scientific discoveries today. Therefore, the quality of research results highly depends on the quality of the research software. Rigorous testing, as we know it from software engineering in the industry, could ensure the quality of the research software but it also requires a substantial effort that is often not rewarded in academia. Therefore, this research explores the effects of research software testing integrated into teaching on research software. In an in-vivo experiment, we integrated the engineering of a test suite for a large-scale network simulation as group projects into a course on software testing at the Blekinge Institute of Technology, Sweden, and qualitatively measured the effects of this integration on the research software. We found that the research software benefited from the integration through substantially improved documentation and fewer hardware and software dependencies. However, this integration was effortful and although the student teams developed elegant and thoughtful test suites, no code by students went directly into the research software since we were not able to make the integration back into the research software obligatory or even remunerative. Although we strongly believe that integrating research software engineering such as testing into teaching is not only valuable for the research software itself but also for students, the research of the next generation, as they get in touch with research software engineering and bleeding-edge research in their field as part of their education, the uncertainty about the intellectual properties of students' code substantially limits the potential of integrating research software testing into teaching.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 559|||| \n",
            "'arxiv_id': arXiv:2406.13144, \n",
            "'paper_link': https://arxiv.org/abs/2406.13144, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.13144, \n",
            "Title: DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party Dialogue Understanding of Conversational Agents \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Recent advancements in Large Language Models (LLMs) have significantly enhanced the capabilities of conversational agents, making them applicable to various fields (e.g., education). Despite their progress, the evaluation of the agents often overlooks the complexities of real-world conversations, such as real-time interactions, multi-party dialogues, and extended contextual dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue simulator. In this simulator, an agent is assigned the role of a character from popular TV shows, requiring it to respond to spontaneous questions using past dialogue information and to distinguish between known and unknown information. Key features of DialSim include assessing the agent's ability to respond within a reasonable time limit, handling long-term multi-party dialogues, and evaluating performance under randomized questioning with LongDialQA, a novel, high-quality question-answering dataset. Our experiments using DialSim reveal the strengths and weaknesses of the latest conversational agents, offering valuable insights for future advancements in conversational AI. DialSim is available at this https URL.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 593|||| \n",
            "'arxiv_id': arXiv:2408.06602, \n",
            "'paper_link': https://arxiv.org/abs/2408.06602, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.06602, \n",
            "Title: Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the \"rational superstition\" phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 597|||| \n",
            "'arxiv_id': arXiv:2408.10839, \n",
            "'paper_link': https://arxiv.org/abs/2408.10839, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.10839, \n",
            "Title: Benchmarking Large Language Models for Math Reasoning Tasks \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: The use of Large Language Models (LLMs) in mathematical reasoning has become a cornerstone of related research, demonstrating the intelligence of these models and enabling potential practical applications through their advanced performance, such as in educational settings. Despite the variety of datasets and in-context learning algorithms designed to improve the ability of LLMs to automate mathematical problem solving, the lack of comprehensive benchmarking across different datasets makes it complicated to select an appropriate model for specific tasks. In this project, we present a benchmark that fairly compares seven state-of-the-art in-context learning algorithms for mathematical problem solving across five widely used mathematical datasets on four powerful foundation models. Furthermore, we explore the trade-off between efficiency and performance, highlighting the practical applications of LLMs for mathematical reasoning. Our results indicate that larger foundation models like GPT-4o and LLaMA 3-70B can solve mathematical reasoning independently from the concrete prompting strategy, while for smaller models the in-context learning approach significantly influences the performance. Moreover, the optimal prompt depends on the chosen foundation model. We open-source our benchmark code to support the integration of additional models in future research.\n",
            "Score: 0.5\n",
            "\n",
            "Document: 602|||| \n",
            "'arxiv_id': arXiv:2408.14512, \n",
            "'paper_link': https://arxiv.org/abs/2408.14512, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.14512, \n",
            "Title: LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with LLM Token Embeddings \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Zero-shot graph machine learning, especially with graph neural networks (GNNs), has garnered significant interest due to the challenge of scarce labeled data. While methods like self-supervised learning and graph prompt learning have been extensively explored, they often rely on fine-tuning with task-specific labels, limiting their effectiveness in zero-shot scenarios. Inspired by the zero-shot capabilities of instruction-fine-tuned large language models (LLMs), we introduce a novel framework named Token Embedding-Aligned Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and cross-task zero-shot learners for graph machine learning. Concretely, we pretrain a GNN, aligning its representations with token embeddings of an LLM. We then train a linear projector that transforms the GNN's representations into a fixed number of graph token embeddings without tuning the LLM. A unified instruction is designed for various graph tasks at different levels, such as node classification (node-level) and link prediction (edge-level). These design choices collectively enhance our method's effectiveness in zero-shot learning, setting it apart from existing methods. Experiments show that our graph token embeddings help the LLM predictor achieve state-of-the-art performance on unseen datasets and tasks compared to other methods using LLMs as predictors.\n",
            "Score: 0.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"collective behavior\", 1),\n",
        "        (\"collective\", 1),\n",
        "        (\"coordination\", 1),\n",
        "        (\"oganization\", 1),\n",
        "        (\"behavior\", 1),\n",
        "        (\"ants\", 1),\n",
        "        (\"insects\", 1),\n",
        "        (\"worms\", 1),\n",
        "        (\"swarm\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "id": "9zPAIaz5xzmJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4584b16-1119-4c2d-8c16-f226bb57f420"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: collective behavior\n",
            "Total Matches in Set: 119\n",
            "Matches Above Score-Floor in Set: 13\n",
            "2024-12-20__041110297445\n",
            "\n",
            "Showing 13 in top-45 out of 119 total results.     -> 13 of 45/119\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 585|||| \n",
            "'arxiv_id': arXiv:2407.20041, \n",
            "'paper_link': https://arxiv.org/abs/2407.20041, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.20041, \n",
            "Title: Counterfactual rewards promote collective transport using individually controlled swarm microrobots \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Swarm robots offer fascinating opportunities to perform complex tasks beyond the capabilities of individual machines. Just as a swarm of ants collectively moves a large object, similar functions can emerge within a group of robots through individual strategies based on local sensing. However, realizing collective functions with individually controlled microrobots is particularly challenging due to their micrometer size, large number of degrees of freedom, strong thermal noise relative to the propulsion speed, complex physical coupling between neighboring microrobots, and surface collisions. Here, we implement Multi-Agent Reinforcement Learning (MARL) to generate a control strategy for up to 200 microrobots whose motions are individually controlled by laser spots. During the learning process, we employ so-called counterfactual rewards that automatically assign credit to the individual microrobots, which allows for fast and unbiased training. With the help of this efficient reward scheme, swarm microrobots learn to collectively transport a large cargo object to an arbitrary position and orientation, similar to ant swarms. We demonstrate that this flexible and versatile swarm robotic system is robust to variations in group size, the presence of malfunctioning units, and environmental noise. Such control strategies can potentially enable complex and automated assembly of mobile micromachines, programmable drug delivery capsules, and other advanced lab-on-a-chip applications.\n",
            "Score: 3\n",
            "\n",
            "Document: 23|||| \n",
            "'arxiv_id': arXiv:2412.14205, \n",
            "'paper_link': https://arxiv.org/abs/2412.14205, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14205, \n",
            "Title: Large-scale Group Brainstorming using Conversational Swarm Intelligence (CSI) versus Traditional Chat \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Conversational Swarm Intelligence (CSI) is an AI-facilitated method for enabling real-time conversational deliberations and prioritizations among networked human groups of potentially unlimited size. Based on the biological principle of Swarm Intelligence and modelled on the decision-making dynamics of fish schools, CSI has been shown in prior studies to amplify group intelligence, increase group participation, and facilitate productive collaboration among hundreds of participants at once. It works by dividing a large population into a set of small subgroups that are woven together by real-time AI agents called Conversational Surrogates. The present study focuses on the use of a CSI platform called Thinkscape to enable real-time brainstorming and prioritization among groups of 75 networked users. The study employed a variant of a common brainstorming intervention called an Alternative Use Task (AUT) and was designed to compare through subjective feedback, the experience of participants brainstorming using a CSI structure vs brainstorming in a single large chat room. This comparison revealed that participants significantly preferred brainstorming with the CSI structure and reported that it felt (i) more collaborative, (ii) more productive, and (iii) was better at surfacing quality answers. In addition, participants using the CSI structure reported (iv) feeling more ownership and more buy-in in the final answers the group converged on and (v) reported feeling more heard as compared to brainstorming in a traditional text chat environment. Overall, the results suggest that CSI is a very promising AI-facilitated method for brainstorming and prioritization among large-scale, networked human groups.\n",
            "Score: 2\n",
            "\n",
            "Document: 130|||| \n",
            "'arxiv_id': arXiv:2412.14469, \n",
            "'paper_link': https://arxiv.org/abs/2412.14469, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14469, \n",
            "Title: Who is Helping Whom? Student Concerns about AI- Teacher Collaboration in Higher Education Classrooms \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: AI's integration into education promises to equip teachers with data-driven insights and intervene in student learning. Despite the intended advancements, there is a lack of understanding of interactions and emerging dynamics in classrooms where various stakeholders including teachers, students, and AI, collaborate. This paper aims to understand how students perceive the implications of AI in Education in terms of classroom collaborative dynamics, especially AI used to observe students and notify teachers to provide targeted help. Using the story completion method, we analyzed narratives from 65 participants, highlighting three challenges: AI decontextualizing of the educational context; AI-teacher cooperation with bias concerns and power disparities; and AI's impact on student behavior that further challenges AI's effectiveness. We argue that for effective and ethical AI-facilitated cooperative education, future AIEd design must factor in the situated nature of implementation. Designers must consider the broader nuances of the education context, impacts on multiple stakeholders, dynamics involving these stakeholders, and the interplay among potential consequences for AI systems and stakeholders. It is crucial to understand the values in the situated context, the capacity and limitations of both AI and humans for effective cooperation, and any implications to the relevant ecosystem.\n",
            "Score: 2\n",
            "\n",
            "Document: 208|||| \n",
            "'arxiv_id': arXiv:2412.14606, \n",
            "'paper_link': https://arxiv.org/abs/2412.14606, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14606, \n",
            "Title: Computational Sociology of Humans and Machines; Conflict and Collaboration \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: This Chapter examines the dynamics of conflict and collaboration in human-machine systems, with a particular focus on large-scale, internet-based collaborative platforms. While these platforms represent successful examples of collective knowledge production, they are also sites of significant conflict, as diverse participants with differing intentions and perspectives interact. The analysis identifies recurring patterns of interaction, including serial attacks, reciprocal revenge, and third-party interventions. These microstructures reveal the role of experience, cultural differences, and topic sensitivity in shaping human-human, human-machine, and machine-machine interactions. The chapter further investigates the role of algorithmic agents and bots, highlighting their dual nature: they enhance collaboration by automating tasks but can also contribute to persistent conflicts with both humans and other machines. We conclude with policy recommendations that emphasize transparency, balance, cultural sensitivity, and governance to maximize the benefits of human-machine synergy while minimizing potential detriments.\n",
            "Score: 2\n",
            "\n",
            "Document: 231|||| \n",
            "'arxiv_id': arXiv:2412.14646, \n",
            "'paper_link': https://arxiv.org/abs/2412.14646, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14646, \n",
            "Title: Optimization of Collective Bayesian Decision-Making in a Swarm of Miniaturized Vibration-Sensing Robots \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Inspection of infrastructure using static sensor nodes has become a well established approach in recent decades. In this work, we present an experimental setup to address a binary inspection task using mobile sensor nodes. The objective is to identify the predominant tile type in a 1mx1m tiled surface composed of vibrating and non-vibrating tiles. A swarm of miniaturized robots, equipped with onboard IMUs for sensing and IR sensors for collision avoidance, performs the inspection. The decision-making approach leverages a Bayesian algorithm, updating robots' belief using inference. The original algorithm uses one of two information sharing strategies. We introduce a novel information sharing strategy, aiming to accelerate the decision-making. To optimize the algorithm parameters, we develop a simulation framework calibrated to our real-world setup in the high-fidelity Webots robotic simulator. We evaluate the three information sharing strategies through simulations and real-world experiments. Moreover, we test the effectiveness of our optimization by placing swarms with optimized and non-optimized parameters in increasingly complex environments with varied spatial correlation and fill ratios. Results show that our proposed information sharing strategy consistently outperforms previously established information-sharing strategies in decision time. Additionally, optimized parameters yield robust performance across different environments. Conversely, non-optimized parameters perform well in simpler scenarios but show reduced accuracy in complex settings.\n",
            "Score: 2\n",
            "\n",
            "Document: 285|||| \n",
            "'arxiv_id': arXiv:2412.14776, \n",
            "'paper_link': https://arxiv.org/abs/2412.14776, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14776, \n",
            "Title: Collaborative Problem Solving in Mixed Reality: A Study on Visual Graph Analysis \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Problem solving is a composite cognitive process, invoking a number of systems and subsystems, such as perception and memory. Individuals may form collectives to solve a given problem together, in collaboration, especially when complexity is thought to be high. To determine if and when collaborative problem solving is desired, we must quantify collaboration first. For this, we investigate the practical virtue of collaborative problem solving. Using visual graph analysis, we perform a study with 72 participants in two countries and three languages. We compare ad hoc pairs to individuals and nominal pairs, solving two different tasks on graphs in visuospatial mixed reality. The average collaborating pair does not outdo its nominal counterpart, but it does have a significant trade-off against the individual: an ad hoc pair uses 1.46 more time to achieve 4.6 higher accuracy. We also use the concept of task instance complexity to quantify differences in complexity. As task instance complexity increases, these differences largely scale, though with two notable exceptions. With this study we show the importance of using nominal groups as benchmark in collaborative virtual environments research. We conclude that a mixed reality environment does not automatically imply superior collaboration.\n",
            "Score: 2\n",
            "\n",
            "Document: 337|||| \n",
            "'arxiv_id': arXiv:2412.14954, \n",
            "'paper_link': https://arxiv.org/abs/2412.14954, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14954, \n",
            "Title: Corn Ear Detection and Orientation Estimation Using Deep Learning \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Monitoring growth behavior of maize plants such as the development of ears can give key insights into the plant's health and development. Traditionally, the measurement of the angle of ears is performed manually, which can be time-consuming and prone to human error. To address these challenges, this paper presents a computer vision-based system for detecting and tracking ears of corn in an image sequence. The proposed system could accurately detect, track, and predict the ear's orientation, which can be useful in monitoring their growth behavior. This can significantly save time compared to manual measurement and enables additional areas of ear orientation research and potential increase in efficiencies for maize production. Using an object detector with keypoint detection, the algorithm proposed could detect 90 percent of all ears. The cardinal estimation had a mean absolute error (MAE) of 18 degrees, compared to a mean 15 degree difference between two people measuring by hand. These results demonstrate the feasibility of using computer vision techniques for monitoring maize growth and can lead to further research in this area.\n",
            "Score: 2\n",
            "\n",
            "Document: 563|||| \n",
            "'arxiv_id': arXiv:2406.16606, \n",
            "'paper_link': https://arxiv.org/abs/2406.16606, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.16606, \n",
            "Title: Cherry on the Cake: Fairness is NOT an Optimization Problem \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: In Fair AI literature, the practice of maliciously creating unfair models that nevertheless satisfy fairness constraints is known as \"cherry-picking\". A cherry-picking model is a model that makes mistakes on purpose, selecting bad individuals from a minority class instead of better candidates from the same minority. The model literally cherry-picks whom to select to superficially meet the fairness constraints while making minimal changes to the unfair model. This practice has been described as \"blatantly unfair\" and has a negative impact on already marginalized communities, undermining the intended purpose of fairness measures specifically designed to protect these communities. A common assumption is that cherry-picking arises solely from malicious intent and that models designed only to optimize fairness metrics would avoid this behavior. We show that this is not the case: models optimized to minimize fairness metrics while maximizing performance are often forced to cherry-pick to some degree. In other words, cherry-picking might be an inevitable outcome of the optimization process itself. To demonstrate this, we use tools from fair cake-cutting, a mathematical subfield that studies the problem of fairly dividing a resource, referred to as the \"cake,\" among a number of participants. This concept is connected to supervised multi-label classification: any dataset can be thought of as a cake that needs to be distributed among different labels, and the model is the function that divides the cake. We adapt these classical results for machine learning and demonstrate how this connection can be prolifically used for fairness and classification in general.\n",
            "Score: 2\n",
            "\n",
            "Document: 593|||| \n",
            "'arxiv_id': arXiv:2408.06602, \n",
            "'paper_link': https://arxiv.org/abs/2408.06602, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.06602, \n",
            "Title: Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the \"rational superstition\" phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.\n",
            "Score: 2\n",
            "\n",
            "Document: 595|||| \n",
            "'arxiv_id': arXiv:2408.10455, \n",
            "'paper_link': https://arxiv.org/abs/2408.10455, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.10455, \n",
            "Title: IDEA: Enhancing the Rule Learning Ability of Large Language Model Agent through Induction, Deduction, and Abduction \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: While large language models (LLMs) have been thoroughly evaluated for deductive and inductive reasoning, their proficiency in holistic rule learning in interactive environments remains less explored. We introduce RULEARN, a novel benchmark to assess the rule-learning abilities of LLM agents in interactive settings. In RULEARN, agents strategically interact with simulated environments to gather observations, discern patterns, and solve complex problems. To enhance the rule-learning capabilities for LLM agents, we propose IDEA, a novel reasoning framework that integrates the process of Induction, Deduction, and Abduction. The IDEA agent generates initial hypotheses from limited observations through abduction, devises plans to validate these hypotheses or leverages them to solve problems via deduction, and refines previous hypotheses through induction, dynamically establishing and applying rules that mimic human rule-learning behaviors. Our evaluation of the IDEA framework, which involves five representative LLMs, demonstrates significant improvements over the baseline. Furthermore, our study with human participants reveals notable discrepancies in rule-learning behaviors between humans and LLMs. We believe our benchmark will serve as a valuable and challenging resource, and IDEA will provide crucial insights for the development of LLM agents capable of human-like rule learning in real-world scenarios. Our code and data is publicly available.\n",
            "Score: 2\n",
            "\n",
            "Document: 606|||| \n",
            "'arxiv_id': arXiv:2409.01174, \n",
            "'paper_link': https://arxiv.org/abs/2409.01174, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.01174, \n",
            "Title: Development and Validation of a Modular Sensor-Based System for Gait Analysis and Control in Lower-Limb Exoskeletons \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: With rapid advancements in exoskeleton hardware technologies, successful assessment and accurate control remain challenging. This study introduces a modular sensor-based system to enhance biomechanical evaluation and control in lower-limb exoskeletons, utilizing advanced sensor technologies and fuzzy logic. We aim to surpass the limitations of current biomechanical evaluation methods confined to laboratories and to address the high costs and complexity of exoskeleton control systems. The system integrates inertial measurement units, force-sensitive resistors, and load cells into instrumented crutches and 3D-printed insoles. These components function both independently and collectively to capture comprehensive biomechanical data, including the anteroposterior center of pressure and crutch ground reaction forces. This data is processed through a central unit using fuzzy logic algorithms for real-time gait phase estimation and exoskeleton control. Validation experiments with three participants, benchmarked against gold-standard motion capture and force plate technologies, demonstrate our system's capability for reliable gait phase detection and precise biomechanical measurements. By offering our designs open-source and integrating cost-effective technologies, this study advances wearable robotics and promotes broader innovation and adoption in exoskeleton research.\n",
            "Score: 2\n",
            "\n",
            "Document: 611|||| \n",
            "'arxiv_id': arXiv:2409.04937, \n",
            "'paper_link': https://arxiv.org/abs/2409.04937, \n",
            "'pdf_link': https://arxiv.org/pdf/2409.04937, \n",
            "Title: CONNECTOR: Enhancing the Traceability of Decentralized Bridge Applications via Automatic Cross-chain Transaction Association \n",
            "Subjects: Software Engineering (cs.SE) \n",
            "Abstract: Decentralized bridge applications are important software that connects various blockchains and facilitates cross-chain asset transfer in the decentralized finance (DeFi) ecosystem which currently operates in a multi-chain environment. Cross-chain transaction association identifies and matches unique transactions executed by bridge DApps, which is important research to enhance the traceability of cross-chain bridge DApps. However, existing methods rely entirely on unobservable internal ledgers or APIs, violating the open and decentralized properties of blockchain. In this paper, we analyze the challenges of this issue and then present CONNECTOR, an automated cross-chain transaction association analysis method based on bridge smart contracts. Specifically, CONNECTOR first identifies deposit transactions by extracting distinctive and generic features from the transaction traces of bridge contracts. With the accurate deposit transactions, CONNECTOR mines the execution logs of bridge contracts to achieve withdrawal transaction matching. We conduct real-world experiments on different types of bridges to demonstrate the effectiveness of CONNECTOR. The experiment demonstrates that CONNECTOR successfully identifies 100% deposit transactions, associates 95.81% withdrawal transactions, and surpasses methods for CeFi bridges. Based on the association results, we obtain interesting findings about cross-chain transaction behaviors in DeFi bridges and analyze the tracing abilities of CONNECTOR to assist the DeFi bridge apps.\n",
            "Score: 2\n",
            "\n",
            "Document: 661|||| \n",
            "'arxiv_id': arXiv:2411.16456, \n",
            "'paper_link': https://arxiv.org/abs/2411.16456, \n",
            "'pdf_link': https://arxiv.org/pdf/2411.16456, \n",
            "Title: Proxima. A DAG based cooperative distributed ledger \n",
            "Subjects: Distributed, Parallel, and Cluster Computing (cs.DC) \n",
            "Abstract: This paper introduces a novel architecture for a distributed ledger, commonly referred to as a \"blockchain\", which is organized in the form of directed acyclic graph (DAG) with UTXO transactions as vertices, rather than as a chain of blocks. Consensus on the state of ledger assets is achieved through the cooperative consensus: a profit-driven behavior of token holders themselves, which is viable only when they cooperate by following the biggest ledger coverage rule. The cooperative behavior is facilitated by enforcing purposefully designed UTXO transaction validity constraints. Token holders are the sole category of participants authorized to make amendments to the ledger, making participation completely permissionless - without miners, validators, committees or staking - and without any need of knowledge about the composition of the set of all participants in the consensus. The setup allows to achieve high throughput and scalability alongside with low transaction costs, while preserving key aspects of high decentralization, open participation, and asynchronicity found in Bitcoin and other proof-of-work blockchains, but without unreasonable energy consumption. Sybil protection is achieved similarly to proof-of-stake blockchains, using tokens native to the ledger, yet the architecture operates in a leaderless manner without block proposers and committee selection.\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"Retrieval-Augmented Systems\", 1),\n",
        "        (\"RAG systems\", 1),\n",
        "        (\"Retrieval-Augmented Generation\", 1),\n",
        "        (\"RAG evaluation metric \", 3),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oZ9kzdx6xhG",
        "outputId": "0885cb36-6459-4c6a-bc16-0380984d673c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Retrieval-Augmented Systems\n",
            "Total Matches in Set: 13\n",
            "Matches Above Score-Floor in Set: 3\n",
            "2024-12-20__041110423195\n",
            "\n",
            "Showing 3 in top-13 out of 13 total results.     -> 3 of 13/13\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 122|||| \n",
            "'arxiv_id': arXiv:2412.14457, \n",
            "'paper_link': https://arxiv.org/abs/2412.14457, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14457, \n",
            "Title: VISA: Retrieval Augmented Generation with Visual Source Attribution \n",
            "Subjects: Information Retrieval (cs.IR) \n",
            "Abstract: Generation with source attribution is important for enhancing the verifiability of retrieval-augmented generation (RAG) systems. However, existing approaches in RAG primarily link generated content to document-level references, making it challenging for users to locate evidence among multiple content-rich retrieved documents. To address this challenge, we propose Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel approach that combines answer generation with visual source attribution. Leveraging large vision-language models (VLMs), VISA identifies the evidence and highlights the exact regions that support the generated answers with bounding boxes in the retrieved document screenshots. To evaluate its effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the medical domain. Experimental results demonstrate the effectiveness of VISA for visual source attribution on documents' original look, as well as highlighting the challenges for improvement. Code, data, and model checkpoints will be released.\n",
            "Score: 2\n",
            "\n",
            "Document: 159|||| \n",
            "'arxiv_id': arXiv:2412.14510, \n",
            "'paper_link': https://arxiv.org/abs/2412.14510, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14510, \n",
            "Title: PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: The emergence of Retrieval-augmented generation (RAG) has alleviated the issues of outdated and hallucinatory content in the generation of large language models (LLMs), yet it still reveals numerous limitations. When a general-purpose LLM serves as the RAG generator, it often suffers from inadequate response informativeness, response robustness, and citation quality. Past approaches to tackle these limitations, either by incorporating additional steps beyond generating responses or optimizing the generator through supervised fine-tuning (SFT), still failed to align with the RAG requirement thoroughly. Consequently, optimizing the RAG generator from multiple preference perspectives while maintaining its end-to-end LLM form remains a challenge. To bridge this gap, we propose Multiple Perspective Preference Alignment for Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator of RAG systems to align with RAG requirements comprehensively. Specifically, we construct high-quality instruction fine-tuning data and multi-perspective preference data by sampling varied quality responses from the generator across different prompt documents quality scenarios. Subsequently, we optimize the generator using SFT and Direct Preference Optimization (DPO). Extensive experiments conducted on four question-answer datasets across three LLMs demonstrate that PA-RAG can significantly enhance the performance of RAG generators. Our code and datasets are available at this https URL.\n",
            "Score: 2\n",
            "\n",
            "Document: 698|||| \n",
            "'arxiv_id': arXiv:2412.10571, \n",
            "'paper_link': https://arxiv.org/abs/2412.10571, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.10571, \n",
            "Title: Evidence Contextualization and Counterfactual Attribution for Conversational QA over Heterogeneous Data with RAG Systems \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Retrieval Augmented Generation (RAG) works as a backbone for interacting with an enterprise's own data via Conversational Question Answering (ConvQA). In a RAG system, a retriever fetches passages from a collection in response to a question, which are then included in the prompt of a large language model (LLM) for generating a natural language (NL) answer. However, several RAG systems today suffer from two shortcomings: (i) retrieved passages usually contain their raw text and lack appropriate document context, negatively impacting both retrieval and answering quality; and (ii) attribution strategies that explain answer generation usually rely only on similarity between the answer and the retrieved passages, thereby only generating plausible but not causal explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies the above concerns by: (i) contextualizing evidence with source metadata and surrounding text; and (ii) computing counterfactual attribution, a causal explanation approach where the contribution of an evidence to an answer is determined by the similarity of the original response to the answer obtained by removing that evidence. To evaluate our proposals, we release a new benchmark ConfQuestions, with 300 hand-created conversational questions, each in English and German, coupled with ground truth URLs, completed questions, and answers from 215 public Confluence pages, that are typical of enterprise wiki spaces with heterogeneous elements. Experiments with RAGONITE on ConfQuestions show the viability of our ideas: contextualization improves RAG performance, and counterfactual attribution is effective at explaining RAG answers.\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 1\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"manifold hypothesis\", 1),\n",
        "        (\"manifolds\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcUutA30ij-m",
        "outputId": "eaf23756-912f-4b5c-8e76-24f203520a93"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: manifold hypothesis\n",
            "Total Matches in Set: 2\n",
            "Matches Above Score-Floor in Set: 2\n",
            "2024-12-20__041110540396\n",
            "\n",
            "Showing 2 in top-2 out of 2 total results.     -> 2 of 2/2\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 1) -> 1\n",
            "\n",
            "\n",
            "Document: 92|||| \n",
            "'arxiv_id': arXiv:2412.14384, \n",
            "'paper_link': https://arxiv.org/abs/2412.14384, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14384, \n",
            "Title: I0T: Embedding Standardization Method Towards Zero Modality Gap \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, $\\text{I0T}_{\\text{post}}$ that reduces the modality gap approximately to zero and (2) a trainable method, $\\text{I0T}_{\\text{async}}$, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, $\\text{I0T}_{\\text{post}}$ can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S).\n",
            "Score: 1\n",
            "\n",
            "Document: 582|||| \n",
            "'arxiv_id': arXiv:2407.17710, \n",
            "'paper_link': https://arxiv.org/abs/2407.17710, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.17710, \n",
            "Title: Revisiting Machine Unlearning with Dimensional Alignment \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Machine unlearning, an emerging research topic focusing on compliance with data privacy regulations, enables trained models to remove the information learned from specific data. While many existing methods indirectly address this issue by intentionally injecting incorrect supervisions, they can drastically and unpredictably alter the decision boundaries and feature spaces, leading to training instability and undesired side effects. To fundamentally approach this task, we first analyze the changes in latent feature spaces between original and retrained models, and observe that the feature representations of samples not involved in training are closely aligned with the feature manifolds of previously seen samples in training. Based on these findings, we introduce a novel evaluation metric for machine unlearning, coined dimensional alignment, which measures the alignment between the eigenspaces of the forget and retain set samples. We employ this metric as a regularizer loss to build a robust and stable unlearning framework, which is further enhanced by integrating a self-distillation loss and an alternating training scheme. Our framework effectively eliminates information from the forget set and preserves knowledge from the retain set. Lastly, we identify critical flaws in established evaluation metrics for machine unlearning, and introduce new evaluation tools that more accurately reflect the fundamental goals of machine unlearning.\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 1\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"sentiment analysis\", 3),\n",
        "        (\"semantic analysis\", 3),\n",
        "        (\"semantic modeling\", 3),\n",
        "        (\"emotion modeling\", 3),\n",
        "        (\"emotion analysis\", 3),\n",
        "        (\"sentiment recognition\", 2),\n",
        "        (\"semantic recognition\", 2),\n",
        "        (\"sentiment\", 1),\n",
        "        (\"semantically blinding\", 1),\n",
        "        (\"disambiguation\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        # (\"\", 1),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zxp71JXvY2dN",
        "outputId": "682a43e1-21da-43aa-8e9a-6eb2b76176bc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: sentiment analysis\n",
            "Total Matches in Set: 8\n",
            "Matches Above Score-Floor in Set: 8\n",
            "2024-12-20__041110643110\n",
            "\n",
            "Showing 8 in top-8 out of 8 total results.     -> 8 of 8/8\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 1) -> 1\n",
            "\n",
            "\n",
            "Document: 313|||| \n",
            "'arxiv_id': arXiv:2412.14849, \n",
            "'paper_link': https://arxiv.org/abs/2412.14849, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14849, \n",
            "Title: DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for Few-Shot Aspect-Based Sentiment Analysis \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Recently developed large language models (LLMs) have presented promising new avenues to address data scarcity in low-resource scenarios. In few-shot aspect-based sentiment analysis (ABSA), previous efforts have explored data augmentation techniques, which prompt LLMs to generate new samples by modifying existing ones. However, these methods fail to produce adequately diverse data, impairing their effectiveness. Besides, some studies apply in-context learning for ABSA by using specific instructions and a few selected examples as prompts. Though promising, LLMs often yield labels that deviate from task requirements. To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize data from two complementary perspectives: \\textit{key-point-driven} and \\textit{instance-driven}, which effectively generate diverse and high-quality ABSA samples in low-resource settings. Furthermore, a \\textit{label refinement} module is integrated to improve the synthetic labels. Extensive experiments demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA solutions and other LLM-oriented data generation methods.\n",
            "Score: 4\n",
            "\n",
            "Document: 649|||| \n",
            "'arxiv_id': arXiv:2411.02664, \n",
            "'paper_link': https://arxiv.org/abs/2411.02664, \n",
            "'pdf_link': https://arxiv.org/pdf/2411.02664, \n",
            "Title: Explanations that reveal all through the definition of encoding \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Feature attributions attempt to highlight what inputs drive predictive power. Good attributions or explanations are thus those that produce inputs that retain this predictive power; accordingly, evaluations of explanations score their quality of prediction. However, evaluations produce scores better than what appears possible from the values in the explanation for a class of explanations, called encoding explanations. Probing for encoding remains a challenge because there is no general characterization of what gives the extra predictive power. We develop a definition of encoding that identifies this extra predictive power via conditional dependence and show that the definition fits existing examples of encoding. This definition implies, in contrast to encoding explanations, that non-encoding explanations contain all the informative inputs used to produce the explanation, giving them a \"what you see is what you get\" property, which makes them transparent and simple to use. Next, we prove that existing scores (ROAR, FRESH, EVAL-X) do not rank non-encoding explanations above encoding ones, and develop STRIPE-X which ranks them correctly. After empirically demonstrating the theoretical insights, we use STRIPE-X to show that despite prompting an LLM to produce non-encoding explanations for a sentiment analysis task, the LLM-generated explanations encode.\n",
            "Score: 4\n",
            "\n",
            "Document: 730|||| \n",
            "'arxiv_id': arXiv:2412.13765, \n",
            "'paper_link': https://arxiv.org/abs/2412.13765, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.13765, \n",
            "Title: LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Current methods for analyzing student engagement in e-learning platforms, including automated systems, often struggle with challenges such as handling fuzzy sentiment in text comments and relying on limited metadata. Traditional approaches, such as surveys and questionnaires, also face issues like small sample sizes and scalability. In this paper, we introduce LLM-SEM (Language Model-Based Student Engagement Metric), a novel approach that leverages video metadata and sentiment analysis of student comments to measure engagement. By utilizing recent Large Language Models (LLMs), we generate high-quality sentiment predictions to mitigate text fuzziness and normalize key features such as views and likes. Our holistic method combines comprehensive metadata with sentiment polarity scores to gauge engagement at both the course and lesson levels. Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement. We fine-tuned TXLM-RoBERTa using human-annotated sentiment datasets to enhance prediction accuracy and utilized LLama 3B, and Gemma 9B from Ollama.\n",
            "Score: 4\n",
            "\n",
            "Document: 96|||| \n",
            "'arxiv_id': arXiv:2412.14399, \n",
            "'paper_link': https://arxiv.org/abs/2412.14399, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14399, \n",
            "Title: LLMSA: A Compositional Neuro-Symbolic Approach to Compilation-free and Customizable Static Analysis \n",
            "Subjects: Programming Languages (cs.PL) \n",
            "Abstract: Static analysis is essential for program optimization, bug detection, and debugging, but its reliance on compilation and limited customization hampers practical use. Advances in LLMs enable a new paradigm of compilation-free, customizable analysis via prompting. LLMs excel in interpreting program semantics on small code snippets and allow users to define analysis tasks in natural language with few-shot examples. However, misalignment with program semantics can cause hallucinations, especially in sophisticated semantic analysis upon lengthy code snippets.\n",
            "We propose LLMSA, a compositional neuro-symbolic approach for compilation-free, customizable static analysis with reduced hallucinations. Specifically, we propose an analysis policy language to support users decomposing an analysis problem into several sub-problems that target simple syntactic or semantic properties upon smaller code snippets. The problem decomposition enables the LLMs to target more manageable semantic-related sub-problems, while the syntactic ones are resolved by parsing-based analysis without hallucinations. An analysis policy is evaluated with lazy, incremental, and parallel prompting, which mitigates the hallucinations and improves the performance. It is shown that LLMSA achieves comparable and even superior performance to existing techniques in various clients. For instance, it attains 66.27% precision and 78.57% recall in taint vulnerability detection, surpassing an industrial approach in F1 score by 0.20.\n",
            "Score: 3\n",
            "\n",
            "Document: 266|||| \n",
            "'arxiv_id': arXiv:2412.14719, \n",
            "'paper_link': https://arxiv.org/abs/2412.14719, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14719, \n",
            "Title: Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Micro-Action Recognition (MAR) has gained increasing attention due to its crucial role as a form of non-verbal communication in social interactions, with promising potential for applications in human communication and emotion analysis. However, current approaches often overlook the inherent ambiguity in micro-actions, which arises from the wide category range and subtle visual differences between categories. This oversight hampers the accuracy of micro-action recognition. In this paper, we propose a novel Prototypical Calibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the ambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to identify the ambiguous sample, categorizing them into distinct sets of ambiguous samples of false negatives and false positives, considering both body- and action-level categories. \\textbf{Secondly}, we implement an ambiguous contrastive refinement module to calibrate these ambiguous samples by regulating the distance between ambiguous samples and their corresponding prototypes. This calibration process aims to pull false negative ($\\mathbb{FN}$) samples closer to their respective prototypes and push false positive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In addition, we propose a new prototypical diversity amplification loss to strengthen the model's capacity by amplifying the differences between different prototypes. \\textbf{Finally}, we propose a prototype-guided rectification to rectify prediction by incorporating the representability of prototypes. Extensive experiments conducted on the benchmark dataset demonstrate the superior performance of our method compared to existing approaches. The code is available at this https URL.\n",
            "Score: 3\n",
            "\n",
            "Document: 565|||| \n",
            "'arxiv_id': arXiv:2406.17256, \n",
            "'paper_link': https://arxiv.org/abs/2406.17256, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.17256, \n",
            "Title: Disentangled Motion Modeling for Video Frame Interpolation \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Video Frame Interpolation (VFI) aims to synthesize intermediate frames between existing frames to enhance visual smoothness and quality. Beyond the conventional methods based on the reconstruction loss, recent works have employed generative models for improved perceptual quality. However, they require complex training and large computational costs for pixel space modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a diffusion-based approach for VFI that enhances visual quality by focusing on intermediate motion modeling. We propose a disentangled two-stage training process. In the initial stage, frame synthesis and flow models are trained to generate accurate frames and flows optimal for synthesis. In the subsequent stage, we introduce a motion diffusion model, which incorporates our novel U-Net architecture specifically designed for optical flow, to generate bi-directional flows between frames. By learning the simpler low-frequency representation of motions, MoMo achieves superior perceptual quality with reduced computational demands compared to the generative modeling methods on the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics across various benchmarks, demonstrating its efficacy and efficiency in VFI.\n",
            "Score: 3\n",
            "\n",
            "Document: 185|||| \n",
            "'arxiv_id': arXiv:2412.14561, \n",
            "'paper_link': https://arxiv.org/abs/2412.14561, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14561, \n",
            "Title: GBRIP: Granular Ball Representation for Imbalanced Partial Label Learning \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Partial label learning (PLL) is a complicated weakly supervised multi-classification task compounded by class imbalance. Currently, existing methods only rely on inter-class pseudo-labeling from inter-class features, often overlooking the significant impact of the intra-class imbalanced features combined with the inter-class. To address these limitations, we introduce Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and multi-center loss to construct a granular ball-based nfeature space through unsupervised learning, effectively capturing the feature distribution within each class. GBRIP mitigates the impact of confusing features by systematically refining label disambiguation and estimating imbalance distributions. The novel multi-center loss function enhances learning by emphasizing the relationships between samples and their respective centers within the granular balls. Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms existing state-of-the-art methods, offering a robust solution to the challenges of imbalanced PLL.\n",
            "Score: 1\n",
            "\n",
            "Document: 384|||| \n",
            "'arxiv_id': arXiv:2412.15093, \n",
            "'paper_link': https://arxiv.org/abs/2412.15093, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15093, \n",
            "Title: Nano-ESG: Extracting Corporate Sustainability Information from News Articles \n",
            "Subjects: Information Retrieval (cs.IR) \n",
            "Abstract: Determining the sustainability impact of companies is a highly complex subject which has garnered more and more attention over the past few years. Today, investors largely rely on sustainability-ratings from established rating-providers in order to analyze how responsibly a company acts. However, those ratings have recently been criticized for being hard to understand and nearly impossible to reproduce.\n",
            "An independent way to find out about the sustainability practices of companies lies in the rich landscape of news article data. In this paper, we explore a different approach to identify key opportunities and challenges of companies in the sustainability domain. We present a novel dataset of more than 840,000 news articles which were gathered for major German companies between January 2023 and September 2024. By applying a mixture of Natural Language Processing techniques, we first identify relevant articles, before summarizing them and extracting their sustainability-related sentiment and aspect using Large Language Models (LLMs). Furthermore, we conduct an evaluation of the obtained data and determine that the LLM-produced answers are accurate. We release both datasets at this https URL.\n",
            "Score: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 45\n",
        "score_floor = 2\n",
        "list_of_lists_of_weights = [[\n",
        "        (\"mental health\", 5),\n",
        "        (\"psychological health\", 5),\n",
        "        (\"psycholog\", 2),  # stem vs. lemma\n",
        "        (\"mental health care\", 3),\n",
        "        (\"neuroscience\", 2),\n",
        "        (\"psychological assessment\", 2),\n",
        "        (\"personality assessment\", 2),\n",
        "        (\"personality inference\", 2),\n",
        "        (\"personality traits\", 2),\n",
        "        (\"personality dimensions\", 2),\n",
        "        (\"emotion\", 15),\n",
        "        (\"sports psychology\", 15),\n",
        "        (\"sentiment recognition\", 10),\n",
        "        (\"Emotion Recognition\", 5),\n",
        "        # (\"\", 5),\n",
        "        # (\"\", 5),\n",
        "\n",
        "        # disease terms\n",
        "        (\"depression\", 5),\n",
        "        (\"anxiety\", 5),\n",
        "        (\"mental disorders\", 2),\n",
        "        (\"social anxiety disorder\", 4),\n",
        "        (\"mental illness\", 2),\n",
        "        (\"Major Depressive Disorder\", 2),\n",
        "        (\"MDD\", 2),\n",
        "        (\"psychological stressors\", 2),\n",
        "        (\"cognitive impairment\", 2),\n",
        "        (\"mci\", 2),\n",
        "        (\"personality\", 1)\n",
        "        # (\"\", 2),\n",
        "        ],]\n",
        "match_print_save(list_of_lists_of_weights, top_n, score_floor)"
      ],
      "metadata": {
        "id": "CixuXw-Fl3-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969707ad-b2ba-46e2-fc89-aa09995022ec"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: mental health\n",
            "Total Matches in Set: 37\n",
            "Matches Above Score-Floor in Set: 36\n",
            "2024-12-20__041110765755\n",
            "\n",
            "Showing 36 in top-37 out of 37 total results.     -> 36 of 37/37\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 2) -> 2\n",
            "\n",
            "\n",
            "Document: 13|||| \n",
            "'arxiv_id': arXiv:2412.14190, \n",
            "'paper_link': https://arxiv.org/abs/2412.14190, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14190, \n",
            "Title: Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion's identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the \"new\" AI relative to the \"original\". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike.\n",
            "Score: 15\n",
            "\n",
            "Document: 44|||| \n",
            "'arxiv_id': arXiv:2412.14233, \n",
            "'paper_link': https://arxiv.org/abs/2412.14233, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14233, \n",
            "Title: Descriptive Caption Enhancement with Visual Specialists for Multimodal Perception \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Training Large Multimodality Models (LMMs) relies on descriptive image caption that connects image and language. Existing methods either distill the caption from the LMM models or construct the captions from the internet images or by human. We propose to leverage off-the-shelf visual specialists, which were trained from annotated images initially not for image captioning, for enhancing the image caption.\n",
            "Our approach, named DCE, explores object low-level and fine-grained attributes (e.g., depth, emotion and fine-grained categories) and object relations (e.g., relative location and human-object-interaction (HOI)), and combine the attributes into the descriptive caption. Experiments demonstrate that such visual specialists are able to improve the performance for visual understanding tasks as well as reasoning that benefits from more accurate visual understanding. We will release the source code and the pipeline so that other visual specialists are easily combined into the pipeline. The complete source code of DCE pipeline and datasets will be available at \\url{this https URL}.\n",
            "Score: 15\n",
            "\n",
            "Document: 52|||| \n",
            "'arxiv_id': arXiv:2412.14295, \n",
            "'paper_link': https://arxiv.org/abs/2412.14295, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14295, \n",
            "Title: Temporally Consistent Object-Centric Learning by Contrasting Slots \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Unsupervised object-centric learning from videos is a promising approach to extract structured representations from large, unlabeled collections of videos. To support downstream tasks like autonomous control, these representations must be both compositional and temporally consistent. Existing approaches based on recurrent processing often lack long-term stability across frames because their training objective does not enforce temporal consistency. In this work, we introduce a novel object-level temporal contrastive loss for video object-centric models that explicitly promotes temporal consistency. Our method significantly improves the temporal consistency of the learned object-centric representations, yielding more reliable video decompositions that facilitate challenging downstream tasks such as unsupervised object dynamics prediction. Furthermore, the inductive bias added by our loss strongly improves object discovery, leading to state-of-the-art results on both synthetic and real-world datasets, outperforming even weakly-supervised methods that leverage motion masks as additional cues.\n",
            "Score: 15\n",
            "\n",
            "Document: 101|||| \n",
            "'arxiv_id': arXiv:2412.14414, \n",
            "'paper_link': https://arxiv.org/abs/2412.14414, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14414, \n",
            "Title: In-Group Love, Out-Group Hate: A Framework to Measure Affective Polarization via Contentious Online Discussions \n",
            "Subjects: Social and Information Networks (cs.SI) \n",
            "Abstract: Affective polarization, the emotional divide between ideological groups marked by in-group love and out-group hate, has intensified in the United States, driving contentious issues like masking and lockdowns during the COVID-19 pandemic. Despite its societal impact, existing models of opinion change fail to account for emotional dynamics nor offer methods to quantify affective polarization robustly and in real-time. In this paper, we introduce a discrete choice model that captures decision-making within affectively polarized social networks and propose a statistical inference method estimate key parameters -- in-group love and out-group hate -- from social media data. Through empirical validation from online discussions about the COVID-19 pandemic, we demonstrate that our approach accurately captures real-world polarization dynamics and explains the rapid emergence of a partisan gap in attitudes towards masking and lockdowns. This framework allows for tracking affective polarization across contentious issues has broad implications for fostering constructive online dialogues in digital spaces.\n",
            "Score: 15\n",
            "\n",
            "Document: 184|||| \n",
            "'arxiv_id': arXiv:2412.14559, \n",
            "'paper_link': https://arxiv.org/abs/2412.14559, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14559, \n",
            "Title: ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The scaling law has been validated in various domains, such as natural language processing (NLP) and massive computer vision tasks; however, its application to motion generation remains largely unexplored. In this paper, we introduce a scalable motion generation framework that includes the motion tokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through comprehensive experiments, we observe the scaling behavior of this system. For the first time, we confirm the existence of scaling laws within the context of motion generation. Specifically, our results demonstrate that the normalized test loss of our prefix autoregressive models adheres to a logarithmic law in relation to compute budgets. Furthermore, we also confirm the power law between Non-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect to compute budgets respectively. Leveraging the scaling law, we predict the optimal transformer size, vocabulary size, and data requirements for a compute budget of $1e18$. The test loss of the system, when trained with the optimal model size, vocabulary size, and required data, aligns precisely with the predicted test loss, thereby validating the scaling law.\n",
            "Score: 15\n",
            "\n",
            "Document: 197|||| \n",
            "'arxiv_id': arXiv:2412.14584, \n",
            "'paper_link': https://arxiv.org/abs/2412.14584, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14584, \n",
            "Title: Simulation-Free Hierarchical Latent Policy Planning for Proactive Dialogues \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Recent advancements in proactive dialogues have garnered significant attention, particularly for more complex objectives (e.g. emotion support and persuasion). Unlike traditional task-oriented dialogues, proactive dialogues demand advanced policy planning and adaptability, requiring rich scenarios and comprehensive policy repositories to develop such systems. However, existing approaches tend to rely on Large Language Models (LLMs) for user simulation and online learning, leading to biases that diverge from realistic scenarios and result in suboptimal efficiency. Moreover, these methods depend on manually defined, context-independent, coarse-grained policies, which not only incur high expert costs but also raise concerns regarding their completeness. In our work, we highlight the potential for automatically discovering policies directly from raw, real-world dialogue records. To this end, we introduce a novel dialogue policy planning framework, LDPP. It fully automates the process from mining policies in dialogue records to learning policy planning. Specifically, we employ a variant of the Variational Autoencoder to discover fine-grained policies represented as latent vectors. After automatically annotating the data with these latent policy labels, we propose an Offline Hierarchical Reinforcement Learning (RL) algorithm in the latent space to develop effective policy planning capabilities. Our experiments demonstrate that LDPP outperforms existing methods on two proactive scenarios, even surpassing ChatGPT with only a 1.8-billion-parameter LLM.\n",
            "Score: 15\n",
            "\n",
            "Document: 260|||| \n",
            "'arxiv_id': arXiv:2412.14706, \n",
            "'paper_link': https://arxiv.org/abs/2412.14706, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14706, \n",
            "Title: EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Diffusion models, particularly latent diffusion models, have demonstrated remarkable success in text-driven human motion generation. However, it remains challenging for latent diffusion models to effectively compose multiple semantic concepts into a single, coherent motion sequence. To address this issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based Models: (1) We interpret the diffusion model as a latent-aware energy-based model that generates motions by composing a set of diffusion models in latent space; (2) We introduce a semantic-aware energy model based on cross-attention, which enables semantic composition and adaptive gradient descent for text embeddings. To overcome the challenges of semantic inconsistency and motion distortion across these two spectrums, we introduce Synergistic Energy Fusion. This design allows the motion latent diffusion model to synthesize high-quality, complex motions by combining multiple energy terms corresponding to textual descriptions. Experiments show that our approach outperforms existing state-of-the-art models on various motion generation tasks, including text-to-motion generation, compositional motion generation, and multi-concept motion generation. Additionally, we demonstrate that our method can be used to extend motion datasets and improve the text-to-motion task.\n",
            "Score: 15\n",
            "\n",
            "Document: 266|||| \n",
            "'arxiv_id': arXiv:2412.14719, \n",
            "'paper_link': https://arxiv.org/abs/2412.14719, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14719, \n",
            "Title: Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Micro-Action Recognition (MAR) has gained increasing attention due to its crucial role as a form of non-verbal communication in social interactions, with promising potential for applications in human communication and emotion analysis. However, current approaches often overlook the inherent ambiguity in micro-actions, which arises from the wide category range and subtle visual differences between categories. This oversight hampers the accuracy of micro-action recognition. In this paper, we propose a novel Prototypical Calibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the ambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to identify the ambiguous sample, categorizing them into distinct sets of ambiguous samples of false negatives and false positives, considering both body- and action-level categories. \\textbf{Secondly}, we implement an ambiguous contrastive refinement module to calibrate these ambiguous samples by regulating the distance between ambiguous samples and their corresponding prototypes. This calibration process aims to pull false negative ($\\mathbb{FN}$) samples closer to their respective prototypes and push false positive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In addition, we propose a new prototypical diversity amplification loss to strengthen the model's capacity by amplifying the differences between different prototypes. \\textbf{Finally}, we propose a prototype-guided rectification to rectify prediction by incorporating the representability of prototypes. Extensive experiments conducted on the benchmark dataset demonstrate the superior performance of our method compared to existing approaches. The code is available at this https URL.\n",
            "Score: 15\n",
            "\n",
            "Document: 343|||| \n",
            "'arxiv_id': arXiv:2412.14965, \n",
            "'paper_link': https://arxiv.org/abs/2412.14965, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14965, \n",
            "Title: Movie2Story: A framework for understanding videos and telling stories in the form of novel text \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Multimodal video-to-text models have made considerable progress, primarily in generating brief descriptions of video content. However, there is still a deficiency in generating rich long-form text descriptions that integrate both video and audio. In this paper, we introduce a framework called M2S, designed to generate novel-length text by combining audio, video, and character recognition. M2S includes modules for video long-form text description and comprehension, audio-based analysis of emotion, speech rate, and character alignment, and visual-based character recognition alignment. By integrating multimodal information using the large language model GPT4o, M2S stands out in the field of multimodal text generation. We demonstrate the effectiveness and accuracy of M2S through comparative experiments and human evaluation. Additionally, the model framework has good scalability and significant potential for future research.\n",
            "Score: 15\n",
            "\n",
            "Document: 351|||| \n",
            "'arxiv_id': arXiv:2412.14982, \n",
            "'paper_link': https://arxiv.org/abs/2412.14982, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14982, \n",
            "Title: Efficient Motion Sickness Assessment: Recreation of On-Road Driving on a Compact Test Track \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: The ability to engage in other activities during the ride is considered by consumers as one of the key reasons for the adoption of automated vehicles. However, engagement in non-driving activities will provoke occupants' motion sickness, deteriorating their overall comfort and thereby risking acceptance of automated driving. Therefore, it is critical to extend our understanding of motion sickness and unravel the modulating factors that affect it through experiments with participants. Currently, most experiments are conducted on public roads (realistic but not reproducible) or test tracks (feasible with prototype automated vehicles). This research study develops a method to design an optimal path and speed reference to efficiently replicate on-road motion sickness exposure on a small test track. The method uses model predictive control to replicate the longitudinal and lateral accelerations collected from on-road drives on a test track of 70 m by 175 m. A within-subject experiment (47 participants) was conducted comparing the occupants' motion sickness occurrence in test-track and on-road conditions, with the conditions being cross-randomized. The results illustrate no difference and no effect of the condition on the occurrence of the average motion sickness across the participants. Meanwhile, there is an overall correspondence of individual sickness levels between on-road and test-track. This paves the path for the employment of our method for a simpler, safer and more replicable assessment of motion sickness.\n",
            "Score: 15\n",
            "\n",
            "Document: 408|||| \n",
            "'arxiv_id': arXiv:2412.15152, \n",
            "'paper_link': https://arxiv.org/abs/2412.15152, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15152, \n",
            "Title: Measuring DNA Microswimmer Locomotion in Complex Flow Environments \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Microswimmers are sub-millimeter swimming microrobots that show potential as a platform for controllable locomotion in applications including targeted cargo delivery and minimally invasive surgery. To be viable for these target applications, microswimmers will eventually need to be able to navigate in environments with dynamic fluid flows and forces. Experimental studies with microswimmers towards this goal are currently rare because of the difficulty isolating intentional microswimmer motion from environment-induced motion. In this work, we present a method for measuring microswimmer locomotion within a complex flow environment using fiducial microspheres. By tracking the particle motion of ferromagnetic and non-magnetic polystyrene fiducial microspheres, we capture the effect of fluid flow and field gradients on microswimmer trajectories. We then determine the field-driven translation of these microswimmers relative to fluid flow and demonstrate the effectiveness of this method by illustrating the motion of multiple microswimmers through different flows.\n",
            "Score: 15\n",
            "\n",
            "Document: 423|||| \n",
            "'arxiv_id': arXiv:2412.15189, \n",
            "'paper_link': https://arxiv.org/abs/2412.15189, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15189, \n",
            "Title: Face the Facts! Evaluating RAG-based Fact-checking Pipelines in Realistic Settings \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Natural Language Processing and Generation systems have recently shown the potential to complement and streamline the costly and time-consuming job of professional fact-checkers. In this work, we lift several constraints of current state-of-the-art pipelines for automated fact-checking based on the Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under more realistic scenarios, RAG-based methods for the generation of verdicts - i.e., short texts discussing the veracity of a claim - evaluating them on stylistically complex claims and heterogeneous, yet reliable, knowledge bases. Our findings show a complex landscape, where, for example, LLM-based retrievers outperform other retrieval techniques, though they still struggle with heterogeneous knowledge bases; larger models excel in verdict faithfulness, while smaller models provide better context adherence, with human evaluations favouring zero-shot and one-shot approaches for informativeness, and fine-tuned models for emotional alignment.\n",
            "Score: 15\n",
            "\n",
            "Document: 526|||| \n",
            "'arxiv_id': arXiv:2403.15249, \n",
            "'paper_link': https://arxiv.org/abs/2403.15249, \n",
            "'pdf_link': https://arxiv.org/pdf/2403.15249, \n",
            "Title: Spectral Motion Alignment for Video Motion Transfer using Diffusion Models \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The evolution of diffusion models has greatly impacted video generation and understanding. Particularly, text-to-video diffusion models (VDMs) have significantly facilitated the customization of input video with target appearance, motion, etc. Despite these advances, challenges persist in accurately distilling motion information from video frames. While existing works leverage the consecutive frame residual as the target motion vector, they inherently lack global motion context and are vulnerable to frame-wise distortions. To address this, we present Spectral Motion Alignment (SMA), a novel framework that refines and aligns motion vectors using Fourier and wavelet transforms. SMA learns motion patterns by incorporating frequency-domain regularization, facilitating the learning of whole-frame global motion dynamics, and mitigating spatial artifacts. Extensive experiments demonstrate SMA's efficacy in improving motion transfer while maintaining computational efficiency and compatibility across various video customization frameworks.\n",
            "Score: 15\n",
            "\n",
            "Document: 565|||| \n",
            "'arxiv_id': arXiv:2406.17256, \n",
            "'paper_link': https://arxiv.org/abs/2406.17256, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.17256, \n",
            "Title: Disentangled Motion Modeling for Video Frame Interpolation \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Video Frame Interpolation (VFI) aims to synthesize intermediate frames between existing frames to enhance visual smoothness and quality. Beyond the conventional methods based on the reconstruction loss, recent works have employed generative models for improved perceptual quality. However, they require complex training and large computational costs for pixel space modeling. In this paper, we introduce disentangled Motion Modeling (MoMo), a diffusion-based approach for VFI that enhances visual quality by focusing on intermediate motion modeling. We propose a disentangled two-stage training process. In the initial stage, frame synthesis and flow models are trained to generate accurate frames and flows optimal for synthesis. In the subsequent stage, we introduce a motion diffusion model, which incorporates our novel U-Net architecture specifically designed for optical flow, to generate bi-directional flows between frames. By learning the simpler low-frequency representation of motions, MoMo achieves superior perceptual quality with reduced computational demands compared to the generative modeling methods on the pixel space. MoMo surpasses state-of-the-art methods in perceptual metrics across various benchmarks, demonstrating its efficacy and efficiency in VFI.\n",
            "Score: 15\n",
            "\n",
            "Document: 585|||| \n",
            "'arxiv_id': arXiv:2407.20041, \n",
            "'paper_link': https://arxiv.org/abs/2407.20041, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.20041, \n",
            "Title: Counterfactual rewards promote collective transport using individually controlled swarm microrobots \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Swarm robots offer fascinating opportunities to perform complex tasks beyond the capabilities of individual machines. Just as a swarm of ants collectively moves a large object, similar functions can emerge within a group of robots through individual strategies based on local sensing. However, realizing collective functions with individually controlled microrobots is particularly challenging due to their micrometer size, large number of degrees of freedom, strong thermal noise relative to the propulsion speed, complex physical coupling between neighboring microrobots, and surface collisions. Here, we implement Multi-Agent Reinforcement Learning (MARL) to generate a control strategy for up to 200 microrobots whose motions are individually controlled by laser spots. During the learning process, we employ so-called counterfactual rewards that automatically assign credit to the individual microrobots, which allows for fast and unbiased training. With the help of this efficient reward scheme, swarm microrobots learn to collectively transport a large cargo object to an arbitrary position and orientation, similar to ant swarms. We demonstrate that this flexible and versatile swarm robotic system is robust to variations in group size, the presence of malfunctioning units, and environmental noise. Such control strategies can potentially enable complex and automated assembly of mobile micromachines, programmable drug delivery capsules, and other advanced lab-on-a-chip applications.\n",
            "Score: 15\n",
            "\n",
            "Document: 641|||| \n",
            "'arxiv_id': arXiv:2410.21596, \n",
            "'paper_link': https://arxiv.org/abs/2410.21596, \n",
            "'pdf_link': https://arxiv.org/pdf/2410.21596, \n",
            "Title: Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Companion chatbots offer a potential solution to the growing epidemic of loneliness, but their impact on users' psychosocial well-being remains poorly understood. This study presents a large-scale survey (n = 404) of regular users of companion chatbots, investigating the relationship between chatbot usage and loneliness. We develop a model explaining approximately 50% of variance in loneliness; while usage does not directly predict loneliness, we identify factors including neuroticism, social network size, and problematic use. We identify seven distinct clusters of users, from socially fulfilled dependent users to lonely moderate users. Different usage patterns can lead to markedly different outcomes, with some users experiencing enhanced social confidence while others risk further isolation. Our work contributes to the ongoing dialogue about the role of AI in social and emotional support, offering insights for developing more targeted and ethical approaches to AI companionship that complement rather than replace human connections.\n",
            "Score: 15\n",
            "\n",
            "Document: 677|||| \n",
            "'arxiv_id': arXiv:2412.07160, \n",
            "'paper_link': https://arxiv.org/abs/2412.07160, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.07160, \n",
            "Title: Motion-aware Contrastive Learning for Temporal Panoptic Scene Graph Generation \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: To equip artificial intelligence with a comprehensive understanding towards a temporal world, video and 4D panoptic scene graph generation abstracts visual data into nodes to represent entities and edges to capture temporal relations. Existing methods encode entity masks tracked across temporal dimensions (mask tubes), then predict their relations with temporal pooling operation, which does not fully utilize the motion indicative of the entities' relation. To overcome this limitation, we introduce a contrastive representation learning framework that focuses on motion pattern for temporal scene graph generation. Firstly, our framework encourages the model to learn close representations for mask tubes of similar subject-relation-object triplets. Secondly, we seek to push apart mask tubes from their temporally shuffled versions. Moreover, we also learn distant representations for mask tubes belonging to the same video but different triplets. Extensive experiments show that our motion-aware contrastive framework significantly improves state-of-the-art methods on both video and 4D datasets.\n",
            "Score: 15\n",
            "\n",
            "Document: 694|||| \n",
            "'arxiv_id': arXiv:2412.09545, \n",
            "'paper_link': https://arxiv.org/abs/2412.09545, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.09545, \n",
            "Title: SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: We introduce SimAvatar, a framework designed to generate simulation-ready clothed 3D human avatars from a text prompt. Current text-driven human avatar generation methods either model hair, clothing, and the human body using a unified geometry or produce hair and garments that are not easily adaptable for simulation within existing simulation pipelines. The primary challenge lies in representing the hair and garment geometry in a way that allows leveraging established prior knowledge from foundational image diffusion models (e.g., Stable Diffusion) while being simulation-ready using either physics or neural simulators. To address this task, we propose a two-stage framework that combines the flexibility of 3D Gaussians with simulation-ready hair strands and garment meshes. Specifically, we first employ three text-conditioned 3D generative models to generate garment mesh, body shape and hair strands from the given text prompt. To leverage prior knowledge from foundational diffusion models, we attach 3D Gaussians to the body mesh, garment mesh, as well as hair strands and learn the avatar appearance through optimization. To drive the avatar given a pose sequence, we first apply physics simulators onto the garment meshes and hair strands. We then transfer the motion onto 3D Gaussians through carefully designed mechanisms for each body part. As a result, our synthesized avatars have vivid texture and realistic dynamic motion. To the best of our knowledge, our method is the first to produce highly realistic, fully simulation-ready 3D avatars, surpassing the capabilities of current approaches.\n",
            "Score: 15\n",
            "\n",
            "Document: 737|||| \n",
            "'arxiv_id': arXiv:2412.14118, \n",
            "'paper_link': https://arxiv.org/abs/2412.14118, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14118, \n",
            "Title: GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for Efficient Multi-Frame Interpolation in DSA Images \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The rapid and accurate direct multi-frame interpolation method for Digital Subtraction Angiography (DSA) images is crucial for reducing radiation and providing real-time assistance to physicians for precise diagnostics and treatment. DSA images contain complex vascular structures and various motions. Applying natural scene Video Frame Interpolation (VFI) methods results in motion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA has specifically addressed these issues for the first time and achieved SOTA results. However, MoSt-DSA's focus on real-time performance leads to insufficient suppression of high-frequency noise and incomplete filtering of low-frequency noise in the generated images. To address these issues within the same computational time scale, we propose GaraMoSt. Specifically, we optimize the network pipeline with a parallel design and propose a module named MG-MSFE. MG-MSFE extracts frame-relative motion and structural features at various granularities in a fully convolutional parallel manner and supports independent, flexible adjustment of context-aware granularity at different scales, thus enhancing computational efficiency and accuracy. Extensive experiments demonstrate that GaraMoSt achieves the SOTA performance in accuracy, robustness, visual effects, and noise suppression, comprehensively surpassing MoSt-DSA and other natural scene VFI methods. The code and models are available at this https URL.\n",
            "Score: 15\n",
            "\n",
            "Document: 757|||| \n",
            "'arxiv_id': arXiv:2404.01172, \n",
            "'paper_link': https://arxiv.org/abs/2404.01172, \n",
            "'pdf_link': https://arxiv.org/pdf/2404.01172, \n",
            "Title: Covering convection with thermal blankets: fluid-structure interactions in thermal convection \n",
            "Subjects: Fluid Dynamics (physics.flu-dyn) \n",
            "Abstract: The continental plates of Earth are known to drift over a geophysical timescale, and their interactions have lead to some of the most spectacular geoformations of our planet while also causing natural disasters such as earthquakes and volcanic activity. Understanding the dynamics of interacting continental plates is thus significant. In this work, we present a fluid mechanical investigation of the plate motion, interaction, and dynamics. Through numerical experiments, we examine the coupling between a convective fluid and plates floating on top of it. With physical modeling, we show the coupling is both mechanical and thermal, leading to the thermal blanket effect: the floating plate is not only transported by the fluid flow beneath, it also prevents the heat from leaving the fluid, leading to a convective flow that further affects the plate motion. By adding several plates to such a coupled fluid-structure interaction, we also investigate how floating plates interact with each other and show that, under proper conditions, small plates can converge into a supercontinent.\n",
            "Score: 15\n",
            "\n",
            "Document: 283|||| \n",
            "'arxiv_id': arXiv:2412.14769, \n",
            "'paper_link': https://arxiv.org/abs/2412.14769, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14769, \n",
            "Title: PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in Left-Behind Children \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Left-behind children (LBCs), numbering over 66 million in China, face severe mental health challenges due to parental migration for work. Early screening and identification of at-risk LBCs is crucial, yet challenging due to the severe shortage of mental health professionals, especially in rural areas. While the House-Tree-Person (HTP) test shows higher child participation rates, its requirement for expert interpretation limits its application in resource-scarce regions. To address this challenge, we propose PsyDraw, a multi-agent system based on Multimodal Large Language Models that assists mental health professionals in analyzing HTP drawings. The system employs specialized agents for feature extraction and psychological interpretation, operating in two stages: comprehensive feature analysis and professional report generation. Evaluation of HTP drawings from 290 primary school students reveals that 71.03% of the analyzes achieved High Consistency with professional evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The system identified 31.03% of cases requiring professional attention, demonstrating its effectiveness as a preliminary screening tool. Currently deployed in pilot schools, \\method shows promise in supporting mental health professionals, particularly in resource-limited areas, while maintaining high professional standards in psychological assessment.\n",
            "Score: 9\n",
            "\n",
            "Document: 16|||| \n",
            "'arxiv_id': arXiv:2412.14194, \n",
            "'paper_link': https://arxiv.org/abs/2412.14194, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14194, \n",
            "Title: Detecting Cognitive Impairment and Psychological Well-being among Older Adults Using Facial, Acoustic, Linguistic, and Cardiovascular Patterns Derived from Remote Conversations \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: INTRODUCTION: The aging society urgently requires scalable methods to monitor cognitive decline and identify social and psychological factors indicative of dementia risk in older adults. METHODS: Our machine learning models captured facial, acoustic, linguistic, and cardiovascular features from 39 individuals with normal cognition or Mild Cognitive Impairment derived from remote video conversations and classified cognitive status, social isolation, neuroticism, and psychological well-being. RESULTS: Our model could distinguish Clinical Dementia Rating Scale of 0.5 (vs. 0) with 0.78 area under the receiver operating characteristic curve (AUC), social isolation with 0.75 AUC, neuroticism with 0.71 AUC, and negative affect scales with 0.79 AUC. DISCUSSION: Our findings demonstrate the feasibility of remotely monitoring cognitive status, social isolation, neuroticism, and psychological well-being. Speech and language patterns were more useful for quantifying cognitive impairment, whereas facial expression and cardiovascular patterns using remote photoplethysmography were more useful for quantifying personality and psychological well-being.\n",
            "Score: 5\n",
            "\n",
            "Document: 402|||| \n",
            "'arxiv_id': arXiv:2412.15137, \n",
            "'paper_link': https://arxiv.org/abs/2412.15137, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15137, \n",
            "Title: Hydrogen in Aviation: Evaluating the Feasibility and Benefits of a Green Fuel Alternative \n",
            "Subjects: Systems and Control (eess.SY) \n",
            "Abstract: Growing concerns regarding environmental health have highlighted the aviation industry's impact and potential mitigation strategies. Previous research has indicated hydrogen's significant potential for reducing the industry's environmental impact, yet implementation challenges remain. Through analysis of light aircraft and military applications, we demonstrate that hydrogen-based systems can achieve performance metrics approaching those of traditional fuels while reducing emissions by up to 74.7%. Our findings show that hydrogen's superior energy-to-mass ratio (120 MJ/kg versus 43 MJ/kg for jet fuel) makes it particularly advantageous for aviation applications compared to battery-electric alternatives. Primary implementation challenges involve cryogenic storage systems (-253C), tank placement optimization, and fueling infrastructure development. The observed efficiency penalties of only 2.23% in military applications suggest hydrogen's viability as a sustainable aviation fuel alternative.\n",
            "Score: 5\n",
            "\n",
            "Document: 593|||| \n",
            "'arxiv_id': arXiv:2408.06602, \n",
            "'paper_link': https://arxiv.org/abs/2408.06602, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.06602, \n",
            "Title: Super-intelligence or Superstition? Exploring Psychological Factors Influencing Belief in AI Predictions about Personal Behavior \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Could belief in AI predictions be just another form of superstition? This study investigates psychological factors that influence belief in AI predictions about personal behavior, comparing it to belief in astrology- and personality-based predictions. Through an experiment with 238 participants, we examined how cognitive style, paranormal beliefs, AI attitudes, personality traits, and other factors affect perceived validity, reliability, usefulness, and personalization of predictions from different sources. Our findings reveal that belief in AI predictions is positively correlated with belief in predictions based on astrology and personality psychology. Notably, paranormal beliefs and positive attitudes about AI significantly increased perceived validity, reliability, usefulness, and personalization of AI predictions. Conscientiousness was negatively correlated with belief in predictions across all sources, and interest in the prediction topic increased believability across predictions. Surprisingly, we found no evidence that cognitive style has an impact on belief in fictitious AI-generated predictions. These results highlight the \"rational superstition\" phenomenon in AI, where belief is driven more by mental heuristics and intuition than critical evaluation. This research advances our understanding of the psychology of human-AI interaction, offering insights into designing and promoting AI systems that foster appropriate trust and skepticism, critical for responsible integration in an increasingly AI-driven world.\n",
            "Score: 5\n",
            "\n",
            "Document: 668|||| \n",
            "'arxiv_id': arXiv:2412.01353, \n",
            "'paper_link': https://arxiv.org/abs/2412.01353, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.01353, \n",
            "Title: Su-RoBERTa: A Semi-supervised Approach to Predicting Suicide Risk through Social Media using Base Language Models \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: In recent times, more and more people are posting about their mental states across various social media platforms. Leveraging this data, AI-based systems can be developed that help in assessing the mental health of individuals, such as suicide risk. This paper is a study done on suicidal risk assessments using Reddit data leveraging Base language models to identify patterns from social media posts. We have demonstrated that using smaller language models, i.e., less than 500M parameters, can also be effective in contrast to LLMs with greater than 500M parameters. We propose Su-RoBERTa, a fine-tuned RoBERTa on suicide risk prediction task that utilized both the labeled and unlabeled Reddit data and tackled class imbalance by data augmentation using GPT-2 model. Our Su-RoBERTa model attained a 69.84% weighted F1 score during the Final evaluation. This paper demonstrates the effectiveness of Base language models for the analysis of the risk factors related to mental health with an efficient computation pipeline\n",
            "Score: 5\n",
            "\n",
            "Document: 735|||| \n",
            "'arxiv_id': arXiv:2412.14011, \n",
            "'paper_link': https://arxiv.org/abs/2412.14011, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14011, \n",
            "Title: Towards an optimised evaluation of teachers' discourse: The case of engaging messages \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Evaluating teachers' skills is crucial for enhancing education quality and student outcomes. Teacher discourse, significantly influencing student performance, is a key component. However, coding this discourse can be laborious. This study addresses this issue by introducing a new methodology for optimising the assessment of teacher discourse. The research consisted of two studies, both within the framework of engaging messages used by secondary education teachers. The first study involved training two large language models on real-world examples from audio-recorded lessons over two academic years to identify and classify the engaging messages from the lessons' transcripts. This resulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and 86.36% in identification and classification, respectively. The second study applied these models to transcripts of audio-recorded lessons from a third academic year to examine the frequency and distribution of message types by educational level and moment of the academic year. Results showed teachers predominantly use messages emphasising engagement benefits, linked to improved outcomes, while one-third highlighted non-engagement disadvantages, associated with increased anxiety. The use of engaging messages declined in Grade 12 and towards the academic year's end. These findings suggest potential interventions to optimise engaging message use, enhancing teaching quality and student outcomes.\n",
            "Score: 5\n",
            "\n",
            "Document: 148|||| \n",
            "'arxiv_id': arXiv:2412.14491, \n",
            "'paper_link': https://arxiv.org/abs/2412.14491, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14491, \n",
            "Title: Mediation Analysis for Probabilities of Causation \n",
            "Subjects: Artificial Intelligence (cs.AI) \n",
            "Abstract: Probabilities of causation (PoC) offer valuable insights for informed decision-making. This paper introduces novel variants of PoC-controlled direct, natural direct, and natural indirect probability of necessity and sufficiency (PNS). These metrics quantify the necessity and sufficiency of a treatment for producing an outcome, accounting for different causal pathways. We develop identification theorems for these new PoC measures, allowing for their estimation from observational data. We demonstrate the practical application of our results through an analysis of a real-world psychology dataset.\n",
            "Score: 2\n",
            "\n",
            "Document: 167|||| \n",
            "'arxiv_id': arXiv:2412.14522, \n",
            "'paper_link': https://arxiv.org/abs/2412.14522, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14522, \n",
            "Title: CAE-T: A Channelwise AutoEncoder with Transformer for EEG Abnormality Detection \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Electroencephalogram (EEG) signals are critical for detecting abnormal brain activity, but their high dimensionality and complexity pose significant challenges for effective analysis. In this paper, we propose CAE-T, a novel framework that combines a channelwise CNN-based autoencoder with a single-head transformer classifier for efficient EEG abnormality detection. The channelwise autoencoder compresses raw EEG signals while preserving channel independence, reducing computational costs and retaining biologically meaningful features. The compressed representations are then fed into the transformer-based classifier, which efficiently models long-term dependencies to distinguish between normal and abnormal signals. Evaluated on the TUH Abnormal EEG Corpus, the proposed model achieves 85.0% accuracy, 76.2% sensitivity, and 91.2% specificity at the per-case level, outperforming baseline models such as EEGNet, Deep4Conv, and FusionCNN. Furthermore, CAE-T requires only 202M FLOPs and 2.9M parameters, making it significantly more efficient than transformer-based alternatives. The framework retains interpretability through its channelwise design, demonstrating great potential for future applications in neuroscience research and clinical practice. The source code is available at this https URL.\n",
            "Score: 2\n",
            "\n",
            "Document: 225|||| \n",
            "'arxiv_id': arXiv:2412.14638, \n",
            "'paper_link': https://arxiv.org/abs/2412.14638, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14638, \n",
            "Title: TuneS: Patient-specific model-based optimization of contact configuration in deep brain stimulation \n",
            "Subjects: Systems and Control (eess.SY) \n",
            "Abstract: Objective: The objective of this study is to develop and evaluate a systematic approach to optimize Deep Brain Stimulation (DBS) parameters, addressing the challenge of identifying patient-specific settings and optimal stimulation targets for various neurological and mental disorders. Methods: TuneS, a novel pipeline to predict clinically optimal DBS contact configurations based on predefined targets and constraints, is introduced. The method relies upon patient-specific models of stimulation spread and extends optimization beyond traditional neural structures to include automated, model-based targeting of streamlines. Results: Initial findings demonstrate that STN motor streamlines consistently receive a significant portion of the allocated stimulation volume, suggesting that a consistent portion of the stimulation should ideally focus on the STN motor streamlines. At the example of a small cohort of Parkinson's disease patients, the value of model-based contact predictions for assessing stimulation targets while observing constraints is demonstrated. Conclusion: TuneS shows promise as a research tool, enabling systematic assessment of DBS target effectiveness and facilitating constraint-aware optimization of stimulation parameters. Significance: The presented pipeline offers a pathway to improve patient-specific DBS therapies and contributes to the broader understanding of effective DBS targeting strategies.\n",
            "Score: 2\n",
            "\n",
            "Document: 328|||| \n",
            "'arxiv_id': arXiv:2412.14902, \n",
            "'paper_link': https://arxiv.org/abs/2412.14902, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.14902, \n",
            "Title: MagicNaming: Consistent Identity Generation by Finding a \"Name Space\" in T2I Diffusion Models \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable of generating famous persons by simply referring to their names. Is it possible to make such models generate generic identities as simple as the famous ones, e.g., just use a name? In this paper, we explore the existence of a \"Name Space\", where any point in the space corresponds to a specific identity. Fortunately, we find some clues in the feature space spanned by text embedding of celebrities' names. Specifically, we first extract the embeddings of celebrities' names in the Laion5B dataset with the text encoder of diffusion models. Such embeddings are used as supervision to learn an encoder that can predict the name (actually an embedding) of a given face image. We experimentally find that such name embeddings work well in promising the generated image with good identity consistency. Note that like the names of celebrities, our predicted name embeddings are disentangled from the semantics of text inputs, making the original generation capability of text-to-image models well-preserved. Moreover, by simply plugging such name embeddings, all variants (e.g., from Civitai) derived from the same base model (i.e., SDXL) readily become identity-aware text-to-image models. Project homepage: \\url{this https URL}.\n",
            "Score: 2\n",
            "\n",
            "Document: 386|||| \n",
            "'arxiv_id': arXiv:2412.15098, \n",
            "'paper_link': https://arxiv.org/abs/2412.15098, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15098, \n",
            "Title: A Cross-Domain Study of the Use of Persuasion Techniques in Online Disinformation \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Disinformation, irrespective of domain or language, aims to deceive or manipulate public opinion, typically through employing advanced persuasion techniques. Qualitative and quantitative research on the weaponisation of persuasion techniques in disinformation has been mostly topic-specific (e.g., COVID-19) with limited cross-domain studies, resulting in a lack of comprehensive understanding of these strategies. This study employs a state-of-the-art persuasion technique classifier to conduct a large-scale, multi-domain analysis of the role of 16 persuasion techniques in disinformation narratives. It shows how different persuasion techniques are employed disproportionately in different disinformation domains. We also include a detailed case study on climate change disinformation, highlighting how linguistic, psychological, and cultural factors shape the adaptation of persuasion strategies to fit unique thematic contexts.\n",
            "Score: 2\n",
            "\n",
            "Document: 390|||| \n",
            "'arxiv_id': arXiv:2412.15113, \n",
            "'paper_link': https://arxiv.org/abs/2412.15113, \n",
            "'pdf_link': https://arxiv.org/pdf/2412.15113, \n",
            "Title: Associative memory inspires improvements for in-context learning using a novel attention residual stream architecture \n",
            "Subjects: Neural and Evolutionary Computing (cs.NE) \n",
            "Abstract: Large language models (LLMs) demonstrate an impressive ability to utilise information within the context of their input sequences to appropriately respond to data unseen by the LLM during its training procedure. This ability is known as in-context learning (ICL). Humans and non-human animals demonstrate similar abilities, however their neural architectures differ substantially from LLMs. Despite this, a critical component within LLMs, the attention mechanism, resembles modern associative memory models, widely used in and influenced by the computational neuroscience community to model biological memory systems. Using this connection, we introduce an associative memory model capable of performing ICL. We use this as inspiration for a novel residual stream architecture which allows information to directly flow between attention heads. We test this architecture during training within a two-layer Transformer and show its ICL abilities manifest more quickly than without this modification. We then apply our architecture in small language models with 8 million parameters, focusing on attention head values, with results also indicating improved ICL performance at this larger and more naturalistic scale.\n",
            "Score: 2\n",
            "\n",
            "Document: 500|||| \n",
            "'arxiv_id': arXiv:2311.04686, \n",
            "'paper_link': https://arxiv.org/abs/2311.04686, \n",
            "'pdf_link': https://arxiv.org/pdf/2311.04686, \n",
            "Title: Robust and Communication-Efficient Federated Domain Adaptation via Random Features \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Modern machine learning (ML) models have grown to a scale where training them on a single machine becomes impractical. As a result, there is a growing trend to leverage federated learning (FL) techniques to train large ML models in a distributed and collaborative manner. These models, however, when deployed on new devices, might struggle to generalize well due to domain shifts. In this context, federated domain adaptation (FDA) emerges as a powerful approach to address this challenge.\n",
            "Most existing FDA approaches typically focus on aligning the distributions between source and target domains by minimizing their (e.g., MMD) distance. Such strategies, however, inevitably introduce high communication overheads and can be highly sensitive to network reliability.\n",
            "In this paper, we introduce RF-TCA, an enhancement to the standard Transfer Component Analysis approach that significantly accelerates computation without compromising theoretical and empirical performance. Leveraging the computational advantage of RF-TCA, we further extend it to FDA setting with FedRF-TCA. The proposed FedRF-TCA protocol boasts communication complexity that is independent of the sample size, while maintaining performance that is either comparable to or even surpasses state-of-the-art FDA methods. We present extensive experiments to showcase the superior performance and robustness (to network condition) of FedRF-TCA.\n",
            "Score: 2\n",
            "\n",
            "Document: 756|||| \n",
            "'arxiv_id': arXiv:2403.15031, \n",
            "'paper_link': https://arxiv.org/abs/2403.15031, \n",
            "'pdf_link': https://arxiv.org/pdf/2403.15031, \n",
            "Title: Image Classification with Rotation-Invariant Variational Quantum Circuits \n",
            "Subjects: Quantum Physics (quant-ph) \n",
            "Abstract: Variational quantum algorithms are gaining attention as an early application of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of variational methods lies in the phenomenon of Barren Plateaus, present in the optimization of variational parameters. Adding geometric inductive bias to the quantum models has been proposed as a potential solution to mitigate this problem, leading to a new field called Geometric Quantum Machine Learning. In this work, an equivariant architecture for variational quantum classifiers is introduced to create a label-invariant model for image classification with $C_4$ rotational label symmetry. The equivariant circuit is benchmarked against two different architectures, and it is experimentally observed that the geometric approach boosts the model's performance. Finally, a classical equivariant convolution operation is proposed to extend the quantum model for the processing of larger images, employing the resources available in NISQ devices.\n",
            "Score: 2\n",
            "\n",
            "Document: 765|||| \n",
            "'arxiv_id': arXiv:2407.13012, \n",
            "'paper_link': https://arxiv.org/abs/2407.13012, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.13012, \n",
            "Title: CUAOA: A Novel CUDA-Accelerated Simulation Framework for the QAOA \n",
            "Subjects: Quantum Physics (quant-ph) \n",
            "Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a prominent quantum algorithm designed to find approximate solutions to combinatorial optimization problems, which are challenging for classical computers. In the current era, where quantum hardware is constrained by noise and limited qubit availability, simulating the QAOA remains essential for research. However, existing state-of-the-art simulation frameworks suffer from long execution times or lack comprehensive functionality, usability, and versatility, often requiring users to implement essential features themselves. Additionally, these frameworks are primarily restricted to Python, limiting their use in safer and faster languages like Rust, which offer, e.g., advanced parallelization capabilities. In this paper, we develop a GPU accelerated QAOA simulation framework utilizing the NVIDIA CUDA toolkit. This framework offers a complete interface for QAOA simulations, enabling the calculation of (exact) expectation values, direct access to the statevector, fast sampling, and high-performance optimization methods using an advanced state-of-the-art gradient calculation technique. The framework is designed for use in Python and Rust, providing flexibility for integration into a wide range of applications, including those requiring fast algorithm implementations leveraging QAOA at its core. The new framework's performance is rigorously benchmarked on the MaxCut problem and compared against the current state-of-the-art general-purpose quantum circuit simulation frameworks Qiskit and Pennylane as well as the specialized QAOA simulation tool QOKit. Our evaluation shows that our approach outperforms the existing state-of-the-art solutions in terms of runtime up to multiple orders of magnitude. Our implementation is publicly available at this https URL and Zenodo.\n",
            "Score: 2\n",
            "\n",
            "Document: 771|||| \n",
            "'arxiv_id': arXiv:2411.06822, \n",
            "'paper_link': https://arxiv.org/abs/2411.06822, \n",
            "'pdf_link': https://arxiv.org/pdf/2411.06822, \n",
            "Title: Efficient Classical Computation of Single-Qubit Marginal Measurement Probabilities to Simulate Certain Classes of Quantum Algorithms \n",
            "Subjects: Quantum Physics (quant-ph) \n",
            "Abstract: Classical simulations of quantum circuits are essential for verifying and benchmarking quantum algorithms, particularly for large circuits, where computational demands increase exponentially with the number of qubits. Among available methods, the classical simulation of quantum circuits inspired by density functional theory -- the so-called QC-DFT method, shows promise for large circuit simulations as it approximates the quantum circuits using single-qubit reduced density matrices to model multi-qubit systems. However, the QC-DFT method performs very poorly when dealing with multi-qubit gates. In this work, we introduce a novel CNOT \"functional\" that leverages neural networks to generate unitary transformations, effectively mitigating the simulation errors observed in the original QC-DFT method. For random circuit simulations, our modified QC-DFT enables efficient computation of single-qubit marginal measurement probabilities, or single-qubit probability (SQPs), and achieves lower SQP errors and higher fidelities than the original QC-DFT method. Despite some limitations in capturing full entanglement and joint probability distributions, we find potential applications of SQPs in simulating Shor's and Grover's algorithms for specific solution classes. These findings advance the capabilities of classical simulations for some quantum problems and provide insights into managing entanglement and gate errors in practical quantum computing.\n",
            "Score: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Timer"
      ],
      "metadata": {
        "id": "BBVy06WenHk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_time_whole_single_task = datetime.now()\n",
        "duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "print(f\"Duration to run -> {duration_time}\")"
      ],
      "metadata": {
        "id": "cCnRP_8anHbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66ac62b8-d8ff-4ae8-875b-1580a123303e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run -> 0_min__4.2_sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See files\n",
        "print(\"List of results saved:\")\n",
        "!ls\n",
        "print(f\"All Articles-Found Results Count = {len(all_results_json_list)}\")"
      ],
      "metadata": {
        "id": "B7IHdEI-v2jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00fa2924-47f1-4312-9eaa-cd173f15954e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of results saved:\n",
            "'Agents for Software Engineering_articles2024-12-20__022542279803.html'\n",
            "'Agents for Software Engineering_articles_2024-12-20__022542279803.json'\n",
            "'Agents for Software Engineering_articles2024-12-20__041110132006.html'\n",
            "'Agents for Software Engineering_articles_2024-12-20__041110132006.json'\n",
            " all_arxiv_article_dicts_2024-12-20__022539101651.json\n",
            " all_arxiv_article_dicts_2024-12-20__041109292906.json\n",
            " all_arxiv_results_2024-12-20__022539321644.json\n",
            " all_arxiv_results_2024-12-20__041109354862.json\n",
            "'collective behavior_articles2024-12-20__022542564102.html'\n",
            "'collective behavior_articles_2024-12-20__022542564102.json'\n",
            "'collective behavior_articles2024-12-20__041110389839.html'\n",
            "'collective behavior_articles_2024-12-20__041110389839.json'\n",
            " disinformation_articles2024-12-20__022541239886.html\n",
            " disinformation_articles_2024-12-20__022541239886.json\n",
            " disinformation_articles2024-12-20__041109825584.html\n",
            " disinformation_articles_2024-12-20__041109825584.json\n",
            "'distance measure_articles2024-12-20__022540144275.html'\n",
            "'distance measure_articles_2024-12-20__022540144275.json'\n",
            "'distance measure_articles2024-12-20__041109563051.html'\n",
            "'distance measure_articles_2024-12-20__041109563051.json'\n",
            " e-Learners_articles2024-12-20__022542412022.html\n",
            " e-Learners_articles_2024-12-20__022542412022.json\n",
            " e-Learners_articles2024-12-20__041110245885.html\n",
            " e-Learners_articles_2024-12-20__041110245885.json\n",
            "'Manifold Approximation_articles2024-12-20__022539752492.html'\n",
            "'Manifold Approximation_articles_2024-12-20__022539752492.json'\n",
            "'Manifold Approximation_articles2024-12-20__041109466128.html'\n",
            "'Manifold Approximation_articles_2024-12-20__041109466128.json'\n",
            "'manifold hypothesis_articles2024-12-20__022542787397.html'\n",
            "'manifold hypothesis_articles_2024-12-20__022542787397.json'\n",
            "'manifold hypothesis_articles2024-12-20__041110611352.html'\n",
            "'manifold hypothesis_articles_2024-12-20__041110611352.json'\n",
            "'mental health_articles2024-12-20__022543093186.html'\n",
            "'mental health_articles_2024-12-20__022543093186.json'\n",
            "'mental health_articles2024-12-20__041110892939.html'\n",
            "'mental health_articles_2024-12-20__041110892939.json'\n",
            "'multiple agents_articles2024-12-20__022542036737.html'\n",
            "'multiple agents_articles_2024-12-20__022542036737.json'\n",
            "'multiple agents_articles2024-12-20__041110018289.html'\n",
            "'multiple agents_articles_2024-12-20__041110018289.json'\n",
            " parametric_articles2024-12-20__022540305541.html\n",
            " parametric_articles_2024-12-20__022540305541.json\n",
            " parametric_articles2024-12-20__041109630164.html\n",
            " parametric_articles_2024-12-20__041109630164.json\n",
            "'Retrieval-Augmented Systems_articles2024-12-20__022542686223.html'\n",
            "'Retrieval-Augmented Systems_articles_2024-12-20__022542686223.json'\n",
            "'Retrieval-Augmented Systems_articles2024-12-20__041110510569.html'\n",
            "'Retrieval-Augmented Systems_articles_2024-12-20__041110510569.json'\n",
            " sample_data\n",
            "'sentiment analysis_articles2024-12-20__022542914955.html'\n",
            "'sentiment analysis_articles_2024-12-20__022542914955.json'\n",
            "'sentiment analysis_articles2024-12-20__041110729412.html'\n",
            "'sentiment analysis_articles_2024-12-20__041110729412.json'\n",
            " Speech-LLM_articles2024-12-20__022541772777.html\n",
            " Speech-LLM_articles_2024-12-20__022541772777.json\n",
            " Speech-LLM_articles2024-12-20__041109930347.html\n",
            " Speech-LLM_articles_2024-12-20__041109930347.json\n",
            " survey_articles2024-12-20__022540789334.html\n",
            " survey_articles_2024-12-20__022540789334.json\n",
            " survey_articles2024-12-20__041109724838.html\n",
            " survey_articles_2024-12-20__041109724838.json\n",
            "All Articles-Found Results Count = 630\n"
          ]
        }
      ]
    }
  ]
}