{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ba77c8fe-bdc2-4e48-91f2-a942055118eb",
        "SOMfhOwlr-zu",
        "YepU-A4Fr_J3"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba77c8fe-bdc2-4e48-91f2-a942055118eb"
      },
      "source": [
        "# Arxiv Explorer Tools - minimal weighted match\n",
        "- Fast: ~5-10 sec to run vs. 5-10 min for embedding or TFIDF versions.\n",
        "- multi-topic: use as many pre-set seaches as you want\n",
        "- extracts articles on topics of interest from the too-many-to-look-through daily pages of articles that come out each day.\n",
        "- saves results to json (for automation later) and html (for easy reading and linking)\n",
        "- minimal weighted match uses a list of phrases and an integer weight for each\n",
        "- arxiv reading uses 'beautiful soup'\n",
        "\n",
        "### Setup & Install:\n",
        "- have python installed and use an python env\n",
        "- use a jupyter notebook or script, etc.\n",
        "- for specialty topics you can create extensive weighted search profiles.\n",
        "\n",
        "### See:\n",
        "- https://medium.com/@GeoffreyGordonAshbrook/search-with-non-generative-ai-d0a3cc77164b\n",
        "- https://github.com/lineality/arxiv_explorer_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11e7a29-5a13-4c90-b3a3-f4409a9013b2"
      },
      "source": [
        "\n",
        "- https://pypi.org/project/beautifulsoup4/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfdea8fa-7a5d-4d32-a88b-1b1f8619e1b3"
      },
      "source": [
        "requirements.txt ->\n",
        "```\n",
        "scikit-learn\n",
        "scipy\n",
        "numpy\n",
        "beautifulsoup4\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e4c5c9be-949c-4c72-b2cf-b26df5316aa2"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "\n",
        "\n",
        "def duration_min_sec(start_time, end_time):\n",
        "\n",
        "    duration = end_time - start_time\n",
        "\n",
        "    duration_seconds = duration.total_seconds()\n",
        "\n",
        "    minutes = int(duration_seconds // 60)\n",
        "    seconds = duration_seconds % 60\n",
        "    time_message = f\"{minutes}_min__{seconds:.1f}_sec\"\n",
        "\n",
        "    return time_message\n",
        "\n",
        "# # start_time_whole_single_task = datetime.now()\n",
        "# end_time_whole_single_task = datetime.now()\n",
        "# duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "# print(f\"Duration to run -> {duration_time}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# minimal weighted matching code"
      ],
      "metadata": {
        "id": "SOMfhOwlr-zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import math\n",
        "# from collections import Counter\n",
        "\n",
        "\n",
        "# And an even more simplistic basic key word search (with optional weights)\n",
        "\n",
        "import re\n",
        "\n",
        "def rank_documents_on_weighted_matches(documents, keyword_weights):\n",
        "    \"\"\"\n",
        "    Ranks documents based on the presence of weighted keywords-phrases.\n",
        "    comparison looks at text without:\n",
        "    - captialization\n",
        "    - spaces\n",
        "    - newlines\n",
        "    - special symbols\n",
        "\n",
        "    Parameters:\n",
        "    documents (list of str): The list of documents to be ranked.\n",
        "    keyword_weights (list of tuple): A list of tuples, where the first element is the keyword and the\n",
        "    second element is the corresponding weight.\n",
        "\n",
        "    Returns:\n",
        "    list of (str, float): A list of tuples, where the first element is the document and the\n",
        "    second element is the ranking score.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    string cleaning steps:\n",
        "    - lower\n",
        "    - strip extra spaces\n",
        "    - remove symbols\n",
        "    - remove newlines\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    ranked_documents = []\n",
        "\n",
        "    for document in documents:\n",
        "        score = 0\n",
        "        # Make the document lowercase and strip all symbols, spaces, and newline characters\n",
        "        match_this_cleaned_document = re.sub(r'[^\\w\\s]', '', document.lower()).replace('\\n', '').replace(' ','')\n",
        "        # print(match_this_cleaned_document)\n",
        "        for keyword, weight in keyword_weights:\n",
        "\n",
        "            # Make the keyword lowercase and strip all symbols, spaces, and newline characters\n",
        "            match_this_cleaned_keyword = re.sub(r'[^\\w\\s]', '', keyword.lower()).replace('\\n', '').replace(' ','')\n",
        "            # print(match_this_cleaned_keyword)\n",
        "            # Check if the keyword-phrase is in the document\n",
        "            if match_this_cleaned_keyword in match_this_cleaned_document:\n",
        "                # If the keyword-phrase is in the document, add its weight to the score\n",
        "                score += weight\n",
        "\n",
        "        ranked_documents.append((document, score))\n",
        "\n",
        "    # Sort the documents by their ranking scores in descending order\n",
        "    ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_documents\n",
        "\n",
        "\n",
        "# ################\n",
        "# # Example usage\n",
        "# ################\n",
        "# corpus = [\n",
        "#     \"This is the first document about machine learning.\",\n",
        "#     \"The second document discusses data analysis and visualization.\",\n",
        "#     \"The third document focuses on natural language processing.\",\n",
        "#     \"The fourth document talks about deep learning and neural networks.\",\n",
        "#     \"\"\"to test line breaks\n",
        "#     Emotion mining\n",
        "#      data\n",
        "#     analysis\n",
        "#     Keywords: emotion mining, sentiment analysis, natural disasters, psychology, technological disasters\"\"\",\n",
        "# ]\n",
        "\n",
        "# keyword_weights = [(\"machine learning\", 3), (\"data analysis\", 2), (\"natural language processing\", 4), (\"deep learning\", 5), (\"neural networks\", 6)]\n",
        "\n",
        "# ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "# for document, score in ranked_documents:\n",
        "#     print(f\"Document: {document}\\nScore: {score}\\n\")\n"
      ],
      "metadata": {
        "id": "bqy_ZPvpr-6o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arxiv Explorerer\n"
      ],
      "metadata": {
        "id": "YepU-A4Fr_J3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "19bd0781-5480-4ec0-9709-07330763fd06"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# Arxiv Explorerer\n",
        "###################\n",
        "\n",
        "# step 1: embed the search-phrase\n",
        "# step 2: embed each text\n",
        "# step 3: get scores\n",
        "# step 4: evaluates if score is succss or fail\n",
        "# step 5: if success: do stuff with text\n",
        "\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "start_time_whole_single_task = datetime.now()\n",
        "\n",
        "\n",
        "# ##########################################\n",
        "# # Make comparison phrase and vectorize it\n",
        "# ##########################################\n",
        "# comparison_phrase = \"computer vision resolution enhancement\"\n",
        "# # comparison_phrase = \"cyber security\"\n",
        "# # comparison_phrase = \"natural language processing\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hght1gb699Pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Article Corpus"
      ],
      "metadata": {
        "id": "ItIQ_onG-IXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_segment_time = datetime.now()\n",
        "\n",
        "#####################\n",
        "# Get Article Corpus\n",
        "#####################\n",
        "\n",
        "# List to hold all article data\n",
        "article_data = []\n",
        "\n",
        "# # Make a request to the website\n",
        "r = requests.get('https://arxiv.org/list/cs/new')\n",
        "\n",
        "url = \"https://arxiv.org/list/cs/new\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "# # Find all the articles\n",
        "articles = soup.find_all('dt')\n",
        "\n",
        "# # Find all the titles\n",
        "articles_title = soup.find_all('div', {'class': 'list-title mathjax'})\n",
        "\n",
        "# Find all the subject on the page\n",
        "articles_subject = soup.find_all('dd')\n",
        "\n",
        "\n",
        "###############\n",
        "# make corpus\n",
        "###############\n",
        "\n",
        "corpus = []\n",
        "report_list = []\n",
        "article_dicts = []\n",
        "\n",
        "for this_index, article in enumerate(articles):\n",
        "\n",
        "    ################################################\n",
        "    # Extract each field of data about each article\n",
        "    ################################################\n",
        "\n",
        "    # Extract the title\n",
        "    title = articles_title[this_index].text.split('Title:')[1].strip()\n",
        "\n",
        "    # Extract the subjects\n",
        "    subjects = articles_subject[this_index].find('span', {'class': 'primary-subject'}).text\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "\n",
        "    abstract_p = article.find_next_sibling('dd').find('p', {'class': 'mathjax'})\n",
        "\n",
        "    # Extract the abstract\n",
        "    if abstract_p:\n",
        "        abstract = abstract_p.text.strip()\n",
        "    else:\n",
        "        abstract = \"\"\n",
        "\n",
        "    pdf_link_segment = article.find('a', {'title': 'Download PDF'})['href']\n",
        "\n",
        "    arxiv_id = article.find('a', {'title': 'Abstract'}).text.strip()\n",
        "    pdf_link = f\"https://arxiv.org{pdf_link_segment}\"\n",
        "    paper_link = f\"https://arxiv.org/abs/{arxiv_id[6:]}\"\n",
        "\n",
        "    # extracted_article_string = title + \" \" + abstract + \" \" + str(subjects)\n",
        "\n",
        "    # assemble corpus\n",
        "    article_characters = f\"{this_index}|||| \"\n",
        "\n",
        "    article_characters += f\"\\n'arxiv_id': {arxiv_id}, \"\n",
        "    article_characters += f\"\\n'paper_link': {paper_link}, \"\n",
        "    article_characters += f\"\\n'pdf_link': {pdf_link}, \"\n",
        "\n",
        "    article_characters += \"\\nTitle: \" + title + \" \"\n",
        "    article_characters += \"\\nSubjects: \" + subjects + \" \"\n",
        "    article_characters += \"\\nAbstract: \" + abstract\n",
        "\n",
        "    ##################################\n",
        "    # Make Bundles (sharing an index)\n",
        "    ##################################\n",
        "\n",
        "    # # add to corpus: just the meaningful text\n",
        "    # corpus.append(extracted_article_string)\n",
        "\n",
        "    # add to simple report_list: includes link and article ID info\n",
        "    report_list.append(article_characters)\n",
        "\n",
        "    # Append the data to the list\n",
        "    article_dicts.append({\n",
        "        'title': title,\n",
        "        'abstract': abstract,\n",
        "        'paper_link': paper_link,\n",
        "        'pdf_link': pdf_link,\n",
        "        'subjects': subjects,\n",
        "        'arxiv_id': arxiv_id,\n",
        "        'article_sequence_index': this_index,\n",
        "    })\n",
        "\n",
        "    # using this because only basic search works\n",
        "    corpus = report_list\n",
        "\n",
        "\n",
        "# # Segment Timer\n",
        "# start_segment_time = datetime.now()\n",
        "end_segment_time = datetime.now()\n",
        "duration_time = duration_min_sec(start_segment_time, end_segment_time)\n",
        "print(f\"Duration to run segment -> {duration_time}\")"
      ],
      "metadata": {
        "id": "e8FPqO0u-IXY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e6f320-3866-4f90-c8a6-04193b88c6bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run segment -> 0_min__3.1_sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inspection (size of corpus)\n",
        "len(corpus)"
      ],
      "metadata": {
        "id": "bve1wNfDBC-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7620c9bf-4fe8-4acf-87be-dddbea40039e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "497"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# print and save: code"
      ],
      "metadata": {
        "id": "WPnLaV3fpCkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def result_counter(ranked_documents):\n",
        "    \"\"\"\n",
        "    count non-zero scored results\n",
        "    \"\"\"\n",
        "\n",
        "    result_count = 0\n",
        "\n",
        "    for this_doc in ranked_documents:\n",
        "        score = this_doc[1]\n",
        "\n",
        "        if score != 0:\n",
        "            result_count += 1\n",
        "\n",
        "    return result_count\n",
        "\n",
        "# def print_and_save(ranked_documents, top_n, name_of_set):\n",
        "#     # Posix UTC Seconds\n",
        "#     # make readable time\n",
        "#     # from datetime import datetime\n",
        "#     date_time = datetime.now()\n",
        "#     clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "#     counter = 0\n",
        "\n",
        "#     results_json_list = []\n",
        "\n",
        "#     for document, score in ranked_documents:\n",
        "\n",
        "#         if score != 0:\n",
        "\n",
        "#             blurb = f\"Document: {document}\\nScore: {score}\\n\"\n",
        "\n",
        "#             print(blurb)\n",
        "\n",
        "#         this_index = int(document.split('||||')[0])\n",
        "\n",
        "#         data_dict = article_dicts[this_index]\n",
        "\n",
        "#         results_json_list.append(data_dict)\n",
        "\n",
        "#         counter += 1\n",
        "#         if counter >= top_n:\n",
        "#             break\n",
        "\n",
        "\n",
        "#     #############\n",
        "#     # Write Data\n",
        "#     #############\n",
        "\n",
        "#     # Save the data to a JSON file\n",
        "#     with open(f'{name_of_set}_articles_{clean_timestamp}.json', 'w') as f:\n",
        "#         json.dump(results_json_list, f)\n",
        "\n",
        "#     # Create an HTML file\n",
        "#     html = '<html><body>'\n",
        "#     for article in results_json_list:\n",
        "#         html += f'<h2><a href=\"{article[\"paper_link\"]}\">{article[\"title\"]}</a></h2>'\n",
        "#         html += f'<p>{article[\"abstract\"]}</p>'\n",
        "#         html += f'<p>Subjects: {str(article[\"subjects\"])}</p>'\n",
        "\n",
        "#         html += f'<a href=\"{article[\"paper_link\"]}\">{article[\"paper_link\"]}</a>'\n",
        "#         html += f'<p>paper link: {str(article[\"paper_link\"])}</p>'\n",
        "\n",
        "#         html += f'<a href=\"{article[\"pdf_link\"]}\">{article[\"pdf_link\"]}</a>'\n",
        "#         html += f'<p>pdf link: {str(article[\"pdf_link\"])}</p>'\n",
        "\n",
        "#         html += f'<p>arxiv id: {str(article[\"arxiv_id\"])}</p>'\n",
        "#         html += f'<p>article_sequence_index id: {str(article[\"article_sequence_index\"])}</p>'\n",
        "\n",
        "#     html += '</body></html>'\n",
        "\n",
        "\n",
        "#     # Save the HTML to a file\n",
        "#     with open(f'{name_of_set}_articles{clean_timestamp}.html', 'w') as f:\n",
        "#         f.write(html)"
      ],
      "metadata": {
        "id": "fsrVgItspCx2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "########################################\n",
        "# Filter, Save, & Print the Raw Results\n",
        "########################################\n",
        "\n",
        "\n",
        "def result_counter(ranked_documents):\n",
        "    \"\"\"\n",
        "    count non-zero scored results\n",
        "    \"\"\"\n",
        "\n",
        "    result_count = 0\n",
        "\n",
        "    for this_doc in ranked_documents:\n",
        "        score = this_doc[1]\n",
        "\n",
        "        if score != 0:\n",
        "            result_count += 1\n",
        "\n",
        "    return result_count\n",
        "\n",
        "\n",
        "# def score_filtered_result_counter(ranked_documents, score_floor=0):\n",
        "#     \"\"\"\n",
        "#     count non-zero scored results that are greater than or equal to score_floor\n",
        "#     \"\"\"\n",
        "\n",
        "#     result_count = 0\n",
        "\n",
        "#     for this_doc in ranked_documents:\n",
        "#         score = this_doc[1]\n",
        "\n",
        "#         if score >= score_floor:\n",
        "#             result_count += 1\n",
        "\n",
        "#     return result_count\n",
        "\n",
        "def score_filtered_result_counter(ranked_documents, score_floor=0):\n",
        "    \"\"\"\n",
        "    count non-zero scored results that are greater than or equal to score_floor\n",
        "    \"\"\"\n",
        "\n",
        "    result_count = 0\n",
        "\n",
        "    for this_doc in ranked_documents:\n",
        "        score = this_doc[1]\n",
        "\n",
        "        if score != 0 and score >= score_floor:\n",
        "            result_count += 1\n",
        "\n",
        "    return result_count\n",
        "\n",
        "\n",
        "def print_and_save(ranked_documents, top_n, name_of_set, score_floor=5):\n",
        "    # Posix UTC Seconds\n",
        "    # make readable time\n",
        "    # from datetime import datetime\n",
        "    date_time = datetime.now()\n",
        "    clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "    counter = 0\n",
        "\n",
        "    results_json_list = []\n",
        "\n",
        "    for document, score in ranked_documents:\n",
        "\n",
        "        if score >= score_floor:\n",
        "\n",
        "            blurb = f\"Document: {document}\\nScore: {score}\\n\"\n",
        "\n",
        "            print(blurb)\n",
        "\n",
        "        this_index = int(document.split('||||')[0])\n",
        "\n",
        "        data_dict = article_dicts[this_index]\n",
        "\n",
        "        results_json_list.append(data_dict)\n",
        "\n",
        "        counter += 1\n",
        "        if counter >= top_n:\n",
        "            break\n",
        "\n",
        "\n",
        "    #############\n",
        "    # Write Data\n",
        "    #############\n",
        "\n",
        "    # Save the data to a JSON file\n",
        "    with open(f'{name_of_set}_articles_{clean_timestamp}.json', 'w') as f:\n",
        "        json.dump(results_json_list, f)\n",
        "\n",
        "    # Create an HTML file\n",
        "    html = '<html><body>'\n",
        "    for article in results_json_list:\n",
        "        html += f'<h2><a href=\"{article[\"paper_link\"]}\">{article[\"title\"]}</a></h2>'\n",
        "        html += f'<p>{article[\"abstract\"]}</p>'\n",
        "        html += f'<p>Subjects: {str(article[\"subjects\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"paper_link\"]}\">{article[\"paper_link\"]}</a>'\n",
        "        html += f'<p>paper link: {str(article[\"paper_link\"])}</p>'\n",
        "\n",
        "        html += f'<a href=\"{article[\"pdf_link\"]}\">{article[\"pdf_link\"]}</a>'\n",
        "        html += f'<p>pdf link: {str(article[\"pdf_link\"])}</p>'\n",
        "\n",
        "        html += f'<p>arxiv id: {str(article[\"arxiv_id\"])}</p>'\n",
        "        html += f'<p>article_sequence_index id: {str(article[\"article_sequence_index\"])}</p>'\n",
        "\n",
        "    html += '</body></html>'\n",
        "\n",
        "\n",
        "    # Save the HTML to a file\n",
        "    with open(f'{name_of_set}_articles{clean_timestamp}.html', 'w') as f:\n",
        "        f.write(html)\n"
      ],
      "metadata": {
        "id": "peVzbe-Di2xH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find top-n articles: use keyword/weights"
      ],
      "metadata": {
        "id": "bt_SeRE_l345"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Results Returned\n",
        "top_n = 45\n",
        "score_floor = 3\n",
        "\n",
        "\n",
        "list_of_lists_of_weights = [\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"computer vision\", 3),\n",
        "    #     (\"resolution\", 2),\n",
        "    #     # (\"natural language processing\", 4),\n",
        "    #     # (\"deep learning\", 5),\n",
        "    #     (\"neural networks\", 6),\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"computer vision\", 3),\n",
        "    #     (\"resolution\", 2),\n",
        "    #     # (\"natural language processing\", 4),\n",
        "    #     # (\"deep learning\", 5),\n",
        "    #     (\"neural networks\", 6),\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # # keyword_weights =\n",
        "    # (\"cognitive science\", 2),  # much too broad...\n",
        "    [\n",
        "        (\"mental health\", 5),\n",
        "        (\"psychological health\", 5),\n",
        "        (\"psycholog\", 2),  # stem vs. lemma\n",
        "\n",
        "\n",
        "        (\"mental health care\", 3),\n",
        "        (\"neuroscience\", 2),\n",
        "        (\"psychological assessment\", 2),\n",
        "        (\"personality assessment\", 2),\n",
        "        (\"personality inference\", 2),\n",
        "        (\"personality traits\", 2),\n",
        "        (\"personality dimensions\", 2),\n",
        "        (\"emotion\", 15),\n",
        "        (\"sports psychology\", 15),\n",
        "        # (\"\", 2),\n",
        "        # (\"\", 2),\n",
        "\n",
        "\n",
        "\n",
        "        # disease terms\n",
        "        (\"depression\", 5),\n",
        "        (\"anxiety\", 5),\n",
        "        (\"mental disorders\", 2),\n",
        "        (\"social anxiety disorder\", 4),\n",
        "        (\"mental illness\", 2),\n",
        "        (\"Major Depressive Disorder\", 2),\n",
        "        (\"MDD\", 2),\n",
        "        (\"psychological stressors\", 2),\n",
        "        (\"cognitive impairment\", 2),\n",
        "        (\"mci\", 2),\n",
        "        # (\"\", 2),\n",
        "        # (\"\", 2),\n",
        "        # (\"\", 2),\n",
        "\n",
        "        ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"benchmark\", 5),\n",
        "    #     (\"model evaluation\", 5),\n",
        "    #     (\"test\", 2),\n",
        "    #     (\"measure\", 2),\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"training set\", 5),\n",
        "    #     (\"synthetic\", 2),\n",
        "    #     (\"generate\", 2),\n",
        "    #     (\"measure\", 2),\n",
        "    # ],\n",
        "\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"graph\", 5),\n",
        "    #     (\"graph generation\", 8),\n",
        "    #     (\"subgraph\", 2),\n",
        "    #     (\"hierarchical graph\", 2),\n",
        "    #     (\"embedding\", 2),\n",
        "    #     (\"knowledge graph\", 2),\n",
        "\n",
        "    #     (\"graph neural networks\", 2),\n",
        "    #     (\"graph representation\", 2),\n",
        "    #     (\"node\", 2),\n",
        "    #      ## collisions: cryptograph, geograph,\n",
        "    # ],\n",
        "\n",
        "]\n",
        "\n",
        "date_time = datetime.now()\n",
        "clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "counter = 0\n",
        "for keyword_weights in list_of_lists_of_weights:\n",
        "\n",
        "    ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "    # user first list item as name of set\n",
        "    name_of_set = list_of_lists_of_weights[counter][0][0]\n",
        "\n",
        "    result_quantity = result_counter(ranked_documents)\n",
        "\n",
        "    score_floor_filtered_quantity = score_filtered_result_counter(ranked_documents, score_floor)\n",
        "\n",
        "    this_max_number = top_n\n",
        "\n",
        "    if top_n > result_quantity:\n",
        "        this_max_number = result_quantity\n",
        "\n",
        "    print(f\"\\n\\nSet Name: {name_of_set}\")\n",
        "    print(f\"Total Matches in Set: {result_quantity}\")\n",
        "    print(f\"Matches Above Score-Floor in Set: {score_floor_filtered_quantity}\")\n",
        "    print(clean_timestamp)\n",
        "\n",
        "    print(f\"\\nShowing {score_floor_filtered_quantity} in top-{this_max_number} out of {result_quantity} total results.     -> {score_floor_filtered_quantity} of {this_max_number}/{result_quantity}\")\n",
        "    print(f\"(Ceiling set at {top_n} (top_n) filtered results.)    -> {top_n}\")\n",
        "    print(f\"(Minimum-included-score, 'Score-Floor' set at {score_floor}) -> {score_floor}\\n\\n\")\n",
        "\n",
        "    print_and_save(ranked_documents, top_n, name_of_set, score_floor)\n",
        "    counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CixuXw-Fl3-f",
        "outputId": "f9b46984-716f-4074-bc82-2853c4c5206b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: mental health\n",
            "Total Matches in Set: 22\n",
            "Matches Above Score-Floor in Set: 17\n",
            "2024-08-18__133655876475\n",
            "\n",
            "Showing 17 in top-22 out of 22 total results.     -> 17 of 22/22\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 3) -> 3\n",
            "\n",
            "\n",
            "Document: 0|||| \n",
            "'arxiv_id': arXiv:2408.07704, \n",
            "'paper_link': https://arxiv.org/abs/2408.07704, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07704, \n",
            "Title: Empathic Responding for Digital Interpersonal Emotion Regulation via Content Recommendation \n",
            "Subjects: Information Retrieval (cs.IR) \n",
            "Abstract: Interpersonal communication plays a key role in managing people's emotions, especially on digital platforms. Studies have shown that people use social media and consume online content to regulate their emotions and find support for rest and recovery. However, these platforms are not designed for emotion regulation, which limits their effectiveness in this regard. To address this issue, we propose an approach to enhance Interpersonal Emotion Regulation (IER) on online platforms through content recommendation. The objective is to empower users to regulate their emotions while actively or passively engaging in online platforms by crafting media content that aligns with IER strategies, particularly empathic responding. The proposed recommendation system is expected to blend system-initiated and user-initiated emotion regulation, paving the way for real-time IER practices on digital media platforms. To assess the efficacy of this approach, a mixed-method research design is used, including the analysis of text-based social media data and a user survey. Digital applications has served as facilitators in this process, given the widespread recognition of digital media applications for Digital Emotion Regulation (DER). The study collects 37.5K instances of user posts and interactions on Reddit over a year to design a Contextual Multi-Armed Bandits (CMAB) based recommendation system using features from user activity and preferences. The experimentation shows that the empathic recommendations generated by the proposed recommendation system are preferred by users over widely accepted ER strategies such as distraction and avoidance.\n",
            "Score: 15\n",
            "\n",
            "Document: 33|||| \n",
            "'arxiv_id': arXiv:2408.07806, \n",
            "'paper_link': https://arxiv.org/abs/2408.07806, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07806, \n",
            "Title: From Decision to Action in Surgical Autonomy: Multi-Modal Large Language Models for Robot-Assisted Blood Suction \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: The rise of Large Language Models (LLMs) has impacted research in robotics and automation. While progress has been made in integrating LLMs into general robotics tasks, a noticeable void persists in their adoption in more specific domains such as surgery, where critical factors such as reasoning, explainability, and safety are paramount. Achieving autonomy in robotic surgery, which entails the ability to reason and adapt to changes in the environment, remains a significant challenge. In this work, we propose a multi-modal LLM integration in robot-assisted surgery for autonomous blood suction. The reasoning and prioritization are delegated to the higher-level task-planning LLM, and the motion planning and execution are handled by the lower-level deep reinforcement learning model, creating a distributed agency between the two components. As surgical operations are highly dynamic and may encounter unforeseen circumstances, blood clots and active bleeding were introduced to influence decision-making. Results showed that using a multi-modal LLM as a higher-level reasoning unit can account for these surgical complexities to achieve a level of reasoning previously unattainable in robot-assisted surgeries. These findings demonstrate the potential of multi-modal LLMs to significantly enhance contextual understanding and decision-making in robotic-assisted surgeries, marking a step toward autonomous surgical systems.\n",
            "Score: 15\n",
            "\n",
            "Document: 54|||| \n",
            "'arxiv_id': arXiv:2408.07851, \n",
            "'paper_link': https://arxiv.org/abs/2408.07851, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07851, \n",
            "Title: SER Evals: In-domain and Out-of-domain Benchmarking for Speech Emotion Recognition \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction.\n",
            "Score: 15\n",
            "\n",
            "Document: 73|||| \n",
            "'arxiv_id': arXiv:2408.07891, \n",
            "'paper_link': https://arxiv.org/abs/2408.07891, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07891, \n",
            "Title: Quantum-inspired Interpretable Deep Learning Architecture for Text Sentiment Analysis \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Text has become the predominant form of communication on social media, embedding a wealth of emotional nuances. Consequently, the extraction of emotional information from text is of paramount importance. Despite previous research making some progress, existing text sentiment analysis models still face challenges in integrating diverse semantic information and lack interpretability. To address these issues, we propose a quantum-inspired deep learning architecture that combines fundamental principles of quantum mechanics (QM principles) with deep learning models for text sentiment analysis. Specifically, we analyze the commonalities between text representation and QM principles to design a quantum-inspired text representation method and further develop a quantum-inspired text embedding layer. Additionally, we design a feature extraction layer based on long short-term memory (LSTM) networks and self-attention mechanisms (SAMs). Finally, we calculate the text density matrix using the quantum complex numbers principle and apply 2D-convolution neural networks (CNNs) for feature condensation and dimensionality reduction. Through a series of visualization, comparative, and ablation experiments, we demonstrate that our model not only shows significant advantages in accuracy and efficiency compared to previous related models but also achieves a certain level of interpretability by integrating QM principles. Our code is available at QISA.\n",
            "Score: 15\n",
            "\n",
            "Document: 78|||| \n",
            "'arxiv_id': arXiv:2408.07900, \n",
            "'paper_link': https://arxiv.org/abs/2408.07900, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07900, \n",
            "Title: Network analysis reveals news press landscape and asymmetric user polarization \n",
            "Subjects: Social and Information Networks (cs.SI) \n",
            "Abstract: Unlike traditional media, online news platforms allow users to consume content that suits their tastes and to facilitate interactions with other people. However, as more personalized consumption of information and interaction with like-minded users increase, ideological bias can inadvertently increase and contribute to the formation of echo chambers, reinforcing the polarization of opinions. Although the structural characteristics of polarization among different ideological groups in online spaces have been extensively studied, research into how these groups emotionally interact with each other has not been as thoroughly explored. From this perspective, we investigate both structural and affective polarization between news media user groups on Naver News, South Korea's largest online news portal, during the period of 2022 Korean presidential election. By utilizing the dataset comprising 333,014 articles and over 36 million user comments, we uncover two distinct groups of users characterized by opposing political leanings and reveal significant bias and polarization among them. Additionally, we reveal the existence of echo chambers within co-commenting networks and investigate the asymmetric affective interaction patterns between the two polarized groups. Classification task of news media articles based on the distinct comment response patterns support the notion that different political groups may employ distinct communication strategies. Our approach based on network analysis on large-scale comment dataset offers novel insights into characteristics of user polarization in the online news platforms and the nuanced interaction nature between user groups.\n",
            "Score: 15\n",
            "\n",
            "Document: 92|||| \n",
            "'arxiv_id': arXiv:2408.07922, \n",
            "'paper_link': https://arxiv.org/abs/2408.07922, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07922, \n",
            "Title: A Deep Features-Based Approach Using Modified ResNet50 and Gradient Boosting for Visual Sentiments Classification \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: The versatile nature of Visual Sentiment Analysis (VSA) is one reason for its rising profile. It isn't easy to efficiently manage social media data with visual information since previous research has concentrated on Sentiment Analysis (SA) of single modalities, like textual. In addition, most visual sentiment studies need to adequately classify sentiment because they are mainly focused on simply merging modal attributes without investigating their intricate relationships. This prompted the suggestion of developing a fusion of deep learning and machine learning algorithms. In this research, a deep feature-based method for multiclass classification has been used to extract deep features from modified ResNet50. Furthermore, gradient boosting algorithm has been used to classify photos containing emotional content. The approach is thoroughly evaluated on two benchmarked datasets, CrowdFlower and GAPED. Finally, cutting-edge deep learning and machine learning models were used to compare the proposed strategy. When compared to state-of-the-art approaches, the proposed method demonstrates exceptional performance on the datasets presented.\n",
            "Score: 15\n",
            "\n",
            "Document: 117|||| \n",
            "'arxiv_id': arXiv:2408.07982, \n",
            "'paper_link': https://arxiv.org/abs/2408.07982, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07982, \n",
            "Title: Toward a Dialogue System Using a Large Language Model to Recognize User Emotions with a Camera \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: The performance of ChatGPTÂ© and other LLMs has improved tremendously, and in online environments, they are increasingly likely to be used in a wide variety of situations, such as ChatBot on web pages, call center operations using voice interaction, and dialogue functions using agents. In the offline environment, multimodal dialogue functions are also being realized, such as guidance by Artificial Intelligence agents (AI agents) using tablet terminals and dialogue systems in the form of LLMs mounted on robots. In this multimodal dialogue, mutual emotion recognition between the AI and the user will become important. So far, there have been methods for expressing emotions on the part of the AI agent or for recognizing them using textual or voice information of the user's utterances, but methods for AI agents to recognize emotions from the user's facial expressions have not been studied. In this study, we examined whether or not LLM-based AI agents can interact with users according to their emotional states by capturing the user in dialogue with a camera, recognizing emotions from facial expressions, and adding such emotion information to prompts. The results confirmed that AI agents can have conversations according to the emotional state for emotional states with relatively high scores, such as Happy and Angry.\n",
            "Score: 15\n",
            "\n",
            "Document: 165|||| \n",
            "'arxiv_id': arXiv:2408.08078, \n",
            "'paper_link': https://arxiv.org/abs/2408.08078, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.08078, \n",
            "Title: Treat Stillness with Movement: Remote Sensing Change Detection via Coarse-grained Temporal Foregrounds Mining \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Current works focus on addressing the remote sensing change detection task using bi-temporal images. Although good performance can be achieved, however, seldom of they consider the motion cues which may also be vital. In this work, we revisit the widely adopted bi-temporal images-based framework and propose a novel Coarse-grained Temporal Mining Augmented (CTMA) framework. To be specific, given the bi-temporal images, we first transform them into a video using interpolation operations. Then, a set of temporal encoders is adopted to extract the motion features from the obtained video for coarse-grained changed region prediction. Subsequently, we design a novel Coarse-grained Foregrounds Augmented Spatial Encoder module to integrate both global and local information. We also introduce a motion augmented strategy that leverages motion cues as an additional output to aggregate with the spatial features for improved results. Meanwhile, we feed the input image pairs into the ResNet to get the different features and also the spatial blocks for fine-grained feature learning. More importantly, we propose a mask augmented strategy that utilizes coarse-grained changed regions, incorporating them into the decoder blocks to enhance the final changed prediction. Extensive experiments conducted on multiple benchmark datasets fully validated the effectiveness of our proposed framework for remote sensing image change detection. The source code of this paper will be released on this https URL\n",
            "Score: 15\n",
            "\n",
            "Document: 196|||| \n",
            "'arxiv_id': arXiv:2408.08134, \n",
            "'paper_link': https://arxiv.org/abs/2408.08134, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.08134, \n",
            "Title: CorrAdaptor: Adaptive Local Context Learning for Correspondence Pruning \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: In the fields of computer vision and robotics, accurate pixel-level correspondences are essential for enabling advanced tasks such as structure-from-motion and simultaneous localization and mapping. Recent correspondence pruning methods usually focus on learning local consistency through k-nearest neighbors, which makes it difficult to capture robust context for each correspondence. We propose CorrAdaptor, a novel architecture that introduces a dual-branch structure capable of adaptively adjusting local contexts through both explicit and implicit local graph learning. Specifically, the explicit branch uses KNN-based graphs tailored for initial neighborhood identification, while the implicit branch leverages a learnable matrix to softly assign neighbors and adaptively expand the local context scope, significantly enhancing the model's robustness and adaptability to complex image variations. Moreover, we design a motion injection module to integrate motion consistency into the network to suppress the impact of outliers and refine local context learning, resulting in substantial performance improvements. The experimental results on extensive correspondence-based tasks indicate that our CorrAdaptor achieves state-of-the-art performance both qualitatively and quantitatively. The code and pre-trained models are available at this https URL.\n",
            "Score: 15\n",
            "\n",
            "Document: 326|||| \n",
            "'arxiv_id': arXiv:2306.00416, \n",
            "'paper_link': https://arxiv.org/abs/2306.00416, \n",
            "'pdf_link': https://arxiv.org/pdf/2306.00416, \n",
            "Title: Interactive Character Control with Auto-Regressive Motion Diffusion Models \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Real-time character control is an essential component for interactive experiences, with a broad range of applications, including physics simulations, video games, and virtual reality. The success of diffusion models for image synthesis has led to the use of these models for motion synthesis. However, the majority of these motion diffusion models are primarily designed for offline applications, where space-time models are used to synthesize an entire sequence of frames simultaneously with a pre-specified length. To enable real-time motion synthesis with diffusion model that allows time-varying controls, we propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional diffusion model takes an initial pose as input, and auto-regressively generates successive motion frames conditioned on the previous frame. Despite its streamlined network architecture, which uses simple MLPs, our framework is capable of generating diverse, long-horizon, and high-fidelity motion sequences. Furthermore, we introduce a suite of techniques for incorporating interactive controls into A-MDM, such as task-oriented sampling, in-painting, and hierarchical reinforcement learning. These techniques enable a pre-trained A-MDM to be efficiently adapted for a variety of new downstream tasks. We conduct a comprehensive suite of experiments to demonstrate the effectiveness of A-MDM, and compare its performance against state-of-the-art auto-regressive methods.\n",
            "Score: 15\n",
            "\n",
            "Document: 327|||| \n",
            "'arxiv_id': arXiv:2306.16927, \n",
            "'paper_link': https://arxiv.org/abs/2306.16927, \n",
            "'pdf_link': https://arxiv.org/pdf/2306.16927, \n",
            "Title: End-to-end Autonomous Driving: Challenges and Frontiers \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm framework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection and motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception and planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need for autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis of more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous driving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world models, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how to incorporate these techniques within the end-to-end driving framework. we maintain an active repository that contains up-to-date literature and open-source projects at this https URL.\n",
            "Score: 15\n",
            "\n",
            "Document: 366|||| \n",
            "'arxiv_id': arXiv:2402.13244, \n",
            "'paper_link': https://arxiv.org/abs/2402.13244, \n",
            "'pdf_link': https://arxiv.org/pdf/2402.13244, \n",
            "Title: Are Fact-Checking Tools Helpful? An Exploration of the Usability of Google Fact Check \n",
            "Subjects: Social and Information Networks (cs.SI) \n",
            "Abstract: Fact-checking-specific search engines such as Google Fact Check are a promising way to combat misinformation on social media, especially during significant events such as the COVID-19 pandemic and the U.S. presidential elections, but the usability of such an approach has not been thoroughly studied. We evaluated the performance of Google Fact Check by analyzing the retrieved fact-checking results regarding 1,000 COVID-19-related false claims and found it able to retrieve the fact-checking results for 15.8% of the input claims, and the results are relatively reliable. We also found that the false claims receiving different fact-checking verdicts (i.e., \"False,\" \"Partly False,\" \"True,\" and \"Unratable\") tend to reflect diverse emotional tones, and fact-checking sources tend to check the claims in different lengths and using dictionary words to various extents. Claim variations addressing the same issue yet described differently are likely to retrieve distinct fact-checking results. We suggested that the quantities of the retrieved fact-checking results could be optimized and that slightly adjusting input wording may be the best practice for users to retrieve more useful information. This study aims to contribute to the understanding of state-of-the-art fact-checking tools and information integrity.\n",
            "Score: 15\n",
            "\n",
            "Document: 390|||| \n",
            "'arxiv_id': arXiv:2404.06687, \n",
            "'paper_link': https://arxiv.org/abs/2404.06687, \n",
            "'pdf_link': https://arxiv.org/pdf/2404.06687, \n",
            "Title: Fast and Accurate Relative Motion Tracking for Dual Industrial Robots \n",
            "Subjects: Robotics (cs.RO) \n",
            "Abstract: Industrial robotic applications such as spraying, welding, and additive manufacturing frequently require fast, accurate, and uniform motion along a 3D spatial curve. To increase process throughput, some manufacturers propose a dual-robot setup to overcome the speed limitation of a single robot. Industrial robot motion is programmed through waypoints connected by motion primitives (Cartesian linear and circular paths and linear joint paths at constant Cartesian speed). The actual robot motion is affected by the blending between these motion primitives and the pose of the robot (an outstretched/near-singularity pose tends to have larger path tracking errors). Choosing the waypoints and the speed along each motion segment to achieve the performance requirement is challenging. At present, there is no automated solution, and laborious manual tuning by robot experts is needed to approach the desired performance. In this paper, we present a systematic three-step approach to designing and programming a dual robot system to optimize system performance. The first step is to select the relative placement between the two robots based on the specified relative motion path. The second step is to select the relative waypoints and the motion primitives. The final step is to update the waypoints iteratively based on the actual measured relative motion. Waypoint iteration is first executed in simulation and then completed using the actual robots. For performance assessment, we use the mean path speed subject to the relative position and orientation constraints and the path speed uniformity constraint. We have demonstrated the effectiveness of this method on two systems, a physical testbed of two ABB robots and a simulation testbed of two FANUC robots, for two challenging test curves.\n",
            "Score: 15\n",
            "\n",
            "Document: 437|||| \n",
            "'arxiv_id': arXiv:2407.18552, \n",
            "'paper_link': https://arxiv.org/abs/2407.18552, \n",
            "'pdf_link': https://arxiv.org/pdf/2407.18552, \n",
            "Title: Multimodal Emotion Recognition using Audio-Video Transformer Fusion with Cross Attention \n",
            "Subjects: Multimedia (cs.MM) \n",
            "Abstract: Understanding emotions is a fundamental aspect of human communication. Integrating audio and video signals offers a more comprehensive understanding of emotional states compared to traditional methods that rely on a single data source, such as speech or facial expressions. Despite its potential, multimodal emotion recognition faces significant challenges, particularly in synchronization, feature extraction, and fusion of diverse data sources. To address these issues, this paper introduces a novel transformer-based model named Audio-Video Transformer Fusion with Cross Attention (AVT-CA). The AVT-CA model employs a transformer fusion approach to effectively capture and synchronize interlinked features from both audio and video inputs, thereby resolving synchronization problems. Additionally, the Cross Attention mechanism within AVT-CA selectively extracts and emphasizes critical features while discarding irrelevant ones from both modalities, addressing feature extraction and fusion challenges. Extensive experimental analysis conducted on the CMU-MOSEI, RAVDESS and CREMA-D datasets demonstrates the efficacy of the proposed model. The results underscore the importance of AVT-CA in developing precise and reliable multimodal emotion recognition systems for practical applications.\n",
            "Score: 15\n",
            "\n",
            "Document: 414|||| \n",
            "'arxiv_id': arXiv:2406.11852, \n",
            "'paper_link': https://arxiv.org/abs/2406.11852, \n",
            "'pdf_link': https://arxiv.org/pdf/2406.11852, \n",
            "Title: Risks from Language Models for Automated Mental Healthcare: Ethics and Structure for Implementation \n",
            "Subjects: Computers and Society (cs.CY) \n",
            "Abstract: Amidst the growing interest in developing task-autonomous AI for automated mental health care, this paper addresses the ethical and practical challenges associated with the issue and proposes a structured framework that delineates levels of autonomy, outlines ethical requirements, and defines beneficial default behaviors for AI agents in the context of mental health support. We also evaluate fourteen state-of-the-art language models (ten off-the-shelf, four fine-tuned) using 16 mental health-related questionnaires designed to reflect various mental health conditions, such as psychosis, mania, depression, suicidal thoughts, and homicidal tendencies. The questionnaire design and response evaluations were conducted by mental health clinicians (M.D.s). We find that existing language models are insufficient to match the standard provided by human professionals who can navigate nuances and appreciate context. This is due to a range of issues, including overly cautious or sycophantic responses and the absence of necessary safeguards. Alarmingly, we find that most of the tested models could cause harm if accessed in mental health emergencies, failing to protect users and potentially exacerbating existing symptoms. We explore solutions to enhance the safety of current models. Before the release of increasingly task-autonomous AI systems in mental health, it is crucial to ensure that these models can reliably detect and manage symptoms of common psychiatric disorders to prevent harm to users. This involves aligning with the ethical framework and default behaviors outlined in our study. We contend that model developers are responsible for refining their systems per these guidelines to safeguard against the risks posed by current AI technologies to user mental health and safety.\n",
            "Trigger warning: Contains and discusses examples of sensitive mental health topics, including suicide and self-harm.\n",
            "Score: 13\n",
            "\n",
            "Document: 254|||| \n",
            "'arxiv_id': arXiv:2408.08261, \n",
            "'paper_link': https://arxiv.org/abs/2408.08261, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.08261, \n",
            "Title: mhGPT: A Lightweight Generative Pre-Trained Transformer for Mental Health Text Analysis \n",
            "Subjects: Computation and Language (cs.CL) \n",
            "Abstract: This paper introduces mhGPT, a lightweight generative pre-trained transformer trained on mental health-related social media and PubMed articles. Fine-tuned for specific mental health tasks, mhGPT was evaluated under limited hardware constraints and compared with state-of-the-art models like MentaLLaMA and Gemma. Despite having only 1.98 billion parameters and using just 5% of the dataset, mhGPT outperformed larger models and matched the performance of models trained on significantly more data. The key contributions include integrating diverse mental health data, creating a custom tokenizer, and optimizing a smaller architecture for low-resource settings. This research could advance AI-driven mental health care, especially in areas with limited computing power.\n",
            "Score: 8\n",
            "\n",
            "Document: 25|||| \n",
            "'arxiv_id': arXiv:2408.07784, \n",
            "'paper_link': https://arxiv.org/abs/2408.07784, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07784, \n",
            "Title: Navigating the Paradox: Challenges and Strategies of University Students Managing Mental Health Medication in Real-World Practices \n",
            "Subjects: Human-Computer Interaction (cs.HC) \n",
            "Abstract: Mental health has become a growing concern among university students. While medication is a common treatment, understanding how university students manage their medication for mental health symptoms in real-world practice has not been fully explored. In this study, we conducted semi-structured interviews with university students to understand the unique challenges in the mental health medication management process and their coping strategies, particularly examining the role of various technologies in this process. We discovered that due to struggles with self-acceptance and the interdependent relationship between medication, symptoms, schedules, and life changes, the medication management process for students was a highly dynamic journey involving frequent dosage changes. Thus, students adopted flexible strategies of using minimal technology to manage their medication in different situations while maintaining a high degree of autonomy. Based on our findings, we propose design implications for future technologies to seamlessly integrate into their daily lives and assist students in managing their mental health medications.\n",
            "Score: 5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See files\n",
        "!ls"
      ],
      "metadata": {
        "id": "B7IHdEI-v2jr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "629fc735-ef7f-4472-f4d5-e3c8ade9fd01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'mental health_articles2024-08-18__132207430193.html'\t sample_data\n",
            "'mental health_articles_2024-08-18__132207430193.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Results Returned\n",
        "top_n = 45\n",
        "score_floor = 3\n",
        "\n",
        "\n",
        "list_of_lists_of_weights = [\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"computer vision\", 3),\n",
        "    #     (\"resolution\", 2),\n",
        "    #     # (\"natural language processing\", 4),\n",
        "    #     # (\"deep learning\", 5),\n",
        "    #     (\"neural networks\", 6),\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # keyword_weights =\n",
        "    [\n",
        "        (\"distance measure\", 10),\n",
        "        (\"similarity measure\", 10),\n",
        "        (\"vector distance\", 10),\n",
        "        (\"distance metric\", 10),\n",
        "        (\"similarity metric\", 10),\n",
        "        (\"dimension reduction\", 10),\n",
        "\n",
        "\n",
        "        (\"similarity\", 1),\n",
        "        (\"distance\", 1),\n",
        "        (\"metric\", 1),\n",
        "\n",
        "    ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # # (\"cognitive science\", 2),  # much too broad...\n",
        "    # [\n",
        "    #     (\"mental health\", 5),\n",
        "    #     (\"psychological health\", 5),\n",
        "    #     (\"psycholog\", 2),  # stem vs. lemma\n",
        "\n",
        "\n",
        "    #     (\"mental health care\", 3),\n",
        "    #     (\"neuroscience\", 2),\n",
        "    #     (\"psychological assessment\", 2),\n",
        "    #     (\"personality assessment\", 2),\n",
        "    #     (\"personality inference\", 2),\n",
        "    #     (\"personality traits\", 2),\n",
        "    #     (\"personality dimensions\", 2),\n",
        "    #     (\"emotion\", 15),\n",
        "    #     (\"sports psychology\", 15),\n",
        "    #     # (\"\", 2),\n",
        "    #     # (\"\", 2),\n",
        "\n",
        "\n",
        "\n",
        "    #     # disease terms\n",
        "    #     (\"depression\", 5),\n",
        "    #     (\"anxiety\", 5),\n",
        "    #     (\"mental disorders\", 2),\n",
        "    #     (\"social anxiety disorder\", 4),\n",
        "    #     (\"mental illness\", 2),\n",
        "    #     (\"Major Depressive Disorder\", 2),\n",
        "    #     (\"MDD\", 2),\n",
        "    #     (\"psychological stressors\", 2),\n",
        "    #     (\"cognitive impairment\", 2),\n",
        "    #     (\"mci\", 2),\n",
        "    #     # (\"\", 2),\n",
        "    #     # (\"\", 2),\n",
        "    #     # (\"\", 2),\n",
        "\n",
        "    #     ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"benchmark\", 5),\n",
        "    #     (\"model evaluation\", 5),\n",
        "    #     (\"test\", 2),\n",
        "    #     (\"measure\", 2),\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"training set\", 5),\n",
        "    #     (\"synthetic\", 2),\n",
        "    #     (\"generate\", 2),\n",
        "    #     (\"measure\", 2),\n",
        "    # ],\n",
        "\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"graph\", 5),\n",
        "    #     (\"graph generation\", 8),\n",
        "    #     (\"subgraph\", 2),\n",
        "    #     (\"hierarchical graph\", 2),\n",
        "    #     (\"embedding\", 2),\n",
        "    #     (\"knowledge graph\", 2),\n",
        "\n",
        "    #     (\"graph neural networks\", 2),\n",
        "    #     (\"graph representation\", 2),\n",
        "    #     (\"node\", 2),\n",
        "    #      ## collisions: cryptograph, geograph,\n",
        "    # ],\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "date_time = datetime.now()\n",
        "clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "counter = 0\n",
        "for keyword_weights in list_of_lists_of_weights:\n",
        "\n",
        "    ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "    # user first list item as name of set\n",
        "    name_of_set = list_of_lists_of_weights[counter][0][0]\n",
        "\n",
        "    result_quantity = result_counter(ranked_documents)\n",
        "\n",
        "    score_floor_filtered_quantity = score_filtered_result_counter(ranked_documents, score_floor)\n",
        "\n",
        "    this_max_number = top_n\n",
        "\n",
        "    if top_n > result_quantity:\n",
        "        this_max_number = result_quantity\n",
        "\n",
        "    print(f\"\\n\\nSet Name: {name_of_set}\")\n",
        "    print(f\"Total Matches in Set: {result_quantity}\")\n",
        "    print(f\"Matches Above Score-Floor in Set: {score_floor_filtered_quantity}\")\n",
        "    print(clean_timestamp)\n",
        "\n",
        "    print(f\"\\nShowing {score_floor_filtered_quantity} in top-{this_max_number} out of {result_quantity} total results.     -> {score_floor_filtered_quantity} of {this_max_number}/{result_quantity}\")\n",
        "    print(f\"(Ceiling set at {top_n} (top_n) filtered results.)    -> {top_n}\")\n",
        "    print(f\"(Minimum-included-score, 'Score-Floor' set at {score_floor}) -> {score_floor}\\n\\n\")\n",
        "\n",
        "    print_and_save(ranked_documents, top_n, name_of_set, score_floor)\n",
        "    counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HOGJ_6VhZtO",
        "outputId": "bebefc7e-f8b5-49df-ecaa-eb5ac6d81d1d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: distance measure\n",
            "Total Matches in Set: 86\n",
            "Matches Above Score-Floor in Set: 2\n",
            "2024-08-18__133621118761\n",
            "\n",
            "Showing 2 in top-45 out of 86 total results.     -> 2 of 45/86\n",
            "(Ceiling set at 45 (top_n) filtered results.)    -> 45\n",
            "(Minimum-included-score, 'Score-Floor' set at 3) -> 3\n",
            "\n",
            "\n",
            "Document: 7|||| \n",
            "'arxiv_id': arXiv:2408.07724, \n",
            "'paper_link': https://arxiv.org/abs/2408.07724, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07724, \n",
            "Title: \"Normalized Stress\" is Not Normalized: How to Interpret Stress Correctly \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Stress is among the most commonly employed quality metrics and optimization criteria for dimension reduction projections of high dimensional data. Complex, high dimensional data is ubiquitous across many scientific disciplines, including machine learning, biology, and the social sciences. One of the primary methods of visualizing these datasets is with two dimensional scatter plots that visually capture some properties of the data. Because visually determining the accuracy of these plots is challenging, researchers often use quality metrics to measure projection accuracy or faithfulness to the full data. One of the most commonly employed metrics, normalized stress, is sensitive to uniform scaling of the projection, despite this act not meaningfully changing anything about the projection. We investigate the effect of scaling on stress and other distance based quality metrics analytically and empirically by showing just how much the values change and how this affects dimension reduction technique evaluations. We introduce a simple technique to make normalized stress scale invariant and show that it accurately captures expected behavior on a small benchmark.\n",
            "Score: 12\n",
            "\n",
            "Document: 2|||| \n",
            "'arxiv_id': arXiv:2408.07706, \n",
            "'paper_link': https://arxiv.org/abs/2408.07706, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07706, \n",
            "Title: A Guide to Similarity Measures \n",
            "Subjects: Information Retrieval (cs.IR) \n",
            "Abstract: Similarity measures play a central role in various data science application domains for a wide assortment of tasks. This guide describes a comprehensive set of prevalent similarity measures to serve both non-experts and professional. Non-experts that wish to understand the motivation for a measure as well as how to use it may find a friendly and detailed exposition of the formulas of the measures, whereas experts may find a glance to the principles of designing similarity measures and ideas for a better way to measure similarity for their desired task in a given application domain.\n",
            "Score: 11\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Max Results Returned\n",
        "top_n = 45\n",
        "score_floor = 3\n",
        "\n",
        "\n",
        "list_of_lists_of_weights = [\n",
        "\n",
        "    # keyword_weights =\n",
        "    [\n",
        "        (\"Manifold Approximation\", 10),\n",
        "        (\"UMAP\", 10),\n",
        "        (\"Uniform Manifold Approximation and Projection\", 10),\n",
        "\n",
        "        (\"dimensionality reduction\", 10),\n",
        "        (\"dimension reduction\", 10),\n",
        "        (\"dimension reduction technique\", 10),\n",
        "\n",
        "        (\"DR\", 1),\n",
        "        (\"stress\", 1),\n",
        "        (\"Manifold\", 1),\n",
        "        (\"lower-dimensional\", 1),\n",
        "        (\"visualiz\", 1),\n",
        "        (\"projection\", 1),\n",
        "        (\"project\", 1),\n",
        "        (\"dimensionality\", 1),\n",
        "        (\"reduction\", 1),\n",
        "    ],\n",
        "\n",
        "\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"computer vision\", 3),\n",
        "    #     (\"resolution\", 2),\n",
        "    #     # (\"natural language processing\", 4),\n",
        "    #     # (\"deep learning\", 5),\n",
        "    #     (\"neural networks\", 6),\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"distance measure\", 10),\n",
        "    #     (\"similarity measure\", 10),\n",
        "    #     (\"vector distance\", 10),\n",
        "    #     (\"distance metric\", 10),\n",
        "    #     (\"similarity metric\", 10),\n",
        "    #     (\"dimension reduction\", 10),\n",
        "\n",
        "\n",
        "    #     (\"similarity\", 1),\n",
        "    #     (\"distance\", 1),\n",
        "    #     (\"metric\", 1),\n",
        "\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # # (\"cognitive science\", 2),  # much too broad...\n",
        "    # [\n",
        "    #     (\"mental health\", 5),\n",
        "    #     (\"psychological health\", 5),\n",
        "    #     (\"psycholog\", 2),  # stem vs. lemma\n",
        "\n",
        "\n",
        "    #     (\"mental health care\", 3),\n",
        "    #     (\"neuroscience\", 2),\n",
        "    #     (\"psychological assessment\", 2),\n",
        "    #     (\"personality assessment\", 2),\n",
        "    #     (\"personality inference\", 2),\n",
        "    #     (\"personality traits\", 2),\n",
        "    #     (\"personality dimensions\", 2),\n",
        "    #     (\"emotion\", 15),\n",
        "    #     (\"sports psychology\", 15),\n",
        "    #     # (\"\", 2),\n",
        "    #     # (\"\", 2),\n",
        "\n",
        "\n",
        "\n",
        "    #     # disease terms\n",
        "    #     (\"depression\", 5),\n",
        "    #     (\"anxiety\", 5),\n",
        "    #     (\"mental disorders\", 2),\n",
        "    #     (\"social anxiety disorder\", 4),\n",
        "    #     (\"mental illness\", 2),\n",
        "    #     (\"Major Depressive Disorder\", 2),\n",
        "    #     (\"MDD\", 2),\n",
        "    #     (\"psychological stressors\", 2),\n",
        "    #     (\"cognitive impairment\", 2),\n",
        "    #     (\"mci\", 2),\n",
        "    #     # (\"\", 2),\n",
        "    #     # (\"\", 2),\n",
        "    #     # (\"\", 2),\n",
        "\n",
        "    #     ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"benchmark\", 5),\n",
        "    #     (\"model evaluation\", 5),\n",
        "    #     (\"test\", 2),\n",
        "    #     (\"measure\", 2),\n",
        "    # ],\n",
        "\n",
        "\n",
        "    # # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"training set\", 5),\n",
        "    #     (\"synthetic\", 2),\n",
        "    #     (\"generate\", 2),\n",
        "    #     (\"measure\", 2),\n",
        "    # ],\n",
        "\n",
        "    # # keyword_weights =\n",
        "    # [\n",
        "    #     (\"graph\", 5),\n",
        "    #     (\"graph generation\", 8),\n",
        "    #     (\"subgraph\", 2),\n",
        "    #     (\"hierarchical graph\", 2),\n",
        "    #     (\"embedding\", 2),\n",
        "    #     (\"knowledge graph\", 2),\n",
        "\n",
        "    #     (\"graph neural networks\", 2),\n",
        "    #     (\"graph representation\", 2),\n",
        "    #     (\"node\", 2),\n",
        "    #      ## collisions: cryptograph, geograph,\n",
        "    # ],\n",
        "\n",
        "]\n",
        "\n",
        "date_time = datetime.now()\n",
        "clean_timestamp = date_time.strftime('%Y-%m-%d__%H%M%S%f')\n",
        "\n",
        "counter = 0\n",
        "for keyword_weights in list_of_lists_of_weights:\n",
        "\n",
        "    ranked_documents = rank_documents_on_weighted_matches(corpus, keyword_weights)\n",
        "\n",
        "    # user first list item as name of set\n",
        "    name_of_set = list_of_lists_of_weights[counter][0][0]\n",
        "\n",
        "    result_quantity = result_counter(ranked_documents)\n",
        "\n",
        "    score_floor_filtered_quantity = score_filtered_result_counter(ranked_documents, score_floor)\n",
        "\n",
        "    this_max_number = top_n\n",
        "\n",
        "    if top_n > result_quantity:\n",
        "        this_max_number = result_quantity\n",
        "\n",
        "    print(f\"\\n\\nSet Name: {name_of_set}\")\n",
        "    print(f\"Total Matches in Set: {result_quantity}\")\n",
        "    print(f\"Matches Above Score-Floor in Set: {score_floor_filtered_quantity}\")\n",
        "    print(clean_timestamp)\n",
        "\n",
        "    print(f\"\\nShowing {score_floor_filtered_quantity} in top-{this_max_number} out of {result_quantity} total results.     -> {score_floor_filtered_quantity} of {this_max_number}/{result_quantity}\")\n",
        "    print(f\"(Ceiling set at {top_n} (top_n) filtered results.)    -> {top_n}\")\n",
        "    print(f\"(Minimum-included-score, 'Score-Floor' set at {score_floor}) -> {score_floor}\\n\\n\")\n",
        "\n",
        "    print_and_save(ranked_documents, top_n, name_of_set, score_floor)\n",
        "    counter += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLOy8bu3elSO",
        "outputId": "805465fc-f916-4f4f-a659-0bdb75e60761"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Set Name: Manifold Approximation\n",
            "Total Matches in Set: 344\n",
            "Matches Above Score-Floor in Set: 6\n",
            "2024-08-18__132326097103\n",
            "\n",
            "Showing top-45 out of 344-total results.        -> 45/344\n",
            "(Ceiling set at 45 (top_n) filtered results.)   -> 45\n",
            "(Score-Floor set at 3 minimum-included-scores.) -> 3\n",
            "\n",
            "\n",
            "Document: 7|||| \n",
            "'arxiv_id': arXiv:2408.07724, \n",
            "'paper_link': https://arxiv.org/abs/2408.07724, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07724, \n",
            "Title: \"Normalized Stress\" is Not Normalized: How to Interpret Stress Correctly \n",
            "Subjects: Machine Learning (cs.LG) \n",
            "Abstract: Stress is among the most commonly employed quality metrics and optimization criteria for dimension reduction projections of high dimensional data. Complex, high dimensional data is ubiquitous across many scientific disciplines, including machine learning, biology, and the social sciences. One of the primary methods of visualizing these datasets is with two dimensional scatter plots that visually capture some properties of the data. Because visually determining the accuracy of these plots is challenging, researchers often use quality metrics to measure projection accuracy or faithfulness to the full data. One of the most commonly employed metrics, normalized stress, is sensitive to uniform scaling of the projection, despite this act not meaningfully changing anything about the projection. We investigate the effect of scaling on stress and other distance based quality metrics analytically and empirically by showing just how much the values change and how this affects dimension reduction technique evaluations. We introduce a simple technique to make normalized stress scale invariant and show that it accurately captures expected behavior on a small benchmark.\n",
            "Score: 25\n",
            "\n",
            "Document: 73|||| \n",
            "'arxiv_id': arXiv:2408.07891, \n",
            "'paper_link': https://arxiv.org/abs/2408.07891, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07891, \n",
            "Title: Quantum-inspired Interpretable Deep Learning Architecture for Text Sentiment Analysis \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Text has become the predominant form of communication on social media, embedding a wealth of emotional nuances. Consequently, the extraction of emotional information from text is of paramount importance. Despite previous research making some progress, existing text sentiment analysis models still face challenges in integrating diverse semantic information and lack interpretability. To address these issues, we propose a quantum-inspired deep learning architecture that combines fundamental principles of quantum mechanics (QM principles) with deep learning models for text sentiment analysis. Specifically, we analyze the commonalities between text representation and QM principles to design a quantum-inspired text representation method and further develop a quantum-inspired text embedding layer. Additionally, we design a feature extraction layer based on long short-term memory (LSTM) networks and self-attention mechanisms (SAMs). Finally, we calculate the text density matrix using the quantum complex numbers principle and apply 2D-convolution neural networks (CNNs) for feature condensation and dimensionality reduction. Through a series of visualization, comparative, and ablation experiments, we demonstrate that our model not only shows significant advantages in accuracy and efficiency compared to previous related models but also achieves a certain level of interpretability by integrating QM principles. Our code is available at QISA.\n",
            "Score: 14\n",
            "\n",
            "Document: 99|||| \n",
            "'arxiv_id': arXiv:2408.07939, \n",
            "'paper_link': https://arxiv.org/abs/2408.07939, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07939, \n",
            "Title: $\\mathcal{H}_2$-optimal Model Reduction of Linear Quadratic Output Systems in Finite Frequency Range \n",
            "Subjects: Systems and Control (eess.SY) \n",
            "Abstract: Linear quadratic output systems constitute an important class of dynamical systems with numerous practical applications. When the order of these models is exceptionally high, simulating and analyzing these systems becomes computationally prohibitive. In such instances, model order reduction offers an effective solution by approximating the original high-order system with a reduced-order model while preserving the system's essential characteristics.\n",
            "In frequency-limited model order reduction, the objective is to maintain the frequency response of the original system within a specified frequency range in the reduced-order model. In this paper, a mathematical expression for the frequency-limited $\\mathcal{H}_2$ norm is derived, which quantifies the error within the desired frequency interval. Subsequently, the necessary conditions for a local optimum of the frequency-limited $\\mathcal{H}_2$ norm of the error are derived. The inherent difficulty in satisfying these conditions within a Petrov-Galerkin projection framework is also discussed. Based on the optimality conditions and Petrov-Galerkin projection, a stationary point iteration algorithm is proposed that enforces three of the four optimality conditions upon convergence. A numerical example is provided to illustrate the algorithm's effectiveness in accurately approximating the original high-order model within the specified frequency interval.\n",
            "Score: 4\n",
            "\n",
            "Document: 242|||| \n",
            "'arxiv_id': arXiv:2408.08231, \n",
            "'paper_link': https://arxiv.org/abs/2408.08231, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.08231, \n",
            "Title: DaRec: A Disentangled Alignment Framework for Large Language Model and Recommender System \n",
            "Subjects: Information Retrieval (cs.IR) \n",
            "Abstract: Benefiting from the strong reasoning capabilities, Large language models (LLMs) have demonstrated remarkable performance in recommender systems. Various efforts have been made to distill knowledge from LLMs to enhance collaborative models, employing techniques like contrastive learning for representation alignment. In this work, we prove that directly aligning the representations of LLMs and collaborative models is sub-optimal for enhancing downstream recommendation tasks performance, based on the information theorem. Consequently, the challenge of effectively aligning semantic representations between collaborative models and LLMs remains unresolved. Inspired by this viewpoint, we propose a novel plug-and-play alignment framework for LLMs and collaborative models. Specifically, we first disentangle the latent representations of both LLMs and collaborative models into specific and shared components via projection layers and representation regularization. Subsequently, we perform both global and local structure alignment on the shared representations to facilitate knowledge transfer. Additionally, we theoretically prove that the specific and shared representations contain more pertinent and less irrelevant information, which can enhance the effectiveness of downstream recommendation tasks. Extensive experimental results on benchmark datasets demonstrate that our method is superior to existing state-of-the-art algorithms.\n",
            "Score: 3\n",
            "\n",
            "Document: 280|||| \n",
            "'arxiv_id': arXiv:2408.07847, \n",
            "'paper_link': https://arxiv.org/abs/2408.07847, \n",
            "'pdf_link': https://arxiv.org/pdf/2408.07847, \n",
            "Title: Time-inversion of spatiotemporal beam dynamics using uncertainty-aware latent evolution reversal \n",
            "Subjects: Accelerator Physics (physics.acc-ph) \n",
            "Abstract: Charged particle dynamics under the influence of electromagnetic fields is a challenging spatiotemporal problem. Many high performance physics-based simulators for predicting behavior in a charged particle beam are computationally expensive, limiting their utility for solving inverse problems online. The problem of estimating upstream six-dimensional phase space given downstream measurements of charged particles in an accelerator is an inverse problem of growing importance. This paper introduces a reverse Latent Evolution Model (rLEM) designed for temporal inversion of forward beam dynamics. In this two-step self-supervised deep learning framework, we utilize a Conditional Variational Autoencoder (CVAE) to project 6D phase space projections of a charged particle beam into a lower-dimensional latent distribution. Subsequently, we autoregressively learn the inverse temporal dynamics in the latent space using a Long Short-Term Memory (LSTM) network. The coupled CVAE-LSTM framework can predict 6D phase space projections across all upstream accelerating sections based on single or multiple downstream phase space measurements as inputs. The proposed model also captures the aleatoric uncertainty of the high-dimensional input data within the latent space. This uncertainty, which reflects potential uncertain measurements at a given module, is propagated through the LSTM to estimate uncertainty bounds for all upstream predictions, demonstrating the robustness of the LSTM against in-distribution variations in the input data.\n",
            "Score: 3\n",
            "\n",
            "Document: 369|||| \n",
            "'arxiv_id': arXiv:2402.15894, \n",
            "'paper_link': https://arxiv.org/abs/2402.15894, \n",
            "'pdf_link': https://arxiv.org/pdf/2402.15894, \n",
            "Title: Multi-graph Graph Matching for Coronary Artery Semantic Labeling \n",
            "Subjects: Computer Vision and Pattern Recognition (cs.CV) \n",
            "Abstract: Coronary artery disease (CAD) stands as the leading cause of death worldwide, and invasive coronary angiography (ICA) remains the gold standard for assessing vascular anatomical information. However, deep learning-based methods encounter challenges in generating semantic labels for arterial segments, primarily due to the morphological similarity between arterial branches and varying anatomy of arterial system between different projection view angles and patients. To address this challenge, we model the vascular tree as a graph and propose a multi-graph graph matching (MGM) algorithm for coronary artery semantic labeling. The MGM algorithm assesses the similarity between arterials in multiple vascular tree graphs, considering the cycle consistency between each pair of graphs. As a result, the unannotated arterial segments are appropriately labeled by matching them with annotated segments. Through the incorporation of anatomical graph structure, radiomics features, and semantic mapping, the proposed MGM model achieves an impressive accuracy of 0.9471 for coronary artery semantic labeling using our multi-site dataset with 718 ICAs. With the semantic labeled arteries, an overall accuracy of 0.9155 was achieved for stenosis detection. The proposed MGM presents a novel tool for coronary artery analysis using multiple ICA-derived graphs, offering valuable insights into vascular health and pathology.\n",
            "Score: 3\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Timer"
      ],
      "metadata": {
        "id": "BBVy06WenHk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end_time_whole_single_task = datetime.now()\n",
        "duration_time = duration_min_sec(start_time_whole_single_task, end_time_whole_single_task)\n",
        "print(f\"Duration to run -> {duration_time}\")"
      ],
      "metadata": {
        "id": "cCnRP_8anHbW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa4fb0c-707e-44da-a6d8-0af823c663f0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration to run -> 0_min__4.6_sec\n"
          ]
        }
      ]
    }
  ]
}